{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/img/icon_wechat.png","path":"img/icon_wechat.png","modified":0,"renderable":0},{"_id":"source/img/article/2019-08-08-sigmoid-function.png","path":"img/article/2019-08-08-sigmoid-function.png","modified":0,"renderable":0},{"_id":"source/img/article/tag.png","path":"img/article/tag.png","modified":0,"renderable":0},{"_id":"themes/huweihuang/source/css/archive.styl","path":"css/archive.styl","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/beantech.css","path":"css/beantech.css","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/beantech.min.css","path":"css/beantech.min.css","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/highlight.styl","path":"css/highlight.styl","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/donate.css","path":"css/donate.css","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/hux-blog.min.css","path":"css/hux-blog.min.css","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/rocket.styl","path":"css/rocket.styl","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/signature.styl","path":"css/signature.styl","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/toc.styl","path":"css/toc.styl","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/widget.styl","path":"css/widget.styl","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.eot","path":"fonts/glyphicons-halflings-regular.eot","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.ttf","path":"fonts/glyphicons-halflings-regular.ttf","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.woff","path":"fonts/glyphicons-halflings-regular.woff","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.woff2","path":"fonts/glyphicons-halflings-regular.woff2","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/bootstrap.min.js","path":"js/bootstrap.min.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/hux-blog.js","path":"js/hux-blog.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/hux-blog.min.js","path":"js/hux-blog.min.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/jquery.nav.js","path":"js/jquery.nav.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/jquery.tagcloud.js","path":"js/jquery.tagcloud.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/toc.js","path":"js/toc.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/totop.js","path":"js/totop.js","modified":0,"renderable":1},{"_id":"source/img/article/2019-08-09-decision-tree.png","path":"img/article/2019-08-09-decision-tree.png","modified":0,"renderable":0},{"_id":"source/img/header_img/about.jpg","path":"img/header_img/about.jpg","modified":0,"renderable":0},{"_id":"themes/huweihuang/source/css/bootstrap.min.css","path":"css/bootstrap.min.css","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.svg","path":"fonts/glyphicons-halflings-regular.svg","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/bootstrap.js","path":"js/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/js/jquery.min.js","path":"js/jquery.min.js","modified":0,"renderable":1},{"_id":"source/img/article/2019-08-07-bias-variance.png","path":"img/article/2019-08-07-bias-variance.png","modified":0,"renderable":0},{"_id":"source/img/header_img/archive.jpg","path":"img/header_img/archive.jpg","modified":0,"renderable":0},{"_id":"source/img/header_img/archives-widget.jpg","path":"img/header_img/archives-widget.jpg","modified":0,"renderable":0},{"_id":"source/img/signature/BeanTechSign-white.png","path":"img/signature/BeanTechSign-white.png","modified":0,"renderable":0},{"_id":"themes/huweihuang/source/css/bootstrap.css","path":"css/bootstrap.css","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/images/ironman.png","path":"css/images/ironman.png","modified":0,"renderable":1},{"_id":"themes/huweihuang/source/css/images/rocket.png","path":"css/images/rocket.png","modified":0,"renderable":1},{"_id":"source/img/signature/BeanTechSign-black.png","path":"img/signature/BeanTechSign-black.png","modified":0,"renderable":0},{"_id":"themes/huweihuang/source/js/jquery.js","path":"js/jquery.js","modified":0,"renderable":1},{"_id":"source/img/article_header/article_bg.jpg","path":"img/article_header/article_bg.jpg","modified":0,"renderable":0},{"_id":"source/img/avatar/wenjing-liu01.jpg","path":"img/avatar/wenjing-liu01.jpg","modified":0,"renderable":0},{"_id":"source/img/blog.jpg","path":"img/blog.jpg","modified":0,"renderable":0},{"_id":"source/img/header_img/home.jpg","path":"img/header_img/home.jpg","modified":0,"renderable":0},{"_id":"source/img/header_img/tag.png","path":"img/header_img/tag.png","modified":0,"renderable":0},{"_id":"source/img/article_header/article_header.png","path":"img/article_header/article_header.png","modified":0,"renderable":0},{"_id":"source/img/header_img/home-bg-o.png","path":"img/header_img/home-bg-o.png","modified":0,"renderable":0},{"_id":"source/img/header_img/home2.png","path":"img/header_img/home2.png","modified":0,"renderable":0},{"_id":"source/img/avatar/wenjing-liu.jpg","path":"img/avatar/wenjing-liu.jpg","modified":0,"renderable":0},{"_id":"source/img/header_img/404.png","path":"img/header_img/404.png","modified":0,"renderable":0}],"Cache":[{"_id":"source/404.md","hash":"8aa56af7bcd7cd23667cbf3eb5b5c9fa4533eb60","modified":1565160468157},{"_id":"source/CNAME","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1565160468157},{"_id":"source/.DS_Store","hash":"dc220c80f9495928cac3e1e2aafe0cf6a031969b","modified":1565160947074},{"_id":"themes/huweihuang/_config.yml","hash":"e800e32847df32dacddb5202823a004557023bcd","modified":1565160468246},{"_id":"themes/huweihuang/LICENSE","hash":"2b209f06bebeb2a8c2b7e187e436f3e1e1fbc8a7","modified":1565160468246},{"_id":"source/_posts/.DS_Store","hash":"df2fbeb1400acda0909a32c1cf6bf492f1121e07","modified":1565160947876},{"_id":"source/_posts/2018-01-02-JavaScript-2018-Frame.md","hash":"b047884a63c01061788c9c44dd8471c8fd9fd83c","modified":1565161905354},{"_id":"source/_posts/2019-07-01-ML-in-action-01.md","hash":"68de2fb59c5ec5f61a42aebbd5850659bcaa4e08","modified":1565150370487},{"_id":"source/_posts/2019-07-29-特征工程.md","hash":"806eefaf7c2fe3d12f88b27e94685c5bb5a179d0","modified":1565158491229},{"_id":"source/_posts/2019-07-30-机器学习优化方法.md","hash":"4e442a13382f994ca3a0af7bc303f3c62fa3ed79","modified":1565158491230},{"_id":"source/_posts/2019-07-31-2019-下半年学习计划.md","hash":"602491051b3ce233d6eb0cd4b49f1495b6cee94c","modified":1565158491231},{"_id":"source/_posts/2019-08-02-框架以及工具总结.md","hash":"b17b8ff33e11b2587a5a6b7526d86ad01bc5289c","modified":1565602989142},{"_id":"source/_posts/2019-08-06-线性回归.md","hash":"7363d7160a81a80b51667b629787aacfb5e484e6","modified":1565158491232},{"_id":"source/_posts/2019-08-07-Bias-Variance.md","hash":"2023b6b3de8ee57e1f79123c8e7e418a96d604ad","modified":1565170950567},{"_id":"source/_posts/2019-08-08-逻辑回归.md","hash":"4ddba99657e4937b3989d59f40104002a8f7d8cd","modified":1565249320062},{"_id":"source/_posts/2019-08-09-决策树-随机森林.md","hash":"79ec493620784cb0ee22a450899bbd8cb7e0fea2","modified":1565330988051},{"_id":"source/_posts/2019-08-09-多重共线性.md","hash":"f5da4f2a4a523e2ad510410934276c9277bf3294","modified":1565236152112},{"_id":"source/_posts/2019-08-08-K-近邻算法.md","hash":"d628248d356a60cb7f493129335389637a37fc56","modified":1565255667528},{"_id":"source/about/index.md","hash":"593416287b0516ce3652c96ddbd0fa80b6453529","modified":1565160468159},{"_id":"source/archive/index.md","hash":"8a773a78ba3dad1ee3f7c7b916788046c89860fb","modified":1565160468159},{"_id":"source/img/icon_wechat.png","hash":"4188058026609de06c6cac88b349a2da831a1783","modified":1565160646066},{"_id":"source/tags/index.md","hash":"f6ad1039c242795de5cd7d81781148f8c5298c28","modified":1565160468245},{"_id":"themes/huweihuang/languages_to_be_added/de.yml","hash":"424a9c1e6ab69334d7873f6574da02ca960aa572","modified":1565160468246},{"_id":"themes/huweihuang/languages_to_be_added/default.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1565160468246},{"_id":"themes/huweihuang/languages_to_be_added/en.yml","hash":"97326c9e6518d9f379778178b3b8f9a58434725d","modified":1565160468247},{"_id":"themes/huweihuang/languages_to_be_added/es.yml","hash":"cb4eeca0ed3768a77e0cd216300f2b2549628b1b","modified":1565160468247},{"_id":"themes/huweihuang/languages_to_be_added/no.yml","hash":"8ca475a3b4f8efe6603030f0013aae39668230e1","modified":1565160468247},{"_id":"themes/huweihuang/languages_to_be_added/pl.yml","hash":"de7eb5850ae65ba7638e907c805fea90617a988c","modified":1565160468248},{"_id":"themes/huweihuang/languages_to_be_added/ru.yml","hash":"42df7afeb7a35dc46d272b7f4fb880a9d9ebcaa5","modified":1565160468248},{"_id":"themes/huweihuang/languages_to_be_added/zh-CN.yml","hash":"7bfcb0b8e97d7e5edcfca8ab26d55d9da2573c1c","modified":1565160468248},{"_id":"themes/huweihuang/layout/404.ejs","hash":"40de38bd399f6f4aef0d6c63c7b13b02d74f1c56","modified":1565160468249},{"_id":"themes/huweihuang/languages_to_be_added/zh-TW.yml","hash":"9acac6cc4f8002c3fa53ff69fb8cf66c915bd016","modified":1565160468248},{"_id":"themes/huweihuang/layout/about.ejs","hash":"edcf8fa3bf7093c974d418ffef42ac89c19af128","modified":1565160468253},{"_id":"themes/huweihuang/layout/archive.ejs","hash":"72a150c8dff0031a9107d12eaa7c2e6c6ce950d2","modified":1565160468254},{"_id":"themes/huweihuang/layout/index.ejs","hash":"dc8a6eaa00d1e7c33a40979afe0953ed5d7b512e","modified":1565160468254},{"_id":"themes/huweihuang/layout/keynote.ejs","hash":"f5689862281e34dbe8402b0e72f632902e53e88b","modified":1565160468255},{"_id":"themes/huweihuang/layout/layout.ejs","hash":"a5af5b99ac3456ab5da1a319455904b979b91601","modified":1565160468255},{"_id":"themes/huweihuang/layout/page.ejs","hash":"5e588f200a7b7cd3ae40402b0dd3b779aac6787f","modified":1565160468256},{"_id":"themes/huweihuang/layout/post.ejs","hash":"4832891997c9d962c8b7b7bce6a778a25df41718","modified":1565160468256},{"_id":"themes/huweihuang/layout/tags.ejs","hash":"2c72eb2e89130658aa068d80d27b561b509c5dcd","modified":1565160468256},{"_id":"source/img/article/2019-08-08-sigmoid-function.png","hash":"e0007172aacc9d184af34c30299e28260c028a33","modified":1565170821077},{"_id":"source/img/article/tag.png","hash":"c8632d64d9471009098b84f70273e63037a4e7b8","modified":1565160646059},{"_id":"themes/huweihuang/layout/_partial/footer.ejs","hash":"66ec1893e4541a191af958a01b618674e4f70313","modified":1565160468249},{"_id":"themes/huweihuang/layout/_partial/head.ejs","hash":"9d223bf2f445addd28746117119b923a6c8f8588","modified":1565160468250},{"_id":"themes/huweihuang/layout/_partial/header.ejs","hash":"3bd09df76e0622d76d186b020393fcab361e6c97","modified":1565160468250},{"_id":"themes/huweihuang/layout/_partial/nav.ejs","hash":"4c905166c960852e9b9a3c9d5c680091e37b481f","modified":1565160468250},{"_id":"themes/huweihuang/layout/_partial/pagination.ejs","hash":"557d6bb069a1d48af49ae912994653f44b32a570","modified":1565160468250},{"_id":"themes/huweihuang/layout/_partial/sidebar.ejs","hash":"2e4e528a555917b2a267da4db2440bcc4a7a65ab","modified":1565160468251},{"_id":"themes/huweihuang/layout/_partial/toc.ejs","hash":"40e11b303df113c64a5ca35b79dd53c824010c09","modified":1565160468251},{"_id":"themes/huweihuang/layout/_widget/archive.ejs","hash":"7594929d472806ca4c64d9906d9903a96de111a0","modified":1565160468251},{"_id":"themes/huweihuang/layout/_widget/category.ejs","hash":"1cf485def07dc06e870dc9613767c6c614bcf428","modified":1565160468252},{"_id":"themes/huweihuang/layout/_widget/featured-tags.ejs","hash":"0c9ce1942f1943dc8891a9302a922ef1ffe300c5","modified":1565160468252},{"_id":"themes/huweihuang/layout/_widget/friends-blog.ejs","hash":"734d3775017aedac185028924baf890a71a74548","modified":1565160468252},{"_id":"themes/huweihuang/layout/_widget/recent-posts.ejs","hash":"e08ab8ba60e31638006acf27f066b989a0a3c433","modified":1565160468252},{"_id":"themes/huweihuang/layout/_widget/short-about.ejs","hash":"315de02246f07c747c32495e107ad7b19cb3ff54","modified":1565160468253},{"_id":"themes/huweihuang/source/css/archive.styl","hash":"715bcbd085eb95ec26c9805c11c374919cde971c","modified":1565160468257},{"_id":"themes/huweihuang/source/css/beantech.css","hash":"4c361354fd8e9851923fb21a620bc079380ebcd8","modified":1565160468257},{"_id":"themes/huweihuang/source/css/beantech.min.css","hash":"05a06230b1a9eca0b30cece54a397008cb77dc50","modified":1565160468258},{"_id":"themes/huweihuang/source/css/highlight.styl","hash":"e842080e6d580f0f70a7df71fbde3c4e49463c19","modified":1565160468261},{"_id":"themes/huweihuang/source/css/donate.css","hash":"f65ac8363d8d215adb896158e7b45165db259a47","modified":1565160468260},{"_id":"themes/huweihuang/source/css/hux-blog.min.css","hash":"1baef04de262aeb7023d835429b49a805ac4ab40","modified":1565160468261},{"_id":"themes/huweihuang/source/css/rocket.styl","hash":"e15c51c8566ecd943112e57592888dd318b6fa6a","modified":1565160468262},{"_id":"themes/huweihuang/source/css/signature.styl","hash":"88159b31c59d59c01a0b534af57242662a2a3969","modified":1565160468262},{"_id":"themes/huweihuang/source/css/toc.styl","hash":"631e97f634d30f53314e2fec8bdde267c1c49f4c","modified":1565160468263},{"_id":"themes/huweihuang/source/css/widget.styl","hash":"7a9f735f5ef323dc2950fbd9d76daa16c9a0f1a9","modified":1565160468263},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.eot","hash":"86b6f62b7853e67d3e635f6512a5a5efc58ea3c3","modified":1565160468263},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.ttf","hash":"44bc1850f570972267b169ae18f1cb06b611ffa2","modified":1565160468265},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.woff","hash":"278e49a86e634da6f2a02f3b47dd9d2a8f26210f","modified":1565160468266},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.woff2","hash":"ca35b697d99cae4d1b60f2d60fcd37771987eb07","modified":1565160468266},{"_id":"themes/huweihuang/source/js/bootstrap.min.js","hash":"b3f2ef9f985e7906c9360756b73cd64bf7733647","modified":1565160468267},{"_id":"themes/huweihuang/source/js/hux-blog.js","hash":"4b4d3c557405d04c3087d36c13e2834fe05c0f73","modified":1565160468268},{"_id":"themes/huweihuang/source/js/hux-blog.min.js","hash":"1563e7f70550ac6b30803d6f449719b853200e35","modified":1565160468268},{"_id":"themes/huweihuang/source/js/jquery.nav.js","hash":"ef2160a456176a4d09cc0b95d52b27dfbbadf2d8","modified":1565160468272},{"_id":"themes/huweihuang/source/js/jquery.tagcloud.js","hash":"4e5fd0b07f3bd935f2e603710447e039e3677211","modified":1565160468272},{"_id":"themes/huweihuang/source/js/toc.js","hash":"41e52551731854224c249d53010c1bae5aa92ffa","modified":1565160468273},{"_id":"themes/huweihuang/source/js/totop.js","hash":"c05360f6fc699ac12e794b1764b4a952713a3017","modified":1565160468273},{"_id":"source/img/article/2019-08-09-decision-tree.png","hash":"dcf96fc37cdf54777738a527e22d2f189369d20c","modified":1565256467902},{"_id":"source/img/header_img/about.jpg","hash":"62a9a76854503c327990cc8d10de293ab6e9588f","modified":1565160468205},{"_id":"themes/huweihuang/source/css/bootstrap.min.css","hash":"fec7b176a4b9a67c0eb5d184f57b84297efc23aa","modified":1565160468260},{"_id":"themes/huweihuang/source/fonts/glyphicons-halflings-regular.svg","hash":"de51a8494180a6db074af2dee2383f0a363c5b08","modified":1565160468264},{"_id":"themes/huweihuang/source/js/bootstrap.js","hash":"f8752e9ae24daec0a0baffd7819122f8c6fd9103","modified":1565160468267},{"_id":"themes/huweihuang/source/js/jquery.min.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1565160468271},{"_id":"source/img/article/2019-08-07-bias-variance.png","hash":"009fbd524ab1352a8d215ee8918a94a8c2f0059a","modified":1565162355000},{"_id":"source/img/header_img/archive.jpg","hash":"990f6f9dd0ecb5348bfcc47305553d58c0d8f326","modified":1565160468206},{"_id":"source/img/header_img/archives-widget.jpg","hash":"68f7d525269a94287e0ad18713ae232fb59dcf71","modified":1565160468208},{"_id":"source/img/signature/BeanTechSign-white.png","hash":"34289ed41cf9ddac2d56be46fbb1515b7d5913cd","modified":1565160468245},{"_id":"themes/huweihuang/source/css/bootstrap.css","hash":"41c54bf695145ae0b4d9020a1da308ceb05dcaf3","modified":1565160468259},{"_id":"themes/huweihuang/source/css/images/ironman.png","hash":"2f0db0ab15d466c4065d9f6102fdf829726d9e3f","modified":1565160468261},{"_id":"themes/huweihuang/source/css/images/rocket.png","hash":"6dee0406955aa9b7a261161d30f2538a671e806b","modified":1565160468262},{"_id":"source/img/signature/BeanTechSign-black.png","hash":"94b7102e819fd6ee082d3fb0166f4de7458c22ff","modified":1565160468243},{"_id":"themes/huweihuang/source/js/jquery.js","hash":"1852661bd11a09ca9b9cb63d1aa6ff390fffaf4e","modified":1565160468270},{"_id":"source/img/article_header/article_bg.jpg","hash":"6af63305c923899017e727b5ca968a2703bc08cf","modified":1565160468163},{"_id":"source/img/avatar/wenjing-liu01.jpg","hash":"adb893c8013f3befbd8cdc4c71235862a2f39253","modified":1565161738464},{"_id":"source/img/blog.jpg","hash":"a76af0b98dbe92ca2d21babcef13f094e409554b","modified":1565160646065},{"_id":"source/img/header_img/home.jpg","hash":"8f1c440427a4aa86b623503a926c027e2e10cd66","modified":1565160468223},{"_id":"source/img/header_img/tag.png","hash":"e83cd7b04ff85bcbc9bd3ebf5e57a55166e82568","modified":1565160468239},{"_id":"source/img/article_header/article_header.png","hash":"d9fa12f1e40924a0db57761d09d52dce450b4f7b","modified":1565160468178},{"_id":"source/img/header_img/home-bg-o.png","hash":"134ece4cb4c49c7ca1403a5afe7f46d0e2f9ecbb","modified":1565160468216},{"_id":"source/img/header_img/home2.png","hash":"da6a3d5ca787bdc25e69655abd879b4f821aeb30","modified":1565160468232},{"_id":"source/img/avatar/wenjing-liu.jpg","hash":"f3a2e0588a2e9d28a2b7cae94cc8ac3c0ab47e3c","modified":1565161841490},{"_id":"source/img/header_img/404.png","hash":"6bf9c224543ec54e250309db89cbdf46e0c4b5ba","modified":1565160468202},{"_id":"source/_posts/2019-08-12-K-均值聚类.md","hash":"613721c7d3546990b5a8f2d69e58f2a6a0f2cc48","modified":1565591274979},{"_id":"source/_posts/2019-08-13-主成分分析.md","hash":"f8f42b0bea9545c62ad9304328f1351a3e86c7ef","modified":1565603147389},{"_id":"source/_posts/2019-08-10-支持向量机.md","hash":"d3af481036d227e7c69e4c6756c1c0897e50081a","modified":1565575414491}],"Category":[],"Data":[],"Page":[{"layout":"404","description":"你来到了没有知识的荒原","header-img":"/img/header_img/404.png","_content":"","source":"404.md","raw":"---\nlayout: 404\ndescription: \"你来到了没有知识的荒原\"\nheader-img: \"/img/header_img/404.png\"\n---\n","date":"2019-08-07T06:47:48.157Z","updated":"2019-08-07T06:47:48.157Z","path":"404.html","title":"","comments":1,"_id":"cjz3pnqdu0000z0ovaew9hpol","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"about","title":"About","date":"2019-07-28T16:00:00.000Z","description":"","header-img":"/img/header_img/about.jpg","aplayer":true,"fixed":false,"_content":"\n###\n\n>\n\n### 关于我\n\n> 一个曾经搞前端的\n> 一个曾经搞JS web全栈的\n> 一个现在专注于数据科学的工程师\n\n### 兴趣方向\n\n> 前端开发\n> JS 全栈\n> 机器学习\n> 深度学习\n> 数据分析\n\n### 参与社区\n >\n > Github:https://github.com/wenjing-liu\n\n\n### 联系我\n\n>Email: wenjing.liu09@foxmail.com\n\n","source":"about/index.md","raw":"---\nlayout: \"about\"\ntitle: \"About\"\ndate: 2019-07-29\ndescription: \"\"\nheader-img: \"/img/header_img/about.jpg\"\naplayer: true\nfixed: false\n---\n\n###\n\n>\n\n### 关于我\n\n> 一个曾经搞前端的\n> 一个曾经搞JS web全栈的\n> 一个现在专注于数据科学的工程师\n\n### 兴趣方向\n\n> 前端开发\n> JS 全栈\n> 机器学习\n> 深度学习\n> 数据分析\n\n### 参与社区\n >\n > Github:https://github.com/wenjing-liu\n\n\n### 联系我\n\n>Email: wenjing.liu09@foxmail.com\n\n","updated":"2019-08-07T06:47:48.159Z","path":"about/index.html","comments":1,"_id":"cjz3pnqf00002z0ovlvzmdi8l","content":"<h3><span id=\"\"> </span></h3>\n<blockquote></blockquote>\n<h3><span id=\"关于我\"> 关于我</span></h3>\n<blockquote>\n<p>一个曾经搞前端的<br>\n一个曾经搞JS web全栈的<br>\n一个现在专注于数据科学的工程师</p>\n</blockquote>\n<h3><span id=\"兴趣方向\"> 兴趣方向</span></h3>\n<blockquote>\n<p>前端开发<br>\nJS 全栈<br>\n机器学习<br>\n深度学习<br>\n数据分析</p>\n</blockquote>\n<h3><span id=\"参与社区\"> 参与社区</span></h3>\n<blockquote>\n<p>Github:<a href=\"https://github.com/wenjing-liu\" target=\"_blank\" rel=\"noopener\">https://github.com/wenjing-liu</a></p>\n</blockquote>\n<h3><span id=\"联系我\"> 联系我</span></h3>\n<blockquote>\n<p>Email: <a href=\"mailto:wenjing.liu09@foxmail.com\" target=\"_blank\" rel=\"noopener\">wenjing.liu09@foxmail.com</a></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"\"><a class=\"markdownIt-Anchor\" href=\"#\"></a> </h3>\n<blockquote></blockquote>\n<h3 id=\"关于我\"><a class=\"markdownIt-Anchor\" href=\"#关于我\"></a> 关于我</h3>\n<blockquote>\n<p>一个曾经搞前端的<br>\n一个曾经搞JS web全栈的<br>\n一个现在专注于数据科学的工程师</p>\n</blockquote>\n<h3 id=\"兴趣方向\"><a class=\"markdownIt-Anchor\" href=\"#兴趣方向\"></a> 兴趣方向</h3>\n<blockquote>\n<p>前端开发<br>\nJS 全栈<br>\n机器学习<br>\n深度学习<br>\n数据分析</p>\n</blockquote>\n<h3 id=\"参与社区\"><a class=\"markdownIt-Anchor\" href=\"#参与社区\"></a> 参与社区</h3>\n<blockquote>\n<p>Github:<a href=\"https://github.com/wenjing-liu\" target=\"_blank\" rel=\"noopener\">https://github.com/wenjing-liu</a></p>\n</blockquote>\n<h3 id=\"联系我\"><a class=\"markdownIt-Anchor\" href=\"#联系我\"></a> 联系我</h3>\n<blockquote>\n<p>Email: <a href=\"mailto:wenjing.liu09@foxmail.com\" target=\"_blank\" rel=\"noopener\">wenjing.liu09@foxmail.com</a></p>\n</blockquote>\n"},{"layout":"archive","title":"Archives","header-img":"/img/header_img/archive.jpg","comments":0,"date":"2019-07-20T12:49:56.000Z","description":"Hey, this is archives","_content":"","source":"archive/index.md","raw":"---\nlayout: \"archive\"\ntitle: \"Archives\"\nheader-img: \"/img/header_img/archive.jpg\"\ncomments: false\ndate: 2019-07-20 20:49:56\ndescription: \"Hey, this is archives\"\n---\n","updated":"2019-08-07T06:47:48.159Z","path":"archive/index.html","_id":"cjz3pnqf30004z0ovno4jxhm9","content":"","site":{"data":{}},"excerpt":"","more":""},{"layout":"tags","title":"Tags","description":"Hey, this is Tags.","header-img":"/img/header_img/tag.png","_content":"","source":"tags/index.md","raw":"---\nlayout: \"tags\"\ntitle: \"Tags\"\ndescription: \"Hey, this is Tags.\"\nheader-img: \"/img/header_img/tag.png\"\n---\n","date":"2019-08-07T06:47:48.245Z","updated":"2019-08-07T06:47:48.245Z","path":"tags/index.html","comments":1,"_id":"cjz3pnqf70007z0ov99tdypua","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"JavaScript—2018 知识框架","catalog":true,"toc_nav_num":true,"date":"2018-05-11T16:00:00.000Z","subtitle":"Keep with new","header-img":"/img/article_header/article_header.png","catagories":["JavaScript"],"_content":"> 写在前面的话： 发现自己的记忆力不好，希望通过总结JavaScript的知识，强化记忆。最近趁着项目不忙，系统的学习了一下，让自己有个全面的知识体系，以及跟进JavaScript的发展步伐。(内容是中英文混杂，有时间后续会重写，有错误不全的地方请指正)\n\n\n### 一、基础JavaScript\n* Variable and Datatypes & variable mutation and type coercion\n* Operators\n* if/ else statements\n* Boolean logic and Switch statements\n* Functions\n* Statements and Expressions\n* Arrays\n* Objects and Properties\n* Object and Methods\n* Loop and Iteration\n* ES5, ES6/ ES2015 and ES2016\n***\n### 二、JavaScript运行原理\n* how code executed: JavaScript Parsers and Engines\n* Execution Contexts and Execution Stack\n* Execution Contexts in Detail: Creation and Execution Phases and Hoisting, hositing in practice\n* Scoping and the Scope Chain\n* 'this' keyword and 'this' in practice\n\n***\n### 三、浏览器中的JavaScript: DOM操作和事件\n* The DOM and DOM Manipulation\n* Math.random, Math.floor\n  * document.querySelector().textContent =\n  * document.querySelector().innerHTML =\n  * document.querySelector().style.display =\n  * document.querySelector().addEventListener(eventName,  function () {})\n  * document.querySelector().src\n  * document.getElementById()\n  * document.querySelector().classList.remove(className)\n  * document.querySelector().classList.add(className)\n  * document.querySelector().classList.toggle(className), if has class, delete, if not , add class\n* HTML and CSS crash course\n* Event and Event Handling: Rolling the Dice\n  * 事件只能在执行栈为空的时候来处理\n***\n### 四、高级JavaScript：Object和Function\n* Everything is an Object: Inheritance and the Prototype Chain\n  * primitives(number, string, boolean, undefined, null) and Objects\n  * constructors and instances\n  * inheritance\n  * prototypes and prototype chains\n  * every JavaScript object has a prototype property, which makes inheritance possible in JavaScript\n  * The prototype property of an object is where we put methods and properties that we want other objects to inherit\n  * The constructor's prototype property is NOT the prototype of the Cosntructor itself, it's the prototype of ALL instances that are created through it\n  * When a certain method (or property) is called, the search starts in the Object itself, and if it cannot be found, the search moves on to the objects's prototype. This continues until the mehtod is found: prototype chain.\n* Creating Objects: Function Constructors\n* The prototype Chain in the console\n  * \\__proto__  and prototype\n  * objectName.hasOwnProperty(propertyName)\n  * objectName instanceof Constructor\n  * console.info(objectName)\n * Creating Objects: Object.create\n  * Object.create(objectPrototype)\n  * Object.create(objectPrototype, obj), obj = { arr1: {value: 'attr'}, attr2: { vlaue: 'attr2'} }\n* Primitives vs Objects\n * primitives contain the values\n * objects point to the values\n* First Class Function: Passing Functions as Arguments\n * A function is an instance of the Object type\n * A function behaves like any other object\n * We can store function in a variable\n * We can pass a function as an argument to another function\n * We can return a function from a function\n * Above all we see in js, first-class functions\n* First Class Function: Functions Returning Functions\n* Immediately Invkoed Function Expression(IIFE)\n * 隔离作用域\n * 写惰性载入（惰性函数) : 例如浏览器的环境监测只需要执行一次，可以用这个\n* Closures\n * An inner function has always access to the variables and parameters of its outer function, even after the outer function has returned\n* Bind, Call and Apply\n * call(thisObj, otherArguments1, arguments2)\n* apply(thisObj, otherArgumentsArray)\n* bind(thisObj， arguments1), 不是立即调用，而是返回函数，参数可以分开设置，调用的时候在设第二个，函数curring\n***\n### 五、下一代JavaScript：ES6/ES2015\n* Variable Declarations with let and const\n * var , function scope\n * let, const , block scope, temporal dead zone\n   * accessing a let or constbefore it is declared throws ReferenceError\n* Blocks and IIFEs\n* String in ES6/ ES2015\n  * template literals (backtick (``) )\n  * string method\n    * str.startsWith('test')\n    * str.endsWith('test')\n    * str.includes('test')\n    * str.repeat(5)\n* Arrow Functions: Basics, Lexical 'this' keyword\n  * method call 'this' point to the object, function call 'this' point to the global object 'window'\n  * arrow function. sharing 'this' keywords with its surrounding\n* Destructuring\n  * [firstName, age] = ['claire', 20]\n  * {firstName, age } = { firstName: 'claire', age: 20}\n  * {firstName: a, age: b} = { firstName: 'claire', age: 20}\n* Arrays in ES6/ ES2015\n  * Array.from()\n  * el.className\n  * for ( el of arr)可以使用continue 和break\n  * forEach，map， continue，break不生效\n  * arr.findIndex(cur => cur > 18)\n  * arr.find(cur => cur > 18)\n* The spread operator\n  * ...[1, 3, 4]\n* Rest parameters\n* Default parameters\n* Maps\n  * new Map()\n  * map.set(key, value)\n  * map.get(key)\n  * map.size\n  * map.delete(key)\n  * map.has(key)\n  * map.clear()\n  * map.forEach\n  * for( let key of map)\n  * map.entries()\n* Classes\n  * static method in class\n  * class is no hosited, must first declared, then use\n  * only add method to class, not properties\n* Classes with Subclasses\n  * inheritance\n\n***\n### 六、异步JavaScript：Promise, Async/Await和Ajax\n* UnderStanding Asynchronous JavaScript: The Event Loop\n  * Event Loop\n* The Old Way: Asynchronous JavaScript with Callbacks\n* From Callback hell to Promises\n * Promise\n   * Object that keeps track about whether a certain event has happened already or not\n   * Determines what happens after the event has happened.\n   * Implements the concept of a furture value that we're expecting\n   * Promise state: pending-> settled/resolved--> fulfilled(rejected)\n* From Promises to Async/Await\n  * Async/Await is used to consume Promise\n  * Async function 总是返回Promise\n  * Promise vs Generator Function vs Async Await\n* Ajax and APIs\n  * Ajax: Asynchronous javascript and xml\n    * api: application programming interface own API, for data coming from you own server\n    * 3rd-party APIs:\n      * Google Maps\n      * Embed Youtube videos\n      * Weather data\n      * Movies data\n      * Send email or SMS\n.....\n   * 好用的跨域工具：https://crossorigin.me/: for cors when practice\n* Making Ajax Calls with Fetch and Promises, and Fetch and Async/Await\n   * fetch vs 传统Ajax(XMLHttpRequest)\n     * Fetch 优点主要有：\n       * 语法简洁，更加语义化\n       * 基于标准 Promise 实现，支持 async/await\n       * 同构方便，使用 isomorphic-fetch\n     * Fetch 常见坑\n       * Fetch 请求默认是不带 cookie 的，需要设置 fetch(url, {credentials: 'include'})\n     * 服务器返回 400，500 错误码时并不会  reject，只有网络错误这些导致请求不能完成时，fetch 才会被 reject。\n***\n### 七、现代JavaScript: 使用ES6, NPM, Babel and Webpack\n* A modern Setup: configuring webpack\n* A modern Setup: the webpack Dev Server\n* A modern Setup: Babel\n* Planning Project with MVC\n* How ES6 Modules work\n* Implementing Persistent Data with localStorage\n","source":"_posts/2018-01-02-JavaScript-2018-Frame.md","raw":"---\ntitle: \"JavaScript—2018 知识框架\"\ncatalog: true\ntoc_nav_num: true\ndate: 2018-05-12\nsubtitle: \"Keep with new\"\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- JavaScript\ncatagories:\n- JavaScript\n\n---\n> 写在前面的话： 发现自己的记忆力不好，希望通过总结JavaScript的知识，强化记忆。最近趁着项目不忙，系统的学习了一下，让自己有个全面的知识体系，以及跟进JavaScript的发展步伐。(内容是中英文混杂，有时间后续会重写，有错误不全的地方请指正)\n\n\n### 一、基础JavaScript\n* Variable and Datatypes & variable mutation and type coercion\n* Operators\n* if/ else statements\n* Boolean logic and Switch statements\n* Functions\n* Statements and Expressions\n* Arrays\n* Objects and Properties\n* Object and Methods\n* Loop and Iteration\n* ES5, ES6/ ES2015 and ES2016\n***\n### 二、JavaScript运行原理\n* how code executed: JavaScript Parsers and Engines\n* Execution Contexts and Execution Stack\n* Execution Contexts in Detail: Creation and Execution Phases and Hoisting, hositing in practice\n* Scoping and the Scope Chain\n* 'this' keyword and 'this' in practice\n\n***\n### 三、浏览器中的JavaScript: DOM操作和事件\n* The DOM and DOM Manipulation\n* Math.random, Math.floor\n  * document.querySelector().textContent =\n  * document.querySelector().innerHTML =\n  * document.querySelector().style.display =\n  * document.querySelector().addEventListener(eventName,  function () {})\n  * document.querySelector().src\n  * document.getElementById()\n  * document.querySelector().classList.remove(className)\n  * document.querySelector().classList.add(className)\n  * document.querySelector().classList.toggle(className), if has class, delete, if not , add class\n* HTML and CSS crash course\n* Event and Event Handling: Rolling the Dice\n  * 事件只能在执行栈为空的时候来处理\n***\n### 四、高级JavaScript：Object和Function\n* Everything is an Object: Inheritance and the Prototype Chain\n  * primitives(number, string, boolean, undefined, null) and Objects\n  * constructors and instances\n  * inheritance\n  * prototypes and prototype chains\n  * every JavaScript object has a prototype property, which makes inheritance possible in JavaScript\n  * The prototype property of an object is where we put methods and properties that we want other objects to inherit\n  * The constructor's prototype property is NOT the prototype of the Cosntructor itself, it's the prototype of ALL instances that are created through it\n  * When a certain method (or property) is called, the search starts in the Object itself, and if it cannot be found, the search moves on to the objects's prototype. This continues until the mehtod is found: prototype chain.\n* Creating Objects: Function Constructors\n* The prototype Chain in the console\n  * \\__proto__  and prototype\n  * objectName.hasOwnProperty(propertyName)\n  * objectName instanceof Constructor\n  * console.info(objectName)\n * Creating Objects: Object.create\n  * Object.create(objectPrototype)\n  * Object.create(objectPrototype, obj), obj = { arr1: {value: 'attr'}, attr2: { vlaue: 'attr2'} }\n* Primitives vs Objects\n * primitives contain the values\n * objects point to the values\n* First Class Function: Passing Functions as Arguments\n * A function is an instance of the Object type\n * A function behaves like any other object\n * We can store function in a variable\n * We can pass a function as an argument to another function\n * We can return a function from a function\n * Above all we see in js, first-class functions\n* First Class Function: Functions Returning Functions\n* Immediately Invkoed Function Expression(IIFE)\n * 隔离作用域\n * 写惰性载入（惰性函数) : 例如浏览器的环境监测只需要执行一次，可以用这个\n* Closures\n * An inner function has always access to the variables and parameters of its outer function, even after the outer function has returned\n* Bind, Call and Apply\n * call(thisObj, otherArguments1, arguments2)\n* apply(thisObj, otherArgumentsArray)\n* bind(thisObj， arguments1), 不是立即调用，而是返回函数，参数可以分开设置，调用的时候在设第二个，函数curring\n***\n### 五、下一代JavaScript：ES6/ES2015\n* Variable Declarations with let and const\n * var , function scope\n * let, const , block scope, temporal dead zone\n   * accessing a let or constbefore it is declared throws ReferenceError\n* Blocks and IIFEs\n* String in ES6/ ES2015\n  * template literals (backtick (``) )\n  * string method\n    * str.startsWith('test')\n    * str.endsWith('test')\n    * str.includes('test')\n    * str.repeat(5)\n* Arrow Functions: Basics, Lexical 'this' keyword\n  * method call 'this' point to the object, function call 'this' point to the global object 'window'\n  * arrow function. sharing 'this' keywords with its surrounding\n* Destructuring\n  * [firstName, age] = ['claire', 20]\n  * {firstName, age } = { firstName: 'claire', age: 20}\n  * {firstName: a, age: b} = { firstName: 'claire', age: 20}\n* Arrays in ES6/ ES2015\n  * Array.from()\n  * el.className\n  * for ( el of arr)可以使用continue 和break\n  * forEach，map， continue，break不生效\n  * arr.findIndex(cur => cur > 18)\n  * arr.find(cur => cur > 18)\n* The spread operator\n  * ...[1, 3, 4]\n* Rest parameters\n* Default parameters\n* Maps\n  * new Map()\n  * map.set(key, value)\n  * map.get(key)\n  * map.size\n  * map.delete(key)\n  * map.has(key)\n  * map.clear()\n  * map.forEach\n  * for( let key of map)\n  * map.entries()\n* Classes\n  * static method in class\n  * class is no hosited, must first declared, then use\n  * only add method to class, not properties\n* Classes with Subclasses\n  * inheritance\n\n***\n### 六、异步JavaScript：Promise, Async/Await和Ajax\n* UnderStanding Asynchronous JavaScript: The Event Loop\n  * Event Loop\n* The Old Way: Asynchronous JavaScript with Callbacks\n* From Callback hell to Promises\n * Promise\n   * Object that keeps track about whether a certain event has happened already or not\n   * Determines what happens after the event has happened.\n   * Implements the concept of a furture value that we're expecting\n   * Promise state: pending-> settled/resolved--> fulfilled(rejected)\n* From Promises to Async/Await\n  * Async/Await is used to consume Promise\n  * Async function 总是返回Promise\n  * Promise vs Generator Function vs Async Await\n* Ajax and APIs\n  * Ajax: Asynchronous javascript and xml\n    * api: application programming interface own API, for data coming from you own server\n    * 3rd-party APIs:\n      * Google Maps\n      * Embed Youtube videos\n      * Weather data\n      * Movies data\n      * Send email or SMS\n.....\n   * 好用的跨域工具：https://crossorigin.me/: for cors when practice\n* Making Ajax Calls with Fetch and Promises, and Fetch and Async/Await\n   * fetch vs 传统Ajax(XMLHttpRequest)\n     * Fetch 优点主要有：\n       * 语法简洁，更加语义化\n       * 基于标准 Promise 实现，支持 async/await\n       * 同构方便，使用 isomorphic-fetch\n     * Fetch 常见坑\n       * Fetch 请求默认是不带 cookie 的，需要设置 fetch(url, {credentials: 'include'})\n     * 服务器返回 400，500 错误码时并不会  reject，只有网络错误这些导致请求不能完成时，fetch 才会被 reject。\n***\n### 七、现代JavaScript: 使用ES6, NPM, Babel and Webpack\n* A modern Setup: configuring webpack\n* A modern Setup: the webpack Dev Server\n* A modern Setup: Babel\n* Planning Project with MVC\n* How ES6 Modules work\n* Implementing Persistent Data with localStorage\n","slug":"2018-01-02-JavaScript-2018-Frame","published":1,"updated":"2019-08-07T07:11:45.354Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqex0001z0ovrrwvjtz6","content":"<blockquote>\n<p>写在前面的话： 发现自己的记忆力不好，希望通过总结JavaScript的知识，强化记忆。最近趁着项目不忙，系统的学习了一下，让自己有个全面的知识体系，以及跟进JavaScript的发展步伐。(内容是中英文混杂，有时间后续会重写，有错误不全的地方请指正)</p>\n</blockquote>\n<h3><span id=\"一-基础javascript\"> 一、基础JavaScript</span></h3>\n<ul>\n<li>Variable and Datatypes &amp; variable mutation and type coercion</li>\n<li>Operators</li>\n<li>if/ else statements</li>\n<li>Boolean logic and Switch statements</li>\n<li>Functions</li>\n<li>Statements and Expressions</li>\n<li>Arrays</li>\n<li>Objects and Properties</li>\n<li>Object and Methods</li>\n<li>Loop and Iteration</li>\n<li>ES5, ES6/ ES2015 and ES2016</li>\n</ul>\n<hr>\n<h3><span id=\"二-javascript运行原理\"> 二、JavaScript运行原理</span></h3>\n<ul>\n<li>how code executed: JavaScript Parsers and Engines</li>\n<li>Execution Contexts and Execution Stack</li>\n<li>Execution Contexts in Detail: Creation and Execution Phases and Hoisting, hositing in practice</li>\n<li>Scoping and the Scope Chain</li>\n<li>‘this’ keyword and ‘this’ in practice</li>\n</ul>\n<hr>\n<h3><span id=\"三-浏览器中的javascript-dom操作和事件\"> 三、浏览器中的JavaScript: DOM操作和事件</span></h3>\n<ul>\n<li>The DOM and DOM Manipulation</li>\n<li>Math.random, Math.floor\n<ul>\n<li>document.querySelector().textContent =</li>\n<li>document.querySelector().innerHTML =</li>\n<li>document.querySelector().style.display =</li>\n<li>document.querySelector().addEventListener(eventName,  function () {})</li>\n<li>document.querySelector().src</li>\n<li>document.getElementById()</li>\n<li>document.querySelector().classList.remove(className)</li>\n<li>document.querySelector().classList.add(className)</li>\n<li>document.querySelector().classList.toggle(className), if has class, delete, if not , add class</li>\n</ul>\n</li>\n<li>HTML and CSS crash course</li>\n<li>Event and Event Handling: Rolling the Dice\n<ul>\n<li>事件只能在执行栈为空的时候来处理</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><span id=\"四-高级javascriptobject和function\"> 四、高级JavaScript：Object和Function</span></h3>\n<ul>\n<li>Everything is an Object: Inheritance and the Prototype Chain\n<ul>\n<li>primitives(number, string, boolean, undefined, null) and Objects</li>\n<li>constructors and instances</li>\n<li>inheritance</li>\n<li>prototypes and prototype chains</li>\n<li>every JavaScript object has a prototype property, which makes inheritance possible in JavaScript</li>\n<li>The prototype property of an object is where we put methods and properties that we want other objects to inherit</li>\n<li>The constructor’s prototype property is NOT the prototype of the Cosntructor itself, it’s the prototype of ALL instances that are created through it</li>\n<li>When a certain method (or property) is called, the search starts in the Object itself, and if it cannot be found, the search moves on to the objects’s prototype. This continues until the mehtod is found: prototype chain.</li>\n</ul>\n</li>\n<li>Creating Objects: Function Constructors</li>\n<li>The prototype Chain in the console\n<ul>\n<li>_<em>proto</em>_  and prototype</li>\n<li>objectName.hasOwnProperty(propertyName)</li>\n<li>objectName instanceof Constructor</li>\n<li><a href=\"http://console.info\" target=\"_blank\" rel=\"noopener\">console.info</a>(objectName)</li>\n</ul>\n</li>\n<li>Creating Objects: Object.create</li>\n<li>Object.create(objectPrototype)</li>\n<li>Object.create(objectPrototype, obj), obj = { arr1: {value: ‘attr’}, attr2: { vlaue: ‘attr2’} }</li>\n<li>Primitives vs Objects</li>\n<li>primitives contain the values</li>\n<li>objects point to the values</li>\n<li>First Class Function: Passing Functions as Arguments</li>\n<li>A function is an instance of the Object type</li>\n<li>A function behaves like any other object</li>\n<li>We can store function in a variable</li>\n<li>We can pass a function as an argument to another function</li>\n<li>We can return a function from a function</li>\n<li>Above all we see in js, first-class functions</li>\n<li>First Class Function: Functions Returning Functions</li>\n<li>Immediately Invkoed Function Expression(IIFE)</li>\n<li>隔离作用域</li>\n<li>写惰性载入（惰性函数) : 例如浏览器的环境监测只需要执行一次，可以用这个</li>\n<li>Closures</li>\n<li>An inner function has always access to the variables and parameters of its outer function, even after the outer function has returned</li>\n<li>Bind, Call and Apply</li>\n<li>call(thisObj, otherArguments1, arguments2)</li>\n<li>apply(thisObj, otherArgumentsArray)</li>\n<li>bind(thisObj， arguments1), 不是立即调用，而是返回函数，参数可以分开设置，调用的时候在设第二个，函数curring</li>\n</ul>\n<hr>\n<h3><span id=\"五-下一代javascriptes6es2015\"> 五、下一代JavaScript：ES6/ES2015</span></h3>\n<ul>\n<li>Variable Declarations with let and const</li>\n<li>var , function scope</li>\n<li>let, const , block scope, temporal dead zone\n<ul>\n<li>accessing a let or constbefore it is declared throws ReferenceError</li>\n</ul>\n</li>\n<li>Blocks and IIFEs</li>\n<li>String in ES6/ ES2015\n<ul>\n<li>template literals (backtick (``) )</li>\n<li>string method\n<ul>\n<li>str.startsWith(‘test’)</li>\n<li>str.endsWith(‘test’)</li>\n<li>str.includes(‘test’)</li>\n<li>str.repeat(5)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Arrow Functions: Basics, Lexical ‘this’ keyword\n<ul>\n<li>method call ‘this’ point to the object, function call ‘this’ point to the global object ‘window’</li>\n<li>arrow function. sharing ‘this’ keywords with its surrounding</li>\n</ul>\n</li>\n<li>Destructuring\n<ul>\n<li>[firstName, age] = [‘claire’, 20]</li>\n<li>{firstName, age } = { firstName: ‘claire’, age: 20}</li>\n<li>{firstName: a, age: b} = { firstName: ‘claire’, age: 20}</li>\n</ul>\n</li>\n<li>Arrays in ES6/ ES2015\n<ul>\n<li>Array.from()</li>\n<li>el.className</li>\n<li>for ( el of arr)可以使用continue 和break</li>\n<li>forEach，map， continue，break不生效</li>\n<li>arr.findIndex(cur =&gt; cur &gt; 18)</li>\n<li>arr.find(cur =&gt; cur &gt; 18)</li>\n</ul>\n</li>\n<li>The spread operator\n<ul>\n<li>…[1, 3, 4]</li>\n</ul>\n</li>\n<li>Rest parameters</li>\n<li>Default parameters</li>\n<li>Maps\n<ul>\n<li>new Map()</li>\n<li>map.set(key, value)</li>\n<li>map.get(key)</li>\n<li>map.size</li>\n<li>map.delete(key)</li>\n<li>map.has(key)</li>\n<li>map.clear()</li>\n<li>map.forEach</li>\n<li>for( let key of map)</li>\n<li>map.entries()</li>\n</ul>\n</li>\n<li>Classes\n<ul>\n<li>static method in class</li>\n<li>class is no hosited, must first declared, then use</li>\n<li>only add method to class, not properties</li>\n</ul>\n</li>\n<li>Classes with Subclasses\n<ul>\n<li>inheritance</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><span id=\"六-异步javascriptpromise-asyncawait和ajax\"> 六、异步JavaScript：Promise, Async/Await和Ajax</span></h3>\n<ul>\n<li>UnderStanding Asynchronous JavaScript: The Event Loop\n<ul>\n<li>Event Loop</li>\n</ul>\n</li>\n<li>The Old Way: Asynchronous JavaScript with Callbacks</li>\n<li>From Callback hell to Promises</li>\n<li>Promise\n<ul>\n<li>Object that keeps track about whether a certain event has happened already or not</li>\n<li>Determines what happens after the event has happened.</li>\n<li>Implements the concept of a furture value that we’re expecting</li>\n<li>Promise state: pending-&gt; settled/resolved–&gt; fulfilled(rejected)</li>\n</ul>\n</li>\n<li>From Promises to Async/Await\n<ul>\n<li>Async/Await is used to consume Promise</li>\n<li>Async function 总是返回Promise</li>\n<li>Promise vs Generator Function vs Async Await</li>\n</ul>\n</li>\n<li>Ajax and APIs\n<ul>\n<li>Ajax: Asynchronous javascript and xml\n<ul>\n<li>api: application programming interface own API, for data coming from you own server</li>\n<li>3rd-party APIs:\n<ul>\n<li>Google Maps</li>\n<li>Embed Youtube videos</li>\n<li>Weather data</li>\n<li>Movies data</li>\n<li>Send email or SMS<br>\n…</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>好用的跨域工具：<a href=\"https://crossorigin.me/:\" target=\"_blank\" rel=\"noopener\">https://crossorigin.me/:</a> for cors when practice</li>\n</ul>\n</li>\n<li>Making Ajax Calls with Fetch and Promises, and Fetch and Async/Await\n<ul>\n<li>fetch vs 传统Ajax(XMLHttpRequest)\n<ul>\n<li>Fetch 优点主要有：\n<ul>\n<li>语法简洁，更加语义化</li>\n<li>基于标准 Promise 实现，支持 async/await</li>\n<li>同构方便，使用 isomorphic-fetch</li>\n</ul>\n</li>\n<li>Fetch 常见坑\n<ul>\n<li>Fetch 请求默认是不带 cookie 的，需要设置 fetch(url, {credentials: ‘include’})</li>\n</ul>\n</li>\n<li>服务器返回 400，500 错误码时并不会  reject，只有网络错误这些导致请求不能完成时，fetch 才会被 reject。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3><span id=\"七-现代javascript-使用es6-npm-babel-and-webpack\"> 七、现代JavaScript: 使用ES6, NPM, Babel and Webpack</span></h3>\n<ul>\n<li>A modern Setup: configuring webpack</li>\n<li>A modern Setup: the webpack Dev Server</li>\n<li>A modern Setup: Babel</li>\n<li>Planning Project with MVC</li>\n<li>How ES6 Modules work</li>\n<li>Implementing Persistent Data with localStorage</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>写在前面的话： 发现自己的记忆力不好，希望通过总结JavaScript的知识，强化记忆。最近趁着项目不忙，系统的学习了一下，让自己有个全面的知识体系，以及跟进JavaScript的发展步伐。(内容是中英文混杂，有时间后续会重写，有错误不全的地方请指正)</p>\n</blockquote>\n<h3 id=\"一-基础javascript\"><a class=\"markdownIt-Anchor\" href=\"#一-基础javascript\"></a> 一、基础JavaScript</h3>\n<ul>\n<li>Variable and Datatypes &amp; variable mutation and type coercion</li>\n<li>Operators</li>\n<li>if/ else statements</li>\n<li>Boolean logic and Switch statements</li>\n<li>Functions</li>\n<li>Statements and Expressions</li>\n<li>Arrays</li>\n<li>Objects and Properties</li>\n<li>Object and Methods</li>\n<li>Loop and Iteration</li>\n<li>ES5, ES6/ ES2015 and ES2016</li>\n</ul>\n<hr>\n<h3 id=\"二-javascript运行原理\"><a class=\"markdownIt-Anchor\" href=\"#二-javascript运行原理\"></a> 二、JavaScript运行原理</h3>\n<ul>\n<li>how code executed: JavaScript Parsers and Engines</li>\n<li>Execution Contexts and Execution Stack</li>\n<li>Execution Contexts in Detail: Creation and Execution Phases and Hoisting, hositing in practice</li>\n<li>Scoping and the Scope Chain</li>\n<li>‘this’ keyword and ‘this’ in practice</li>\n</ul>\n<hr>\n<h3 id=\"三-浏览器中的javascript-dom操作和事件\"><a class=\"markdownIt-Anchor\" href=\"#三-浏览器中的javascript-dom操作和事件\"></a> 三、浏览器中的JavaScript: DOM操作和事件</h3>\n<ul>\n<li>The DOM and DOM Manipulation</li>\n<li>Math.random, Math.floor\n<ul>\n<li>document.querySelector().textContent =</li>\n<li>document.querySelector().innerHTML =</li>\n<li>document.querySelector().style.display =</li>\n<li>document.querySelector().addEventListener(eventName,  function () {})</li>\n<li>document.querySelector().src</li>\n<li>document.getElementById()</li>\n<li>document.querySelector().classList.remove(className)</li>\n<li>document.querySelector().classList.add(className)</li>\n<li>document.querySelector().classList.toggle(className), if has class, delete, if not , add class</li>\n</ul>\n</li>\n<li>HTML and CSS crash course</li>\n<li>Event and Event Handling: Rolling the Dice\n<ul>\n<li>事件只能在执行栈为空的时候来处理</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"四-高级javascriptobject和function\"><a class=\"markdownIt-Anchor\" href=\"#四-高级javascriptobject和function\"></a> 四、高级JavaScript：Object和Function</h3>\n<ul>\n<li>Everything is an Object: Inheritance and the Prototype Chain\n<ul>\n<li>primitives(number, string, boolean, undefined, null) and Objects</li>\n<li>constructors and instances</li>\n<li>inheritance</li>\n<li>prototypes and prototype chains</li>\n<li>every JavaScript object has a prototype property, which makes inheritance possible in JavaScript</li>\n<li>The prototype property of an object is where we put methods and properties that we want other objects to inherit</li>\n<li>The constructor’s prototype property is NOT the prototype of the Cosntructor itself, it’s the prototype of ALL instances that are created through it</li>\n<li>When a certain method (or property) is called, the search starts in the Object itself, and if it cannot be found, the search moves on to the objects’s prototype. This continues until the mehtod is found: prototype chain.</li>\n</ul>\n</li>\n<li>Creating Objects: Function Constructors</li>\n<li>The prototype Chain in the console\n<ul>\n<li>_<em>proto</em>_  and prototype</li>\n<li>objectName.hasOwnProperty(propertyName)</li>\n<li>objectName instanceof Constructor</li>\n<li><a href=\"http://console.info\" target=\"_blank\" rel=\"noopener\">console.info</a>(objectName)</li>\n</ul>\n</li>\n<li>Creating Objects: Object.create</li>\n<li>Object.create(objectPrototype)</li>\n<li>Object.create(objectPrototype, obj), obj = { arr1: {value: ‘attr’}, attr2: { vlaue: ‘attr2’} }</li>\n<li>Primitives vs Objects</li>\n<li>primitives contain the values</li>\n<li>objects point to the values</li>\n<li>First Class Function: Passing Functions as Arguments</li>\n<li>A function is an instance of the Object type</li>\n<li>A function behaves like any other object</li>\n<li>We can store function in a variable</li>\n<li>We can pass a function as an argument to another function</li>\n<li>We can return a function from a function</li>\n<li>Above all we see in js, first-class functions</li>\n<li>First Class Function: Functions Returning Functions</li>\n<li>Immediately Invkoed Function Expression(IIFE)</li>\n<li>隔离作用域</li>\n<li>写惰性载入（惰性函数) : 例如浏览器的环境监测只需要执行一次，可以用这个</li>\n<li>Closures</li>\n<li>An inner function has always access to the variables and parameters of its outer function, even after the outer function has returned</li>\n<li>Bind, Call and Apply</li>\n<li>call(thisObj, otherArguments1, arguments2)</li>\n<li>apply(thisObj, otherArgumentsArray)</li>\n<li>bind(thisObj， arguments1), 不是立即调用，而是返回函数，参数可以分开设置，调用的时候在设第二个，函数curring</li>\n</ul>\n<hr>\n<h3 id=\"五-下一代javascriptes6es2015\"><a class=\"markdownIt-Anchor\" href=\"#五-下一代javascriptes6es2015\"></a> 五、下一代JavaScript：ES6/ES2015</h3>\n<ul>\n<li>Variable Declarations with let and const</li>\n<li>var , function scope</li>\n<li>let, const , block scope, temporal dead zone\n<ul>\n<li>accessing a let or constbefore it is declared throws ReferenceError</li>\n</ul>\n</li>\n<li>Blocks and IIFEs</li>\n<li>String in ES6/ ES2015\n<ul>\n<li>template literals (backtick (``) )</li>\n<li>string method\n<ul>\n<li>str.startsWith(‘test’)</li>\n<li>str.endsWith(‘test’)</li>\n<li>str.includes(‘test’)</li>\n<li>str.repeat(5)</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Arrow Functions: Basics, Lexical ‘this’ keyword\n<ul>\n<li>method call ‘this’ point to the object, function call ‘this’ point to the global object ‘window’</li>\n<li>arrow function. sharing ‘this’ keywords with its surrounding</li>\n</ul>\n</li>\n<li>Destructuring\n<ul>\n<li>[firstName, age] = [‘claire’, 20]</li>\n<li>{firstName, age } = { firstName: ‘claire’, age: 20}</li>\n<li>{firstName: a, age: b} = { firstName: ‘claire’, age: 20}</li>\n</ul>\n</li>\n<li>Arrays in ES6/ ES2015\n<ul>\n<li>Array.from()</li>\n<li>el.className</li>\n<li>for ( el of arr)可以使用continue 和break</li>\n<li>forEach，map， continue，break不生效</li>\n<li>arr.findIndex(cur =&gt; cur &gt; 18)</li>\n<li>arr.find(cur =&gt; cur &gt; 18)</li>\n</ul>\n</li>\n<li>The spread operator\n<ul>\n<li>…[1, 3, 4]</li>\n</ul>\n</li>\n<li>Rest parameters</li>\n<li>Default parameters</li>\n<li>Maps\n<ul>\n<li>new Map()</li>\n<li>map.set(key, value)</li>\n<li>map.get(key)</li>\n<li>map.size</li>\n<li>map.delete(key)</li>\n<li>map.has(key)</li>\n<li>map.clear()</li>\n<li>map.forEach</li>\n<li>for( let key of map)</li>\n<li>map.entries()</li>\n</ul>\n</li>\n<li>Classes\n<ul>\n<li>static method in class</li>\n<li>class is no hosited, must first declared, then use</li>\n<li>only add method to class, not properties</li>\n</ul>\n</li>\n<li>Classes with Subclasses\n<ul>\n<li>inheritance</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"六-异步javascriptpromise-asyncawait和ajax\"><a class=\"markdownIt-Anchor\" href=\"#六-异步javascriptpromise-asyncawait和ajax\"></a> 六、异步JavaScript：Promise, Async/Await和Ajax</h3>\n<ul>\n<li>UnderStanding Asynchronous JavaScript: The Event Loop\n<ul>\n<li>Event Loop</li>\n</ul>\n</li>\n<li>The Old Way: Asynchronous JavaScript with Callbacks</li>\n<li>From Callback hell to Promises</li>\n<li>Promise\n<ul>\n<li>Object that keeps track about whether a certain event has happened already or not</li>\n<li>Determines what happens after the event has happened.</li>\n<li>Implements the concept of a furture value that we’re expecting</li>\n<li>Promise state: pending-&gt; settled/resolved–&gt; fulfilled(rejected)</li>\n</ul>\n</li>\n<li>From Promises to Async/Await\n<ul>\n<li>Async/Await is used to consume Promise</li>\n<li>Async function 总是返回Promise</li>\n<li>Promise vs Generator Function vs Async Await</li>\n</ul>\n</li>\n<li>Ajax and APIs\n<ul>\n<li>Ajax: Asynchronous javascript and xml\n<ul>\n<li>api: application programming interface own API, for data coming from you own server</li>\n<li>3rd-party APIs:\n<ul>\n<li>Google Maps</li>\n<li>Embed Youtube videos</li>\n<li>Weather data</li>\n<li>Movies data</li>\n<li>Send email or SMS<br>\n…</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>好用的跨域工具：<a href=\"https://crossorigin.me/:\" target=\"_blank\" rel=\"noopener\">https://crossorigin.me/:</a> for cors when practice</li>\n</ul>\n</li>\n<li>Making Ajax Calls with Fetch and Promises, and Fetch and Async/Await\n<ul>\n<li>fetch vs 传统Ajax(XMLHttpRequest)\n<ul>\n<li>Fetch 优点主要有：\n<ul>\n<li>语法简洁，更加语义化</li>\n<li>基于标准 Promise 实现，支持 async/await</li>\n<li>同构方便，使用 isomorphic-fetch</li>\n</ul>\n</li>\n<li>Fetch 常见坑\n<ul>\n<li>Fetch 请求默认是不带 cookie 的，需要设置 fetch(url, {credentials: ‘include’})</li>\n</ul>\n</li>\n<li>服务器返回 400，500 错误码时并不会  reject，只有网络错误这些导致请求不能完成时，fetch 才会被 reject。</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"七-现代javascript-使用es6-npm-babel-and-webpack\"><a class=\"markdownIt-Anchor\" href=\"#七-现代javascript-使用es6-npm-babel-and-webpack\"></a> 七、现代JavaScript: 使用ES6, NPM, Babel and Webpack</h3>\n<ul>\n<li>A modern Setup: configuring webpack</li>\n<li>A modern Setup: the webpack Dev Server</li>\n<li>A modern Setup: Babel</li>\n<li>Planning Project with MVC</li>\n<li>How ES6 Modules work</li>\n<li>Implementing Persistent Data with localStorage</li>\n</ul>\n"},{"title":"Machine Learning Basics","catalog":true,"toc_nav_num":true,"date":"2019-06-30T16:00:00.000Z","subtitle":"Day 1","header-img":"/img/article_header/article_header.png","catagories":["Machine Learning in Action"],"_content":"\n### A brief overview of machine learning\n\n### Key tasks in machine Learning \n\n* Classification\n\n* Regression\n\n\n### How to choose a right algorithm\n\n|  Supervised learning tasks |  | \n| ------------- |:-------------:| \n| k-Nearest Neighbors| Linear | \n| Naive Bayes      | Locally weighted linear    | \n| Support vector machines | Ridge      | \n| Decision trees | Lasso      | \n\n|  Unsupervised learning tasks |  | \n| ------------- |:-------------:| \n| k-Means| Expectation maximization | \n| DBSCAN      | Parzen window    | \n\n### Why you need know about machine learning\n\n### Why python is so great for machine learning\n\n","source":"_posts/2019-07-01-ML-in-action-01.md","raw":"---\ntitle: \"Machine Learning Basics\"\ncatalog: true\ntoc_nav_num: true\ndate: 2019-07-01\nsubtitle: \"Day 1\"\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- Machine Learning\ncatagories:\n- Machine Learning in Action\n---\n\n### A brief overview of machine learning\n\n### Key tasks in machine Learning \n\n* Classification\n\n* Regression\n\n\n### How to choose a right algorithm\n\n|  Supervised learning tasks |  | \n| ------------- |:-------------:| \n| k-Nearest Neighbors| Linear | \n| Naive Bayes      | Locally weighted linear    | \n| Support vector machines | Ridge      | \n| Decision trees | Lasso      | \n\n|  Unsupervised learning tasks |  | \n| ------------- |:-------------:| \n| k-Means| Expectation maximization | \n| DBSCAN      | Parzen window    | \n\n### Why you need know about machine learning\n\n### Why python is so great for machine learning\n\n","slug":"2019-07-01-ML-in-action-01","published":1,"updated":"2019-08-07T03:59:30.487Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqf10003z0ovvrf1w10g","content":"<h3><span id=\"a-brief-overview-of-machine-learning\"> A brief overview of machine learning</span></h3>\n<h3><span id=\"key-tasks-in-machine-learning\"> Key tasks in machine Learning</span></h3>\n<ul>\n<li>\n<p>Classification</p>\n</li>\n<li>\n<p>Regression</p>\n</li>\n</ul>\n<h3><span id=\"how-to-choose-a-right-algorithm\"> How to choose a right algorithm</span></h3>\n<table>\n<thead>\n<tr>\n<th>Supervised learning tasks</th>\n<th style=\"text-align:center\"></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k-Nearest Neighbors</td>\n<td style=\"text-align:center\">Linear</td>\n</tr>\n<tr>\n<td>Naive Bayes</td>\n<td style=\"text-align:center\">Locally weighted linear</td>\n</tr>\n<tr>\n<td>Support vector machines</td>\n<td style=\"text-align:center\">Ridge</td>\n</tr>\n<tr>\n<td>Decision trees</td>\n<td style=\"text-align:center\">Lasso</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Unsupervised learning tasks</th>\n<th style=\"text-align:center\"></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k-Means</td>\n<td style=\"text-align:center\">Expectation maximization</td>\n</tr>\n<tr>\n<td>DBSCAN</td>\n<td style=\"text-align:center\">Parzen window</td>\n</tr>\n</tbody>\n</table>\n<h3><span id=\"why-you-need-know-about-machine-learning\"> Why you need know about machine learning</span></h3>\n<h3><span id=\"why-python-is-so-great-for-machine-learning\"> Why python is so great for machine learning</span></h3>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"a-brief-overview-of-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#a-brief-overview-of-machine-learning\"></a> A brief overview of machine learning</h3>\n<h3 id=\"key-tasks-in-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#key-tasks-in-machine-learning\"></a> Key tasks in machine Learning</h3>\n<ul>\n<li>\n<p>Classification</p>\n</li>\n<li>\n<p>Regression</p>\n</li>\n</ul>\n<h3 id=\"how-to-choose-a-right-algorithm\"><a class=\"markdownIt-Anchor\" href=\"#how-to-choose-a-right-algorithm\"></a> How to choose a right algorithm</h3>\n<table>\n<thead>\n<tr>\n<th>Supervised learning tasks</th>\n<th style=\"text-align:center\"></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k-Nearest Neighbors</td>\n<td style=\"text-align:center\">Linear</td>\n</tr>\n<tr>\n<td>Naive Bayes</td>\n<td style=\"text-align:center\">Locally weighted linear</td>\n</tr>\n<tr>\n<td>Support vector machines</td>\n<td style=\"text-align:center\">Ridge</td>\n</tr>\n<tr>\n<td>Decision trees</td>\n<td style=\"text-align:center\">Lasso</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Unsupervised learning tasks</th>\n<th style=\"text-align:center\"></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>k-Means</td>\n<td style=\"text-align:center\">Expectation maximization</td>\n</tr>\n<tr>\n<td>DBSCAN</td>\n<td style=\"text-align:center\">Parzen window</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"why-you-need-know-about-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#why-you-need-know-about-machine-learning\"></a> Why you need know about machine learning</h3>\n<h3 id=\"why-python-is-so-great-for-machine-learning\"><a class=\"markdownIt-Anchor\" href=\"#why-python-is-so-great-for-machine-learning\"></a> Why python is so great for machine learning</h3>\n"},{"title":"机器学习优化方法","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-07-30T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["机器学习"],"_content":"\n> 机器学习算法 = 模型表征 + 模型评估 + 优化算法\n>\n> 无论何种类型的机器学习，最后都归结为求解最优化问题\n>\n> 求一个目标函数的极值=> 最优化问题\n\n\n### 数学模型\n#### 监督学习\n* 找到一个最佳的映射函数f(x)，使得对训练样本的损失函数最小化（最小化经验风险或结构风险)\n  $$\n  min_{W}\\frac{1}{N}\\sum_{i=1}^{N}L(W,x_{i},y_{i}) + \\lambda||W||_{2}^2\n  $$\n  N为训练样本数，L是对单个样本的损失函数，w是要求解的模型参数，是映射函数的参数 $x_{i}$为样本的特征向量, $y_{i}$ 为样本的标签值\n* 一个最优的概率密度函数p(x)，使得对训练样本的对数似然函数极大化（最大似然估计）\n  $$\n  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n  $$\n  $\\theta$ 是要求解的模型参数，是概率密度函数的参数。\n#### 非监督学习\n* 以聚类算法为例，算法要是的每个类的样本离类中心的距离之和最小化\n  $$\n    min_{S}\\sum_{i=1}^k\\sum_{x \\in S_{i}}||x-\\mu_{i}||^2\n  $$\n  k为类型数，x为样本向量， $\\mu_{i}$ 为类中心向量， $S_{i}$ 为第 $i$ 个类的样本集合\n* 强化学习， 要找到一个最优的策略，即状态s到动作a的映射函数（确定性策略，对于非确定性策略，是执行每个动作的概率）\n  $$\n    a=\\pi(s)\n  $$\n  任意给定一个状态，执行这个策略函数所确定的动作a之后，得到的累计回报最大化:\n  $$\n    max_{\\pi}V_{\\pi}(s)\n  $$\n  使用的是状态价值函数\n### 优化算法分类\n\n\n\n\n\n\n### 参考资料\n* [机器学习中的最优化算法总结](https://zhuanlan.zhihu.com/p/42689565)\n* [你想知道的特征工程，机器学习优化方法都在这了！收藏！](https://juejin.im/post/*5d3bb3c5e51d455d6d53591d#heading-7)\n* [机器学习中的优化方法](https://zhuanlan.zhihu.com/p/36196698)\n* [理解机器学习中常用优化方法](http://oath2yangmen.online/2018/01/29/%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/)","source":"_posts/2019-07-30-机器学习优化方法.md","raw":"---\ntitle: \"机器学习优化方法\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-07-30 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 优化方法\ncatagories:\n- 机器学习\n\n---\n\n> 机器学习算法 = 模型表征 + 模型评估 + 优化算法\n>\n> 无论何种类型的机器学习，最后都归结为求解最优化问题\n>\n> 求一个目标函数的极值=> 最优化问题\n\n\n### 数学模型\n#### 监督学习\n* 找到一个最佳的映射函数f(x)，使得对训练样本的损失函数最小化（最小化经验风险或结构风险)\n  $$\n  min_{W}\\frac{1}{N}\\sum_{i=1}^{N}L(W,x_{i},y_{i}) + \\lambda||W||_{2}^2\n  $$\n  N为训练样本数，L是对单个样本的损失函数，w是要求解的模型参数，是映射函数的参数 $x_{i}$为样本的特征向量, $y_{i}$ 为样本的标签值\n* 一个最优的概率密度函数p(x)，使得对训练样本的对数似然函数极大化（最大似然估计）\n  $$\n  max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n  $$\n  $\\theta$ 是要求解的模型参数，是概率密度函数的参数。\n#### 非监督学习\n* 以聚类算法为例，算法要是的每个类的样本离类中心的距离之和最小化\n  $$\n    min_{S}\\sum_{i=1}^k\\sum_{x \\in S_{i}}||x-\\mu_{i}||^2\n  $$\n  k为类型数，x为样本向量， $\\mu_{i}$ 为类中心向量， $S_{i}$ 为第 $i$ 个类的样本集合\n* 强化学习， 要找到一个最优的策略，即状态s到动作a的映射函数（确定性策略，对于非确定性策略，是执行每个动作的概率）\n  $$\n    a=\\pi(s)\n  $$\n  任意给定一个状态，执行这个策略函数所确定的动作a之后，得到的累计回报最大化:\n  $$\n    max_{\\pi}V_{\\pi}(s)\n  $$\n  使用的是状态价值函数\n### 优化算法分类\n\n\n\n\n\n\n### 参考资料\n* [机器学习中的最优化算法总结](https://zhuanlan.zhihu.com/p/42689565)\n* [你想知道的特征工程，机器学习优化方法都在这了！收藏！](https://juejin.im/post/*5d3bb3c5e51d455d6d53591d#heading-7)\n* [机器学习中的优化方法](https://zhuanlan.zhihu.com/p/36196698)\n* [理解机器学习中常用优化方法](http://oath2yangmen.online/2018/01/29/%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/)","slug":"2019-07-30-机器学习优化方法","published":1,"updated":"2019-08-07T06:14:51.230Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqf50006z0ov3ik1r9d9","content":"<blockquote>\n<p>机器学习算法 = 模型表征 + 模型评估 + 优化算法</p>\n<p>无论何种类型的机器学习，最后都归结为求解最优化问题</p>\n<p>求一个目标函数的极值=&gt; 最优化问题</p>\n</blockquote>\n<h3><span id=\"数学模型\"> 数学模型</span></h3>\n<h4><span id=\"监督学习\"> 监督学习</span></h4>\n<ul>\n<li>找到一个最佳的映射函数f(x)，使得对训练样本的损失函数最小化（最小化经验风险或结构风险)<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>W</mi></mrow></msub><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><mi>L</mi><mo>(</mo><mi>W</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mrow><mi>i</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub><mo>)</mo><mo>+</mo><mi>λ</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>W</mi><mi mathvariant=\"normal\">∣</mi><msubsup><mi mathvariant=\"normal\">∣</mi><mrow><mn>2</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">min_{W}\\frac{1}{N}\\sum_{i=1}^{N}L(W,x_{i},y_{i}) + \\lambda||W||_{2}^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.8283360000000002em;\"></span><span class=\"strut bottom\" style=\"height:3.106005em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">m</span><span class=\"mord mathit\">i</span><span class=\"mord\"><span class=\"mord mathit\">n</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">L</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mord mathit\">λ</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathrm\">∣</span><span class=\"mord\"><span class=\"mord mathrm\">∣</span><span class=\"vlist\"><span style=\"top:0.247em;margin-left:0em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span style=\"top:-0.4129999999999999em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\nN为训练样本数，L是对单个样本的损失函数，w是要求解的模型参数，是映射函数的参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>为样本的特征向量, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 为样本的标签值</li>\n<li>一个最优的概率密度函数p(x)，使得对训练样本的对数似然函数极大化（最大似然估计）max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n\n<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 是要求解的模型参数，是概率密度函数的参数。</li>\n</ul>\n<h4><span id=\"非监督学习\"> 非监督学习</span></h4>\n<ul>\n<li>以聚类算法为例，算法要是的每个类的样本离类中心的距离之和最小化<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>S</mi></mrow></msub><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><msub><mi>S</mi><mrow><mi>i</mi></mrow></msub></mrow></msub><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo>−</mo><msub><mi>μ</mi><mrow><mi>i</mi></mrow></msub><mi mathvariant=\"normal\">∣</mi><msup><mi mathvariant=\"normal\">∣</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">  min_{S}\\sum_{i=1}^k\\sum_{x \\in S_{i}}||x-\\mu_{i}||^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.8361130000000003em;\"></span><span class=\"strut bottom\" style=\"height:3.235449em;vertical-align:-1.399336em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">m</span><span class=\"mord mathit\">i</span><span class=\"mord\"><span class=\"mord mathit\">n</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1943359999999998em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">x</span><span class=\"mrel\">∈</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">S</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.05764em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord scriptscriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\">x</span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">μ</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span><span class=\"mord\"><span class=\"mord mathrm\">∣</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\nk为类型数，x为样本向量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>μ</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">μ</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 为类中心向量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>S</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">S</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.05764em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 为第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.65952em;\"></span><span class=\"strut bottom\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">i</span></span></span></span> 个类的样本集合</li>\n<li>强化学习， 要找到一个最优的策略，即状态s到动作a的映射函数（确定性策略，对于非确定性策略，是执行每个动作的概率）<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>π</mi><mo>(</mo><mi>s</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">  a=\\pi(s)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">a</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathit\">s</span><span class=\"mclose\">)</span></span></span></span></span></p>\n任意给定一个状态，执行这个策略函数所确定的动作a之后，得到的累计回报最大化:<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>π</mi></mrow></msub><msub><mi>V</mi><mrow><mi>π</mi></mrow></msub><mo>(</mo><mi>s</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">  max_{\\pi}V_{\\pi}(s)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">m</span><span class=\"mord mathit\">a</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.22222em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">s</span><span class=\"mclose\">)</span></span></span></span></span></p>\n使用的是状态价值函数</li>\n</ul>\n<h3><span id=\"优化算法分类\"> 优化算法分类</span></h3>\n<h3><span id=\"参考资料\"> 参考资料</span></h3>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/42689565\" target=\"_blank\" rel=\"noopener\">机器学习中的最优化算法总结</a></li>\n<li><a href=\"https://juejin.im/post/*5d3bb3c5e51d455d6d53591d#heading-7\" target=\"_blank\" rel=\"noopener\">你想知道的特征工程，机器学习优化方法都在这了！收藏！</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/36196698\" target=\"_blank\" rel=\"noopener\">机器学习中的优化方法</a></li>\n<li><a href=\"http://oath2yangmen.online/2018/01/29/%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/\" target=\"_blank\" rel=\"noopener\">理解机器学习中常用优化方法</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>机器学习算法 = 模型表征 + 模型评估 + 优化算法</p>\n<p>无论何种类型的机器学习，最后都归结为求解最优化问题</p>\n<p>求一个目标函数的极值=&gt; 最优化问题</p>\n</blockquote>\n<h3 id=\"数学模型\"><a class=\"markdownIt-Anchor\" href=\"#数学模型\"></a> 数学模型</h3>\n<h4 id=\"监督学习\"><a class=\"markdownIt-Anchor\" href=\"#监督学习\"></a> 监督学习</h4>\n<ul>\n<li>找到一个最佳的映射函数f(x)，使得对训练样本的损失函数最小化（最小化经验风险或结构风险)<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>W</mi></mrow></msub><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>N</mi></mrow></msubsup><mi>L</mi><mo>(</mo><mi>W</mi><mo separator=\"true\">,</mo><msub><mi>x</mi><mrow><mi>i</mi></mrow></msub><mo separator=\"true\">,</mo><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub><mo>)</mo><mo>+</mo><mi>λ</mi><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>W</mi><mi mathvariant=\"normal\">∣</mi><msubsup><mi mathvariant=\"normal\">∣</mi><mrow><mn>2</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">min_{W}\\frac{1}{N}\\sum_{i=1}^{N}L(W,x_{i},y_{i}) + \\lambda||W||_{2}^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.8283360000000002em;\"></span><span class=\"strut bottom\" style=\"height:3.106005em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">m</span><span class=\"mord mathit\">i</span><span class=\"mord\"><span class=\"mord mathit\">n</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">L</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mord mathit\">λ</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathrm\">∣</span><span class=\"mord\"><span class=\"mord mathrm\">∣</span><span class=\"vlist\"><span style=\"top:0.247em;margin-left:0em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span style=\"top:-0.4129999999999999em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\nN为训练样本数，L是对单个样本的损失函数，w是要求解的模型参数，是映射函数的参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>为样本的特征向量, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 为样本的标签值</li>\n<li>一个最优的概率密度函数p(x)，使得对训练样本的对数似然函数极大化（最大似然估计）max\\sum_{i=1}^l\\lnp(x_{i};\\theta)\n\n<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 是要求解的模型参数，是概率密度函数的参数。</li>\n</ul>\n<h4 id=\"非监督学习\"><a class=\"markdownIt-Anchor\" href=\"#非监督学习\"></a> 非监督学习</h4>\n<ul>\n<li>以聚类算法为例，算法要是的每个类的样本离类中心的距离之和最小化<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>i</mi><msub><mi>n</mi><mrow><mi>S</mi></mrow></msub><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><msub><mi>S</mi><mrow><mi>i</mi></mrow></msub></mrow></msub><mi mathvariant=\"normal\">∣</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo>−</mo><msub><mi>μ</mi><mrow><mi>i</mi></mrow></msub><mi mathvariant=\"normal\">∣</mi><msup><mi mathvariant=\"normal\">∣</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">  min_{S}\\sum_{i=1}^k\\sum_{x \\in S_{i}}||x-\\mu_{i}||^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.8361130000000003em;\"></span><span class=\"strut bottom\" style=\"height:3.235449em;vertical-align:-1.399336em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">m</span><span class=\"mord mathit\">i</span><span class=\"mord\"><span class=\"mord mathit\">n</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">S</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1943359999999998em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">x</span><span class=\"mrel\">∈</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">S</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.07142857142857144em;margin-left:-0.05764em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord scriptscriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\">x</span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">μ</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span><span class=\"mord\"><span class=\"mord mathrm\">∣</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\nk为类型数，x为样本向量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>μ</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mu_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">μ</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 为类中心向量， <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>S</mi><mrow><mi>i</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">S_{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">S</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.05764em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 为第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.65952em;\"></span><span class=\"strut bottom\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">i</span></span></span></span> 个类的样本集合</li>\n<li>强化学习， 要找到一个最优的策略，即状态s到动作a的映射函数（确定性策略，对于非确定性策略，是执行每个动作的概率）<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>a</mi><mo>=</mo><mi>π</mi><mo>(</mo><mi>s</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">  a=\\pi(s)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">a</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">π</span><span class=\"mopen\">(</span><span class=\"mord mathit\">s</span><span class=\"mclose\">)</span></span></span></span></span></p>\n任意给定一个状态，执行这个策略函数所确定的动作a之后，得到的累计回报最大化:<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>a</mi><msub><mi>x</mi><mrow><mi>π</mi></mrow></msub><msub><mi>V</mi><mrow><mi>π</mi></mrow></msub><mo>(</mo><mi>s</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">  max_{\\pi}V_{\\pi}(s)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\">m</span><span class=\"mord mathit\">a</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.22222em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">π</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">s</span><span class=\"mclose\">)</span></span></span></span></span></p>\n使用的是状态价值函数</li>\n</ul>\n<h3 id=\"优化算法分类\"><a class=\"markdownIt-Anchor\" href=\"#优化算法分类\"></a> 优化算法分类</h3>\n<h3 id=\"参考资料\"><a class=\"markdownIt-Anchor\" href=\"#参考资料\"></a> 参考资料</h3>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/42689565\" target=\"_blank\" rel=\"noopener\">机器学习中的最优化算法总结</a></li>\n<li><a href=\"https://juejin.im/post/*5d3bb3c5e51d455d6d53591d#heading-7\" target=\"_blank\" rel=\"noopener\">你想知道的特征工程，机器学习优化方法都在这了！收藏！</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/36196698\" target=\"_blank\" rel=\"noopener\">机器学习中的优化方法</a></li>\n<li><a href=\"http://oath2yangmen.online/2018/01/29/%E7%90%86%E8%A7%A3%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/\" target=\"_blank\" rel=\"noopener\">理解机器学习中常用优化方法</a></li>\n</ul>\n"},{"title":"特征工程","catalog":true,"toc_nav_num":true,"date":"2019-07-29T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["机器学习"],"_content":"\n> 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已\n> 实践中学习的手艺\n> More data beats clever algorithms, but better data beats more data\n\n### 特征工程包含以下几个方面：\n#### 特征使用方案\n* 要实现我们的目标需要哪些数据？\n  - 基于业务理解，尽可能找出对因变量有影响的所有自变量\n* 可用性评估\n  - 获取难度\n  - 覆盖率\n  - 准确率\n\n#### 特征获取方案\n* 如何获取这些特征？\n* 如何存储？\n\n#### 特征处理\n* 特征特征清洗\n  - 清洗异样样本\n  - 采样\n    + 数据不均衡\n    + 样本权重\n* 预处理\n  - 单个特征\n    + 归一化\n    + 离散化\n    + Dummy Coding\n    + 缺失值\n    + 数据变换\n      - log\n      - 指数\n      - Box-Cox\n  - 多个特征\n    + Filter\n      - 思路：自变量和目标变量之间的关联\n      - 相关系数\n      - 卡方检验\n      - 信息增益、 互信息\n    + Wrapper\n      - 思路: 通过目标函数（AUC/MSE）来决定是否加入一个变量\n      - 迭代：产生特征子集，评价\n        + 完全搜索\n        + 启发式搜索\n        + 随机搜索\n          - GA\n          - SA\n    + Embedded\n      - 思路：学习期自身自动选择特征\n      - 正则化\n        + L1 -- Lasso\n        + L2 -- Ridge\n      - 决策树 -- 熵、信息增益\n      - 深度学习\n  - 衍生变量 -- 对原始数据加工，生成有商业意义的变量\n\n#### 特征监控\n* 特征有效性分析\n* 特征监控\n\n### 最基本的特征工程方法\n#### 类别特征\n  * 特点\n    - 几乎总是需要一些处理\n    - 高基数可以创建非常稀疏的数据\n    - 很难插补缺失数据\n  * 方法\n    - 独热编码（One-Hot Encoding）\n      + 概念\n        - 在长度为K的数组上做 one-of-k 编码\n        - 基本方法： 与大多数线性算法一起使用\n        - 删除第一列可以避免多重共线性\n        - 稀疏格式对内存友好\n        - 当前大多数实现都没有处理缺失，看不见的数据\n      + 例子\n        - ['BR']\n          | 国家  =>|国家=NL  | 国家=BR  | 国家=US  |\n          |---|---|---|---|---|---|\n          |  NL | |||\n          |  BR | [   0|   1|   0]|\n          |  US |   |||\n        - 编码集：[0, 1, 0]\n        - 编码稀疏： 2:1\n    - 散列编码（Hash encoding)\n      + 概念\n        - 独热编码是否具有固定长度的数组？\n        - 避免极其稀疏的数据\n        - 可能会引入碰撞\n        - 可以重复的使用不同的哈希函数和包获得小的精度\n        - 碰撞通常会降低结果，但可能会改善结果\n        - 优雅地处理新变量（例如：新用户代理）\n      + 例子\n        - ['BR']\n        - hash('BR') =>\n          | 国家  | hash1  | hash2 | hash3  | hash4| hash5|\n          |---|---|---|---|---|---|\n          |  NL |   |   |   |   ||\n          |  BR |[   0|   1|   0|   0| 0]|\n          |  US |   |   |   |   ||\n        - 编码集：[0, 1, 0, 0, 0]\n        - 编码稀疏： 2:1\n\n    - 标签编码（Label encoding）\n      + 概念\n        - 为每个类别变量提供唯一的数字ID\n        - 对于非线性基于树的算法很有用\n        - 不增加维度\n        - 随机化类别变量到数值变量的映射并重新训练， 平均以获得精度较小的凸凹\n      + 例子\n        - ['Queenstown']\n          | 城市  =>| city  |\n          |---|---|\n          |  Cherbourg |  1 |\n          |  Queenstown |  2 |\n          |  Southhampton |  3 |\n        - 编码： [2]\n    - 计数编码（Count encoding）\n      + 概念\n        - 将类别变量替换为他们在训练数据中的计数\n        - 适用于线性和非线性算法\n        - 可以对异常值敏感\n        - 可以加入对数转换（log transform）, 与计数一起工作的很好\n        - 用‘1’替换看不见的数据\n        - 可能会发生冲突：相同的编码，不同的变量\n      + 例子\n        - ['A6GHBD78']\n          | teacher_id  |   teacher_id|\n          |---|---|---|\n          |  DEADB33F |   4|\n          |  A6GHBD78|   3|\n          |  DEADB33F |   4|\n          |  FCKGWRHQ |   1|\n          |  DEADB33F |   4|\n          |  A6GHBD78|   3|\n          |  A6GHBD78|   3|\n          |  DEADB33F |   4|\n        - 编码： [3]\n    - 标签计数编码（LabelCount encoding）\n      + 概念\n        - 在训练数据中，对类别变量进行排名\n        - 适用于线性和非线性算法\n        - 对异常值不敏感\n        - 不会对不同的变量赋予相同的编码\n        - 两全其美\n      + 例子\n        - ['nl']\n          | tld  |   tld|\n          |---|---|---|\n          |  nl |   3|\n          |  nl|   3|\n          |  nl |   3|\n          |  nl |   3|\n          |  de |   2|\n          |  de|   2|\n          |  fr|   1|\n          |  fr |   1|\n        - 编码： [3]\n    - Target encoding\n      + 概念\n        - 根据目标比率对类别变量进行编码（二进制分类或回归）\n        - 小心避免过度拟合（overfit）\n        - 堆叠的形式：单变量模型，其输出平均目标\n        - 以交叉验证方式进行\n        - 添加平滑以避免将变量编码设置为0\n        - 添加随机噪音以对抗过拟合\n        - 什么时候用：线性和非线性的最佳编码\n      + 例子\n        | role  |  y |  role |\n        |---|---|---|---|---|\n        |  manager |  1 |  0.5 |\n        |  engineer |  1 |   0.66|\n        |  scientist |  1 |   1.|\n        | manager| 0|0.5|\n        | engineer|0|0.66|\n        | engineer|1|0.66|\n\n    - 类别嵌入（Category Embedding）\n      + 概念\n        - 使用神经网络从类别变量创建密集嵌入\n        - 将函数逼近问题中的类别变量映射到欧几里德空间\n        - 更快的模型训练\n        - 减少内存开销\n        - 可以提供比1-hot 编码更好的精准度\n      + 例子\n        | role  |  role 3-D embedding |\n        |---|---|\n        |  manager |  [0.05, 0.10, 0.96] |\n        |  engineer | [0.72, 0.66, 0.17]|\n        |  scientist |  [0.75, 0.62, 0.15]|\n        | manager| [0.05, 0.10, 0.96]|\n        | engineer|[0.72, 0.66, 0.17]|\n        | engineer|[0.72, 0.66, 0.17]|\n\n    - NaN编码（NaN encoding）\n      + 概念\n        - 为NaN值提供显式编码而不是忽略\n        - NaN值可以保存信息\n        - 小心避免过度拟合\n        - 仅在训练数据和测试数据中的Nan值是由相同的原因引起时才可以用，或者本地验证证明它持有信息\n      + 例子\n        - ['NaN']\n          | UA | UA=mobile| UA=tablet| UA=NaN|\n          |---|---|---|---|\n          |mobile| |||\n          |tablet||||\n          |mobile||||\n          |NaN|0|0|1|\n          |mobile||||\n        - 编码： [0, 0, 1]\n\n    - 多项式编码（Polynomial encoding）\n      + 概念\n        - 类别变量之间的编码交互\n        - 没有交互的线性算法无法解决XOR问题\n        - 多项式内核可以解决XOR问题\n        - 爆炸式的特征空间： 使用FS，散列或者VW\n      + 例子\n        |A  |B  |y  | A=1*B=1| A=0*B=1| A=1*B=0 | A=0*B=0| y|\n        |---|---|---|---|---|---|---|---|\n        |1  |1  | 1 | 1|   0|  0|0|1 |\n        |0  |1  | 0 | 0|   1|  0|0|0 |\n        |1  |0  | 0 | 0|   0|  1|0|0 |\n        |0  |0  | 1 | 0|   0|  0|1|1 |\n    - 扩展编码（Expansion encoding）\n      + 概念\n        - 从单个变量创建多个类别变量\n        - 某些高基数功能（如用户代理）在其中包含更多信息：\n          + is_mobile?\n          + is_latest_version?\n          + Operation_system\n          + Browser_build\n          + etc...\n      + 例子\n        - Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\n          ==>\n          |UA1|UA2|UA3|UA4|UA5|\n          |---|---|---|---|---|\n          |Chrome|53.0.2785.143|Desktop|Mac|10_10_4|\n    - 合并编码（Consolidation encoding）\n      + 概念\n        - 将不同的类别变量映射到同一个变量\n        - 拼写错误，略有不同的职位描述，全名 vs 缩写\n        - 真实数据很乱，自由文本尤其如此\n      + 例子\n        |company_desc   =>| desc1| company_desc2|\n        |---------------|------|--------------|\n        |Shell          | Shell|Gas station|\n        |shel           | Shell|Gas station|\n        |SHELL          | Shell|Gas station|\n        |Shell Gasonline| Shell|Gas station|\n        |BP             | BP| Gas station|\n        |British Petr. | BP | Gas station|\n        |B&P           | BP | Gas station|\n        |BP Gas Station| BP| Gas station|\n        |bp            | BP | Gas station|\n        |Procter&Gamble| P&G| Manufacturer|\n#### 数值特征\n* 特点\n  - 可以更容易地输入算法\n  - 可以构成浮点数，计数，数字\n  - 更容易估算缺失的数据\n* 方法\n  - 四舍五入（Rounding）\n    + 概念\n      - 四舍五入数值变量\n      - 有损压缩： 保留数据的最重要特征\n      - 有时太精确只是噪音\n      - 四舍五入的变量可以视为类别变量\n      - 可以在四舍五入之前做对数变换\n    + 例子\n      |age | age1|age2|\n      |---|---|---|\n      |23.6671| 23|2|\n      |23.8891|23|2|\n      |22.1261|22|2|\n      |19.5506|19|1|\n      |18.2114|18|1|\n  - 分档（Binning）\n    + 概念\n      - 将数值变量放入bin（档）中并使用bin-ID进行编码\n      - 分档可以务实设置，通过分位数，均匀的，或使用模型到最佳分档\n      - 可以使得训练集所见范围以外的变量正常工作\n    + 例子\n      |risk_score| rs[-inf, 33]| rs[33,66]|rs[66,inf]|\n      |---|---|---|---|\n      |15| 1|0|0|\n      |77|0|0|1|\n      |78|0|0|1|\n      |55|0|1|0|\n      |42|0|1|0|\n  - 缩放（Scaling）\n    + 概念\n      - 将数值变量扩展到特定范围\n      - Standard（Z）缩放\n      - MinMax 缩放\n      - Root 缩放\n      - Log 缩放\n  - 估算（Imputation）\n    + 概念\n      - 估算缺失的变量\n      - 硬编码可与估算相结合\n      - 平均：非常基本的\n      - 中位数：对异常值鲁棒性比较高\n      - 忽略： 只是推迟问题\n      - 使用模型：可以暴露算法的偏差\n    + 例子\n      |wage|hours|gender|y => | wage|hours|gender_y|\n      |---|---|---|---|---|---|---|\n      |1600|40|0|1|1600|40|0|\n      |2200|50|1|1|2200|50|1|\n      |1800|36|0|0|1800|36|0|\n      |2100|45|1|0|2100|45|?|\n      |2050|60|NaN|0|2050|60|?|\n      |1650|36|0|1|1650|36|?|\n  - 相互作用（Interactions）\n    + 概念\n      - 具体来说编码数值变量之间的相互作用\n      - 尝试：减法，加法，乘法，除法\n      - 使用：通过统 计测试选择特征，或训练模型特征重要性\n      - 忽略：人的直觉; 奇怪的相互作用可以带来显著的改进\n  - 线性算法的非线性编码（No-linear encoding for linear algo's）\n    + 概念\n      - 硬编码非线性以改进线性算法\n      - 多项式内核\n      - 叶编码（随机森林嵌入）\n      - 遗传算法\n      - 局部线性嵌入，光谱嵌入，t-SNE\n  - 行统计（Row statistics）\n    + 概念\n      - 创建一行数据的统计信息\n      - NaN的数量\n      - 0的数量\n      - 负数值的数量\n      - 平均值，最大值，最小值，偏度 等\n#### 时间特征 (Temporal variables)\n* 特点\n  - 时间变量，如日期，需要更好的本地验证方案（如回测）\n  - 容易在这里犯错误\n  - 很多机会进行重大改进\n* 方法\n  - 投射到一个圆\n    + 将单个要素（如day_of_week）转换为圆上的两个坐标\n    + 确保max和min之间的距离与min和min +1相同\n    + 用于day_of_week，day_of_month，hour_of_day等\n  - 趋势线（Trendlines）\n    + 不是编码总支出，而是编码上周花费，上个月花费，去年花费。\n    + 给算法的趋势：两个支出相同的客户可能会有截然不同的行为 - 一个客户可能开始花费更多，而另一个客户开始减少支出。\n  - 与重大事件的接近度（Closeness to major events）\n    + 硬编码类别特征，如：date_3_days_before_holidays = 1\n    + 尝试：国庆节，重大体育赛事，周末，一个月的第一个星期六等\n    + 这些因素可能对支出行为产生重大影响\n\n#### 空间特征（Spatial Variables）\n* 特点\n  - 空间变量是在空间编码的位置的变量\n  - 例子包括：GPS坐标，城市，国家，地址\n* 方法\n  - 分类位置（Categorizing location）\n    + 克里金法（Kriging）\n    + K-means聚类\n    + 原始纬度经度\n    + 将城市转换为纬度经度\n    + 将邮政编码添加到街道名称\n  - 与枢纽的接近度（Closeness to Hubs）\n    + 找到位置与主要枢纽之间的紧密程度\n    + 小城镇继承了附近大城市的一些文化/背景\n    + 电话位置可以映射到附近的企业和超市\n  - 空间欺诈行为（Spatial fraudulent behavior）\n    + 位置事件数据可以指示可疑行为\n    + 不可能的旅行速度：不同国家的多个同步交易\n    + 在不同的城镇消费，而不是在家或送货地址\n    + 永远不要在同一地点消费\n  - 探索（Exploration）\n    + 数据探索可以发现数据健康问题，异常值，噪声，特征工程想法，特征清洗想法\n    + 可以使用：Console, Notebook, Pandas\n    + 尝试简单的统计数据：最小值，最大值\n    + 将目标结合起来，以便在信息之间找到相关性\n  - 迭代/调试（Iteration / Debugging）\n    + 特征工程是一个迭代过程：使你的pipelines适合快速迭代\n    + 使用子线性调试：输出有关过程的中间信息，进行伪log\n    + 使用允许快速实验的工具\n    + 失败的想法会多余成功的想法\n  - 标签工程（Label Engineering）\n    + 可以将标签/目标/因变量视为数据的一个特征，反之亦然\n    + 对数变换：y  - > log（y + 1）| exp（y_pred） -  1\n    + Square 变换\n    + Box-Cox变换\n    + 创建分数，在回归中转换二进制目标\n    + 训练回归器以预测测试集中不可用的特征\n\n#### 自然语言处理（Natural Language Processing）\n* 特点\n  - 可以用类别特征中相同的方法\n  - 深度学习（自动特征工程）正在慢慢的吞噬这个领域，但是具有好的特征工程的浅学习仍然具有竞争力\n  - 高稀疏性的数据将带来“维度诅咒”\n  - 特征工程具有很多机会\n* 方法\n  - 所有方法列表\n    + 转换成小写字母\n    + 删除非字母数字\n    + 修复\n    + 编码标点符号\n    + 符号化\n    + 令牌克（Token-grams）\n    + skipgrams\n    + char-grams\n    + 删除停用词\n    + 删除罕见的单词\n    + 非常常见的词\n    + 拼写纠正\n    + 砍字\n    + 词干\n    + 词形还原\n    + 文档特征\n    + 实体的插入和提取\n    + 简化\n    + Word2Vec 和 GloVe / Doc2Vec\n    + 字符串相似性\n    + 阅读水平\n    + 最近邻居\n    + TF*IDF\n    + BayesSVM，矢量化，LDA，LSA\n  - 清洗（Cleaning）\n    + 转换成小写字母：使标记独立于大写：“I work at NASA” -> “i work at nasa”.\n    + 转换成Unidecode：将字符转换为ascii-对应物：\n    + 删除非字母数字：删除不在[a-z] [A-Z] [0-9]中的任何内容来清理文本，“Breaking! Amsterdam (2009)” -> “Breaking Amsterdam 2009”\n    + 修复：修复编码问题或修剪intertoken空格。 “C a s a C a f＆eacute;” - >“CasaCafé”\n  - 符号化（Tokenizing）\n    + 编码标点符号：硬编码“！”和“？”作为标记。\n    + 符号化（Tokenize）：划分句子标记成单词记号\n    + N-Grams：将连续的符号编码为符号,  “I like the Beatles” -> [“I like”, “like the”, “the Beatles”]\n    + Skip-grams: 编码连续的符号，但跳过一些,  “I like the Beatles” -> [“I the”, “like Beatles”]\n    + Char-grams: 与N-gram相同，但字符级别, “Beatles” - > [“Bea”, “eat”, “atl”, “tle”, “les”]\n    + Affixes：与char-gram相同，但是仅限于前缀与后缀\n  - 删除（Removing）\n    + 停用词：删除停用词列表中出现的单词/标记\n    + 稀有单词：删除仅在训练集中出现几次的单词\n    + 常用词：删除可能不在停用词列表中的极其常见的词\n  - 根（Roots）\n    + 拼写纠正：将字符更改为正确的拼写\n    + 切（chop）： 仅取一个单词的前n（8）个字符\n    + 词干：将词/标记减少到其根， “cars” -> “car”\n    + Lemmatize：找到语义根， “never be late” -> “never are late”\n  - 丰富（Enrich）\n    + 文档特征：计算空格数量，tab数量，换行数量，字符数量，以及令牌数量等\n    + 实体插入：向文本中插入更多的通用的规范， “Microsoft releases Windows” -> “Microsoft (company) releases Windows (application)”\n    + 解析树：将句子解析为逻辑形式，“Alice hits Bill” -> Alice/Noun_subject hits/Verb Bill/Noun_object.\n    + 阅读级别：计算文档的阅读级别\n  - 相似性（Similarities）\n    + 令牌相似性：计算出现在两个文本中的令牌数\n    + 压缩距离：查看是否可以使用其他文本更好地压缩一个文本\n    + Levenshtein / Hamming / Jaccard 距离：通过查看在另一个字符串中转换一个字符串所需的操作数来检查两个字符串之间的相似性\n    + Word2Vec / Glove：检查两个平均向量之间的余弦相似度\n  - TF-IDF\n    + 术语频率：减少对长文档的偏差\n    + 反向文档频率：减少对常见令牌的偏差\n    + TF-IDF：用于标识文档中最重要的标记，删除不重要的标记，或作为降维的预处理步骤\n  - 降维\n    + PCA：将文本缩小为50或100维向量\n    + SVD：将文本缩小为50或100维向量\n    + LDA：TF-IDF，然后是SVD\n    + LSA：创建主题向量\n  - 外部模型\n    + 情绪分析器：为任何文本获取负面或正面情绪的向量\n    + 主题模型：使用另一个数据集为新数据集创建主题向量\n#### 深度学习/神经网络\n* 神经网络声称是端到端的自动特征工程。所以特征工程没用了么？并不是这样的。特征工程将重点转移到架构工程。\n* 尽管承诺：计算机视觉使用的特征， 例如：\n  - HOG\n  - SIFT\n  - whitening\n  - perturbation\n  - image pyramids\n  - rotation\n  - z-scaling\n  - log-scaling\n  - frame- grams\n  - external semantic data\n  - ...\n\n#### Leakage / Golden Features\n* 特征工程可以帮助利用泄露\n* 逆向工程\n  - 使用rainbow表反转 MD5 Hash\n  - 将 TF-IDF反转回术语频率\n  - 编码样本数据集的顺序\n  - 编码文件创建日期\n* 规则挖掘\n  - 查找简单的规则（并对其进行编码）以帮助您的模型\n\n<!-- $$\nf(n) = \\begin{cases}\n \\frac{n}{2},\n & \\text{if } n\\text{ is even}\n \\\\ 3n+1, & \\text{if } n\\text{ is odd}\n \\end{cases}\n$$ -->\n\n\n### 参考资料\n[Tips & Tricks for Feature Engineering / Applied Machine Learning](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750)\n[]","source":"_posts/2019-07-29-特征工程.md","raw":"---\ntitle: \"特征工程\"\ncatalog: true\ntoc_nav_num: true\n# mathjax: true\ndate: 2019-07-29 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 特征工程\ncatagories:\n- 机器学习\n\n---\n\n> 数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已\n> 实践中学习的手艺\n> More data beats clever algorithms, but better data beats more data\n\n### 特征工程包含以下几个方面：\n#### 特征使用方案\n* 要实现我们的目标需要哪些数据？\n  - 基于业务理解，尽可能找出对因变量有影响的所有自变量\n* 可用性评估\n  - 获取难度\n  - 覆盖率\n  - 准确率\n\n#### 特征获取方案\n* 如何获取这些特征？\n* 如何存储？\n\n#### 特征处理\n* 特征特征清洗\n  - 清洗异样样本\n  - 采样\n    + 数据不均衡\n    + 样本权重\n* 预处理\n  - 单个特征\n    + 归一化\n    + 离散化\n    + Dummy Coding\n    + 缺失值\n    + 数据变换\n      - log\n      - 指数\n      - Box-Cox\n  - 多个特征\n    + Filter\n      - 思路：自变量和目标变量之间的关联\n      - 相关系数\n      - 卡方检验\n      - 信息增益、 互信息\n    + Wrapper\n      - 思路: 通过目标函数（AUC/MSE）来决定是否加入一个变量\n      - 迭代：产生特征子集，评价\n        + 完全搜索\n        + 启发式搜索\n        + 随机搜索\n          - GA\n          - SA\n    + Embedded\n      - 思路：学习期自身自动选择特征\n      - 正则化\n        + L1 -- Lasso\n        + L2 -- Ridge\n      - 决策树 -- 熵、信息增益\n      - 深度学习\n  - 衍生变量 -- 对原始数据加工，生成有商业意义的变量\n\n#### 特征监控\n* 特征有效性分析\n* 特征监控\n\n### 最基本的特征工程方法\n#### 类别特征\n  * 特点\n    - 几乎总是需要一些处理\n    - 高基数可以创建非常稀疏的数据\n    - 很难插补缺失数据\n  * 方法\n    - 独热编码（One-Hot Encoding）\n      + 概念\n        - 在长度为K的数组上做 one-of-k 编码\n        - 基本方法： 与大多数线性算法一起使用\n        - 删除第一列可以避免多重共线性\n        - 稀疏格式对内存友好\n        - 当前大多数实现都没有处理缺失，看不见的数据\n      + 例子\n        - ['BR']\n          | 国家  =>|国家=NL  | 国家=BR  | 国家=US  |\n          |---|---|---|---|---|---|\n          |  NL | |||\n          |  BR | [   0|   1|   0]|\n          |  US |   |||\n        - 编码集：[0, 1, 0]\n        - 编码稀疏： 2:1\n    - 散列编码（Hash encoding)\n      + 概念\n        - 独热编码是否具有固定长度的数组？\n        - 避免极其稀疏的数据\n        - 可能会引入碰撞\n        - 可以重复的使用不同的哈希函数和包获得小的精度\n        - 碰撞通常会降低结果，但可能会改善结果\n        - 优雅地处理新变量（例如：新用户代理）\n      + 例子\n        - ['BR']\n        - hash('BR') =>\n          | 国家  | hash1  | hash2 | hash3  | hash4| hash5|\n          |---|---|---|---|---|---|\n          |  NL |   |   |   |   ||\n          |  BR |[   0|   1|   0|   0| 0]|\n          |  US |   |   |   |   ||\n        - 编码集：[0, 1, 0, 0, 0]\n        - 编码稀疏： 2:1\n\n    - 标签编码（Label encoding）\n      + 概念\n        - 为每个类别变量提供唯一的数字ID\n        - 对于非线性基于树的算法很有用\n        - 不增加维度\n        - 随机化类别变量到数值变量的映射并重新训练， 平均以获得精度较小的凸凹\n      + 例子\n        - ['Queenstown']\n          | 城市  =>| city  |\n          |---|---|\n          |  Cherbourg |  1 |\n          |  Queenstown |  2 |\n          |  Southhampton |  3 |\n        - 编码： [2]\n    - 计数编码（Count encoding）\n      + 概念\n        - 将类别变量替换为他们在训练数据中的计数\n        - 适用于线性和非线性算法\n        - 可以对异常值敏感\n        - 可以加入对数转换（log transform）, 与计数一起工作的很好\n        - 用‘1’替换看不见的数据\n        - 可能会发生冲突：相同的编码，不同的变量\n      + 例子\n        - ['A6GHBD78']\n          | teacher_id  |   teacher_id|\n          |---|---|---|\n          |  DEADB33F |   4|\n          |  A6GHBD78|   3|\n          |  DEADB33F |   4|\n          |  FCKGWRHQ |   1|\n          |  DEADB33F |   4|\n          |  A6GHBD78|   3|\n          |  A6GHBD78|   3|\n          |  DEADB33F |   4|\n        - 编码： [3]\n    - 标签计数编码（LabelCount encoding）\n      + 概念\n        - 在训练数据中，对类别变量进行排名\n        - 适用于线性和非线性算法\n        - 对异常值不敏感\n        - 不会对不同的变量赋予相同的编码\n        - 两全其美\n      + 例子\n        - ['nl']\n          | tld  |   tld|\n          |---|---|---|\n          |  nl |   3|\n          |  nl|   3|\n          |  nl |   3|\n          |  nl |   3|\n          |  de |   2|\n          |  de|   2|\n          |  fr|   1|\n          |  fr |   1|\n        - 编码： [3]\n    - Target encoding\n      + 概念\n        - 根据目标比率对类别变量进行编码（二进制分类或回归）\n        - 小心避免过度拟合（overfit）\n        - 堆叠的形式：单变量模型，其输出平均目标\n        - 以交叉验证方式进行\n        - 添加平滑以避免将变量编码设置为0\n        - 添加随机噪音以对抗过拟合\n        - 什么时候用：线性和非线性的最佳编码\n      + 例子\n        | role  |  y |  role |\n        |---|---|---|---|---|\n        |  manager |  1 |  0.5 |\n        |  engineer |  1 |   0.66|\n        |  scientist |  1 |   1.|\n        | manager| 0|0.5|\n        | engineer|0|0.66|\n        | engineer|1|0.66|\n\n    - 类别嵌入（Category Embedding）\n      + 概念\n        - 使用神经网络从类别变量创建密集嵌入\n        - 将函数逼近问题中的类别变量映射到欧几里德空间\n        - 更快的模型训练\n        - 减少内存开销\n        - 可以提供比1-hot 编码更好的精准度\n      + 例子\n        | role  |  role 3-D embedding |\n        |---|---|\n        |  manager |  [0.05, 0.10, 0.96] |\n        |  engineer | [0.72, 0.66, 0.17]|\n        |  scientist |  [0.75, 0.62, 0.15]|\n        | manager| [0.05, 0.10, 0.96]|\n        | engineer|[0.72, 0.66, 0.17]|\n        | engineer|[0.72, 0.66, 0.17]|\n\n    - NaN编码（NaN encoding）\n      + 概念\n        - 为NaN值提供显式编码而不是忽略\n        - NaN值可以保存信息\n        - 小心避免过度拟合\n        - 仅在训练数据和测试数据中的Nan值是由相同的原因引起时才可以用，或者本地验证证明它持有信息\n      + 例子\n        - ['NaN']\n          | UA | UA=mobile| UA=tablet| UA=NaN|\n          |---|---|---|---|\n          |mobile| |||\n          |tablet||||\n          |mobile||||\n          |NaN|0|0|1|\n          |mobile||||\n        - 编码： [0, 0, 1]\n\n    - 多项式编码（Polynomial encoding）\n      + 概念\n        - 类别变量之间的编码交互\n        - 没有交互的线性算法无法解决XOR问题\n        - 多项式内核可以解决XOR问题\n        - 爆炸式的特征空间： 使用FS，散列或者VW\n      + 例子\n        |A  |B  |y  | A=1*B=1| A=0*B=1| A=1*B=0 | A=0*B=0| y|\n        |---|---|---|---|---|---|---|---|\n        |1  |1  | 1 | 1|   0|  0|0|1 |\n        |0  |1  | 0 | 0|   1|  0|0|0 |\n        |1  |0  | 0 | 0|   0|  1|0|0 |\n        |0  |0  | 1 | 0|   0|  0|1|1 |\n    - 扩展编码（Expansion encoding）\n      + 概念\n        - 从单个变量创建多个类别变量\n        - 某些高基数功能（如用户代理）在其中包含更多信息：\n          + is_mobile?\n          + is_latest_version?\n          + Operation_system\n          + Browser_build\n          + etc...\n      + 例子\n        - Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\n          ==>\n          |UA1|UA2|UA3|UA4|UA5|\n          |---|---|---|---|---|\n          |Chrome|53.0.2785.143|Desktop|Mac|10_10_4|\n    - 合并编码（Consolidation encoding）\n      + 概念\n        - 将不同的类别变量映射到同一个变量\n        - 拼写错误，略有不同的职位描述，全名 vs 缩写\n        - 真实数据很乱，自由文本尤其如此\n      + 例子\n        |company_desc   =>| desc1| company_desc2|\n        |---------------|------|--------------|\n        |Shell          | Shell|Gas station|\n        |shel           | Shell|Gas station|\n        |SHELL          | Shell|Gas station|\n        |Shell Gasonline| Shell|Gas station|\n        |BP             | BP| Gas station|\n        |British Petr. | BP | Gas station|\n        |B&P           | BP | Gas station|\n        |BP Gas Station| BP| Gas station|\n        |bp            | BP | Gas station|\n        |Procter&Gamble| P&G| Manufacturer|\n#### 数值特征\n* 特点\n  - 可以更容易地输入算法\n  - 可以构成浮点数，计数，数字\n  - 更容易估算缺失的数据\n* 方法\n  - 四舍五入（Rounding）\n    + 概念\n      - 四舍五入数值变量\n      - 有损压缩： 保留数据的最重要特征\n      - 有时太精确只是噪音\n      - 四舍五入的变量可以视为类别变量\n      - 可以在四舍五入之前做对数变换\n    + 例子\n      |age | age1|age2|\n      |---|---|---|\n      |23.6671| 23|2|\n      |23.8891|23|2|\n      |22.1261|22|2|\n      |19.5506|19|1|\n      |18.2114|18|1|\n  - 分档（Binning）\n    + 概念\n      - 将数值变量放入bin（档）中并使用bin-ID进行编码\n      - 分档可以务实设置，通过分位数，均匀的，或使用模型到最佳分档\n      - 可以使得训练集所见范围以外的变量正常工作\n    + 例子\n      |risk_score| rs[-inf, 33]| rs[33,66]|rs[66,inf]|\n      |---|---|---|---|\n      |15| 1|0|0|\n      |77|0|0|1|\n      |78|0|0|1|\n      |55|0|1|0|\n      |42|0|1|0|\n  - 缩放（Scaling）\n    + 概念\n      - 将数值变量扩展到特定范围\n      - Standard（Z）缩放\n      - MinMax 缩放\n      - Root 缩放\n      - Log 缩放\n  - 估算（Imputation）\n    + 概念\n      - 估算缺失的变量\n      - 硬编码可与估算相结合\n      - 平均：非常基本的\n      - 中位数：对异常值鲁棒性比较高\n      - 忽略： 只是推迟问题\n      - 使用模型：可以暴露算法的偏差\n    + 例子\n      |wage|hours|gender|y => | wage|hours|gender_y|\n      |---|---|---|---|---|---|---|\n      |1600|40|0|1|1600|40|0|\n      |2200|50|1|1|2200|50|1|\n      |1800|36|0|0|1800|36|0|\n      |2100|45|1|0|2100|45|?|\n      |2050|60|NaN|0|2050|60|?|\n      |1650|36|0|1|1650|36|?|\n  - 相互作用（Interactions）\n    + 概念\n      - 具体来说编码数值变量之间的相互作用\n      - 尝试：减法，加法，乘法，除法\n      - 使用：通过统 计测试选择特征，或训练模型特征重要性\n      - 忽略：人的直觉; 奇怪的相互作用可以带来显著的改进\n  - 线性算法的非线性编码（No-linear encoding for linear algo's）\n    + 概念\n      - 硬编码非线性以改进线性算法\n      - 多项式内核\n      - 叶编码（随机森林嵌入）\n      - 遗传算法\n      - 局部线性嵌入，光谱嵌入，t-SNE\n  - 行统计（Row statistics）\n    + 概念\n      - 创建一行数据的统计信息\n      - NaN的数量\n      - 0的数量\n      - 负数值的数量\n      - 平均值，最大值，最小值，偏度 等\n#### 时间特征 (Temporal variables)\n* 特点\n  - 时间变量，如日期，需要更好的本地验证方案（如回测）\n  - 容易在这里犯错误\n  - 很多机会进行重大改进\n* 方法\n  - 投射到一个圆\n    + 将单个要素（如day_of_week）转换为圆上的两个坐标\n    + 确保max和min之间的距离与min和min +1相同\n    + 用于day_of_week，day_of_month，hour_of_day等\n  - 趋势线（Trendlines）\n    + 不是编码总支出，而是编码上周花费，上个月花费，去年花费。\n    + 给算法的趋势：两个支出相同的客户可能会有截然不同的行为 - 一个客户可能开始花费更多，而另一个客户开始减少支出。\n  - 与重大事件的接近度（Closeness to major events）\n    + 硬编码类别特征，如：date_3_days_before_holidays = 1\n    + 尝试：国庆节，重大体育赛事，周末，一个月的第一个星期六等\n    + 这些因素可能对支出行为产生重大影响\n\n#### 空间特征（Spatial Variables）\n* 特点\n  - 空间变量是在空间编码的位置的变量\n  - 例子包括：GPS坐标，城市，国家，地址\n* 方法\n  - 分类位置（Categorizing location）\n    + 克里金法（Kriging）\n    + K-means聚类\n    + 原始纬度经度\n    + 将城市转换为纬度经度\n    + 将邮政编码添加到街道名称\n  - 与枢纽的接近度（Closeness to Hubs）\n    + 找到位置与主要枢纽之间的紧密程度\n    + 小城镇继承了附近大城市的一些文化/背景\n    + 电话位置可以映射到附近的企业和超市\n  - 空间欺诈行为（Spatial fraudulent behavior）\n    + 位置事件数据可以指示可疑行为\n    + 不可能的旅行速度：不同国家的多个同步交易\n    + 在不同的城镇消费，而不是在家或送货地址\n    + 永远不要在同一地点消费\n  - 探索（Exploration）\n    + 数据探索可以发现数据健康问题，异常值，噪声，特征工程想法，特征清洗想法\n    + 可以使用：Console, Notebook, Pandas\n    + 尝试简单的统计数据：最小值，最大值\n    + 将目标结合起来，以便在信息之间找到相关性\n  - 迭代/调试（Iteration / Debugging）\n    + 特征工程是一个迭代过程：使你的pipelines适合快速迭代\n    + 使用子线性调试：输出有关过程的中间信息，进行伪log\n    + 使用允许快速实验的工具\n    + 失败的想法会多余成功的想法\n  - 标签工程（Label Engineering）\n    + 可以将标签/目标/因变量视为数据的一个特征，反之亦然\n    + 对数变换：y  - > log（y + 1）| exp（y_pred） -  1\n    + Square 变换\n    + Box-Cox变换\n    + 创建分数，在回归中转换二进制目标\n    + 训练回归器以预测测试集中不可用的特征\n\n#### 自然语言处理（Natural Language Processing）\n* 特点\n  - 可以用类别特征中相同的方法\n  - 深度学习（自动特征工程）正在慢慢的吞噬这个领域，但是具有好的特征工程的浅学习仍然具有竞争力\n  - 高稀疏性的数据将带来“维度诅咒”\n  - 特征工程具有很多机会\n* 方法\n  - 所有方法列表\n    + 转换成小写字母\n    + 删除非字母数字\n    + 修复\n    + 编码标点符号\n    + 符号化\n    + 令牌克（Token-grams）\n    + skipgrams\n    + char-grams\n    + 删除停用词\n    + 删除罕见的单词\n    + 非常常见的词\n    + 拼写纠正\n    + 砍字\n    + 词干\n    + 词形还原\n    + 文档特征\n    + 实体的插入和提取\n    + 简化\n    + Word2Vec 和 GloVe / Doc2Vec\n    + 字符串相似性\n    + 阅读水平\n    + 最近邻居\n    + TF*IDF\n    + BayesSVM，矢量化，LDA，LSA\n  - 清洗（Cleaning）\n    + 转换成小写字母：使标记独立于大写：“I work at NASA” -> “i work at nasa”.\n    + 转换成Unidecode：将字符转换为ascii-对应物：\n    + 删除非字母数字：删除不在[a-z] [A-Z] [0-9]中的任何内容来清理文本，“Breaking! Amsterdam (2009)” -> “Breaking Amsterdam 2009”\n    + 修复：修复编码问题或修剪intertoken空格。 “C a s a C a f＆eacute;” - >“CasaCafé”\n  - 符号化（Tokenizing）\n    + 编码标点符号：硬编码“！”和“？”作为标记。\n    + 符号化（Tokenize）：划分句子标记成单词记号\n    + N-Grams：将连续的符号编码为符号,  “I like the Beatles” -> [“I like”, “like the”, “the Beatles”]\n    + Skip-grams: 编码连续的符号，但跳过一些,  “I like the Beatles” -> [“I the”, “like Beatles”]\n    + Char-grams: 与N-gram相同，但字符级别, “Beatles” - > [“Bea”, “eat”, “atl”, “tle”, “les”]\n    + Affixes：与char-gram相同，但是仅限于前缀与后缀\n  - 删除（Removing）\n    + 停用词：删除停用词列表中出现的单词/标记\n    + 稀有单词：删除仅在训练集中出现几次的单词\n    + 常用词：删除可能不在停用词列表中的极其常见的词\n  - 根（Roots）\n    + 拼写纠正：将字符更改为正确的拼写\n    + 切（chop）： 仅取一个单词的前n（8）个字符\n    + 词干：将词/标记减少到其根， “cars” -> “car”\n    + Lemmatize：找到语义根， “never be late” -> “never are late”\n  - 丰富（Enrich）\n    + 文档特征：计算空格数量，tab数量，换行数量，字符数量，以及令牌数量等\n    + 实体插入：向文本中插入更多的通用的规范， “Microsoft releases Windows” -> “Microsoft (company) releases Windows (application)”\n    + 解析树：将句子解析为逻辑形式，“Alice hits Bill” -> Alice/Noun_subject hits/Verb Bill/Noun_object.\n    + 阅读级别：计算文档的阅读级别\n  - 相似性（Similarities）\n    + 令牌相似性：计算出现在两个文本中的令牌数\n    + 压缩距离：查看是否可以使用其他文本更好地压缩一个文本\n    + Levenshtein / Hamming / Jaccard 距离：通过查看在另一个字符串中转换一个字符串所需的操作数来检查两个字符串之间的相似性\n    + Word2Vec / Glove：检查两个平均向量之间的余弦相似度\n  - TF-IDF\n    + 术语频率：减少对长文档的偏差\n    + 反向文档频率：减少对常见令牌的偏差\n    + TF-IDF：用于标识文档中最重要的标记，删除不重要的标记，或作为降维的预处理步骤\n  - 降维\n    + PCA：将文本缩小为50或100维向量\n    + SVD：将文本缩小为50或100维向量\n    + LDA：TF-IDF，然后是SVD\n    + LSA：创建主题向量\n  - 外部模型\n    + 情绪分析器：为任何文本获取负面或正面情绪的向量\n    + 主题模型：使用另一个数据集为新数据集创建主题向量\n#### 深度学习/神经网络\n* 神经网络声称是端到端的自动特征工程。所以特征工程没用了么？并不是这样的。特征工程将重点转移到架构工程。\n* 尽管承诺：计算机视觉使用的特征， 例如：\n  - HOG\n  - SIFT\n  - whitening\n  - perturbation\n  - image pyramids\n  - rotation\n  - z-scaling\n  - log-scaling\n  - frame- grams\n  - external semantic data\n  - ...\n\n#### Leakage / Golden Features\n* 特征工程可以帮助利用泄露\n* 逆向工程\n  - 使用rainbow表反转 MD5 Hash\n  - 将 TF-IDF反转回术语频率\n  - 编码样本数据集的顺序\n  - 编码文件创建日期\n* 规则挖掘\n  - 查找简单的规则（并对其进行编码）以帮助您的模型\n\n<!-- $$\nf(n) = \\begin{cases}\n \\frac{n}{2},\n & \\text{if } n\\text{ is even}\n \\\\ 3n+1, & \\text{if } n\\text{ is odd}\n \\end{cases}\n$$ -->\n\n\n### 参考资料\n[Tips & Tricks for Feature Engineering / Applied Machine Learning](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750)\n[]","slug":"2019-07-29-特征工程","published":1,"updated":"2019-08-07T06:14:51.229Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqf80008z0ov513xohfn","content":"<blockquote>\n<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已<br>\n实践中学习的手艺<br>\nMore data beats clever algorithms, but better data beats more data</p>\n</blockquote>\n<h3><span id=\"特征工程包含以下几个方面\"> 特征工程包含以下几个方面：</span></h3>\n<h4><span id=\"特征使用方案\"> 特征使用方案</span></h4>\n<ul>\n<li>要实现我们的目标需要哪些数据？\n<ul>\n<li>基于业务理解，尽可能找出对因变量有影响的所有自变量</li>\n</ul>\n</li>\n<li>可用性评估\n<ul>\n<li>获取难度</li>\n<li>覆盖率</li>\n<li>准确率</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"特征获取方案\"> 特征获取方案</span></h4>\n<ul>\n<li>如何获取这些特征？</li>\n<li>如何存储？</li>\n</ul>\n<h4><span id=\"特征处理\"> 特征处理</span></h4>\n<ul>\n<li>特征特征清洗\n<ul>\n<li>清洗异样样本</li>\n<li>采样\n<ul>\n<li>数据不均衡</li>\n<li>样本权重</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>预处理\n<ul>\n<li>单个特征\n<ul>\n<li>归一化</li>\n<li>离散化</li>\n<li>Dummy Coding</li>\n<li>缺失值</li>\n<li>数据变换\n<ul>\n<li>log</li>\n<li>指数</li>\n<li>Box-Cox</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>多个特征\n<ul>\n<li>Filter\n<ul>\n<li>思路：自变量和目标变量之间的关联</li>\n<li>相关系数</li>\n<li>卡方检验</li>\n<li>信息增益、 互信息</li>\n</ul>\n</li>\n<li>Wrapper\n<ul>\n<li>思路: 通过目标函数（AUC/MSE）来决定是否加入一个变量</li>\n<li>迭代：产生特征子集，评价\n<ul>\n<li>完全搜索</li>\n<li>启发式搜索</li>\n<li>随机搜索\n<ul>\n<li>GA</li>\n<li>SA</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Embedded\n<ul>\n<li>思路：学习期自身自动选择特征</li>\n<li>正则化\n<ul>\n<li>L1 – Lasso</li>\n<li>L2 – Ridge</li>\n</ul>\n</li>\n<li>决策树 – 熵、信息增益</li>\n<li>深度学习</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>衍生变量 – 对原始数据加工，生成有商业意义的变量</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"特征监控\"> 特征监控</span></h4>\n<ul>\n<li>特征有效性分析</li>\n<li>特征监控</li>\n</ul>\n<h3><span id=\"最基本的特征工程方法\"> 最基本的特征工程方法</span></h3>\n<h4><span id=\"类别特征\"> 类别特征</span></h4>\n<ul>\n<li>特点\n<ul>\n<li>几乎总是需要一些处理</li>\n<li>高基数可以创建非常稀疏的数据</li>\n<li>很难插补缺失数据</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>\n<p>独热编码（One-Hot Encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>在长度为K的数组上做 one-of-k 编码</li>\n<li>基本方法： 与大多数线性算法一起使用</li>\n<li>删除第一列可以避免多重共线性</li>\n<li>稀疏格式对内存友好</li>\n<li>当前大多数实现都没有处理缺失，看不见的数据</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘BR’]\n<table>\n<thead>\n<tr>\n<th>国家  =&gt;</th>\n<th>国家=NL</th>\n<th>国家=BR</th>\n<th>国家=US</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NL</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>BR</td>\n<td>[   0</td>\n<td>1</td>\n<td>0]</td>\n</tr>\n<tr>\n<td>US</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码集：[0, 1, 0]</li>\n<li>编码稀疏： 2:1</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>散列编码（Hash encoding)</p>\n<ul>\n<li>概念\n<ul>\n<li>独热编码是否具有固定长度的数组？</li>\n<li>避免极其稀疏的数据</li>\n<li>可能会引入碰撞</li>\n<li>可以重复的使用不同的哈希函数和包获得小的精度</li>\n<li>碰撞通常会降低结果，但可能会改善结果</li>\n<li>优雅地处理新变量（例如：新用户代理）</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘BR’]</li>\n<li>hash(‘BR’) =&gt;\n<table>\n<thead>\n<tr>\n<th>国家</th>\n<th>hash1</th>\n<th>hash2</th>\n<th>hash3</th>\n<th>hash4</th>\n<th>hash5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NL</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>BR</td>\n<td>[   0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0]</td>\n</tr>\n<tr>\n<td>US</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码集：[0, 1, 0, 0, 0]</li>\n<li>编码稀疏： 2:1</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>标签编码（Label encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>为每个类别变量提供唯一的数字ID</li>\n<li>对于非线性基于树的算法很有用</li>\n<li>不增加维度</li>\n<li>随机化类别变量到数值变量的映射并重新训练， 平均以获得精度较小的凸凹</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘Queenstown’]\n<table>\n<thead>\n<tr>\n<th>城市  =&gt;</th>\n<th>city</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cherbourg</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Queenstown</td>\n<td>2</td>\n</tr>\n<tr>\n<td>Southhampton</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [2]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>计数编码（Count encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>将类别变量替换为他们在训练数据中的计数</li>\n<li>适用于线性和非线性算法</li>\n<li>可以对异常值敏感</li>\n<li>可以加入对数转换（log transform）, 与计数一起工作的很好</li>\n<li>用‘1’替换看不见的数据</li>\n<li>可能会发生冲突：相同的编码，不同的变量</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘A6GHBD78’]\n<table>\n<thead>\n<tr>\n<th>teacher_id</th>\n<th>teacher_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n<tr>\n<td>A6GHBD78</td>\n<td>3</td>\n</tr>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n<tr>\n<td>FCKGWRHQ</td>\n<td>1</td>\n</tr>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n<tr>\n<td>A6GHBD78</td>\n<td>3</td>\n</tr>\n<tr>\n<td>A6GHBD78</td>\n<td>3</td>\n</tr>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [3]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>标签计数编码（LabelCount encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>在训练数据中，对类别变量进行排名</li>\n<li>适用于线性和非线性算法</li>\n<li>对异常值不敏感</li>\n<li>不会对不同的变量赋予相同的编码</li>\n<li>两全其美</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘nl’]\n<table>\n<thead>\n<tr>\n<th>tld</th>\n<th>tld</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>de</td>\n<td>2</td>\n</tr>\n<tr>\n<td>de</td>\n<td>2</td>\n</tr>\n<tr>\n<td>fr</td>\n<td>1</td>\n</tr>\n<tr>\n<td>fr</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [3]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Target encoding</p>\n<ul>\n<li>概念\n<ul>\n<li>根据目标比率对类别变量进行编码（二进制分类或回归）</li>\n<li>小心避免过度拟合（overfit）</li>\n<li>堆叠的形式：单变量模型，其输出平均目标</li>\n<li>以交叉验证方式进行</li>\n<li>添加平滑以避免将变量编码设置为0</li>\n<li>添加随机噪音以对抗过拟合</li>\n<li>什么时候用：线性和非线性的最佳编码</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>role</th>\n<th>y</th>\n<th>role</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>manager</td>\n<td>1</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>1</td>\n<td>0.66</td>\n</tr>\n<tr>\n<td>scientist</td>\n<td>1</td>\n<td>1.</td>\n</tr>\n<tr>\n<td>manager</td>\n<td>0</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>0</td>\n<td>0.66</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>1</td>\n<td>0.66</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>\n<p>类别嵌入（Category Embedding）</p>\n<ul>\n<li>概念\n<ul>\n<li>使用神经网络从类别变量创建密集嵌入</li>\n<li>将函数逼近问题中的类别变量映射到欧几里德空间</li>\n<li>更快的模型训练</li>\n<li>减少内存开销</li>\n<li>可以提供比1-hot 编码更好的精准度</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>role</th>\n<th>role 3-D embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>manager</td>\n<td>[0.05, 0.10, 0.96]</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>[0.72, 0.66, 0.17]</td>\n</tr>\n<tr>\n<td>scientist</td>\n<td>[0.75, 0.62, 0.15]</td>\n</tr>\n<tr>\n<td>manager</td>\n<td>[0.05, 0.10, 0.96]</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>[0.72, 0.66, 0.17]</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>[0.72, 0.66, 0.17]</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>\n<p>NaN编码（NaN encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>为NaN值提供显式编码而不是忽略</li>\n<li>NaN值可以保存信息</li>\n<li>小心避免过度拟合</li>\n<li>仅在训练数据和测试数据中的Nan值是由相同的原因引起时才可以用，或者本地验证证明它持有信息</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘NaN’]\n<table>\n<thead>\n<tr>\n<th>UA</th>\n<th>UA=mobile</th>\n<th>UA=tablet</th>\n<th>UA=NaN</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>mobile</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>tablet</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>mobile</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>NaN</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>mobile</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [0, 0, 1]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>多项式编码（Polynomial encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>类别变量之间的编码交互</li>\n<li>没有交互的线性算法无法解决XOR问题</li>\n<li>多项式内核可以解决XOR问题</li>\n<li>爆炸式的特征空间： 使用FS，散列或者VW</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>A</th>\n<th>B</th>\n<th>y</th>\n<th>A=1*B=1</th>\n<th>A=0*B=1</th>\n<th>A=1*B=0</th>\n<th>A=0*B=0</th>\n<th>y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>\n<p>扩展编码（Expansion encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>从单个变量创建多个类别变量</li>\n<li>某些高基数功能（如用户代理）在其中包含更多信息：\n<ul>\n<li>is_mobile?</li>\n<li>is_latest_version?</li>\n<li>Operation_system</li>\n<li>Browser_build</li>\n<li>etc…</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36<br>\n==&gt;\n<table>\n<thead>\n<tr>\n<th>UA1</th>\n<th>UA2</th>\n<th>UA3</th>\n<th>UA4</th>\n<th>UA5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Chrome</td>\n<td>53.0.2785.143</td>\n<td>Desktop</td>\n<td>Mac</td>\n<td>10_10_4</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>合并编码（Consolidation encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>将不同的类别变量映射到同一个变量</li>\n<li>拼写错误，略有不同的职位描述，全名 vs 缩写</li>\n<li>真实数据很乱，自由文本尤其如此</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>company_desc   =&gt;</th>\n<th>desc1</th>\n<th>company_desc2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Shell</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>shel</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>SHELL</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>Shell Gasonline</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>BP</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>British Petr.</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>B&amp;P</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>BP Gas Station</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>bp</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>Procter&amp;Gamble</td>\n<td>P&amp;G</td>\n<td>Manufacturer</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"数值特征\"> 数值特征</span></h4>\n<ul>\n<li>特点\n<ul>\n<li>可以更容易地输入算法</li>\n<li>可以构成浮点数，计数，数字</li>\n<li>更容易估算缺失的数据</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>四舍五入（Rounding）\n<ul>\n<li>概念\n<ul>\n<li>四舍五入数值变量</li>\n<li>有损压缩： 保留数据的最重要特征</li>\n<li>有时太精确只是噪音</li>\n<li>四舍五入的变量可以视为类别变量</li>\n<li>可以在四舍五入之前做对数变换</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>age</th>\n<th>age1</th>\n<th>age2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>23.6671</td>\n<td>23</td>\n<td>2</td>\n</tr>\n<tr>\n<td>23.8891</td>\n<td>23</td>\n<td>2</td>\n</tr>\n<tr>\n<td>22.1261</td>\n<td>22</td>\n<td>2</td>\n</tr>\n<tr>\n<td>19.5506</td>\n<td>19</td>\n<td>1</td>\n</tr>\n<tr>\n<td>18.2114</td>\n<td>18</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>分档（Binning）\n<ul>\n<li>概念\n<ul>\n<li>将数值变量放入bin（档）中并使用bin-ID进行编码</li>\n<li>分档可以务实设置，通过分位数，均匀的，或使用模型到最佳分档</li>\n<li>可以使得训练集所见范围以外的变量正常工作</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>risk_score</th>\n<th>rs[-inf, 33]</th>\n<th>rs[33,66]</th>\n<th>rs[66,inf]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>15</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>77</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>78</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>55</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>42</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>缩放（Scaling）\n<ul>\n<li>概念\n<ul>\n<li>将数值变量扩展到特定范围</li>\n<li>Standard（Z）缩放</li>\n<li>MinMax 缩放</li>\n<li>Root 缩放</li>\n<li>Log 缩放</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>估算（Imputation）\n<ul>\n<li>概念\n<ul>\n<li>估算缺失的变量</li>\n<li>硬编码可与估算相结合</li>\n<li>平均：非常基本的</li>\n<li>中位数：对异常值鲁棒性比较高</li>\n<li>忽略： 只是推迟问题</li>\n<li>使用模型：可以暴露算法的偏差</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>wage</th>\n<th>hours</th>\n<th>gender</th>\n<th>y =&gt;</th>\n<th>wage</th>\n<th>hours</th>\n<th>gender_y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1600</td>\n<td>40</td>\n<td>0</td>\n<td>1</td>\n<td>1600</td>\n<td>40</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2200</td>\n<td>50</td>\n<td>1</td>\n<td>1</td>\n<td>2200</td>\n<td>50</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1800</td>\n<td>36</td>\n<td>0</td>\n<td>0</td>\n<td>1800</td>\n<td>36</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2100</td>\n<td>45</td>\n<td>1</td>\n<td>0</td>\n<td>2100</td>\n<td>45</td>\n<td>?</td>\n</tr>\n<tr>\n<td>2050</td>\n<td>60</td>\n<td>NaN</td>\n<td>0</td>\n<td>2050</td>\n<td>60</td>\n<td>?</td>\n</tr>\n<tr>\n<td>1650</td>\n<td>36</td>\n<td>0</td>\n<td>1</td>\n<td>1650</td>\n<td>36</td>\n<td>?</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>相互作用（Interactions）\n<ul>\n<li>概念\n<ul>\n<li>具体来说编码数值变量之间的相互作用</li>\n<li>尝试：减法，加法，乘法，除法</li>\n<li>使用：通过统 计测试选择特征，或训练模型特征重要性</li>\n<li>忽略：人的直觉; 奇怪的相互作用可以带来显著的改进</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>线性算法的非线性编码（No-linear encoding for linear algo’s）\n<ul>\n<li>概念\n<ul>\n<li>硬编码非线性以改进线性算法</li>\n<li>多项式内核</li>\n<li>叶编码（随机森林嵌入）</li>\n<li>遗传算法</li>\n<li>局部线性嵌入，光谱嵌入，t-SNE</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>行统计（Row statistics）\n<ul>\n<li>概念\n<ul>\n<li>创建一行数据的统计信息</li>\n<li>NaN的数量</li>\n<li>0的数量</li>\n<li>负数值的数量</li>\n<li>平均值，最大值，最小值，偏度 等</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"时间特征-temporal-variables\"> 时间特征 (Temporal variables)</span></h4>\n<ul>\n<li>特点\n<ul>\n<li>时间变量，如日期，需要更好的本地验证方案（如回测）</li>\n<li>容易在这里犯错误</li>\n<li>很多机会进行重大改进</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>投射到一个圆\n<ul>\n<li>将单个要素（如day_of_week）转换为圆上的两个坐标</li>\n<li>确保max和min之间的距离与min和min +1相同</li>\n<li>用于day_of_week，day_of_month，hour_of_day等</li>\n</ul>\n</li>\n<li>趋势线（Trendlines）\n<ul>\n<li>不是编码总支出，而是编码上周花费，上个月花费，去年花费。</li>\n<li>给算法的趋势：两个支出相同的客户可能会有截然不同的行为 - 一个客户可能开始花费更多，而另一个客户开始减少支出。</li>\n</ul>\n</li>\n<li>与重大事件的接近度（Closeness to major events）\n<ul>\n<li>硬编码类别特征，如：date_3_days_before_holidays = 1</li>\n<li>尝试：国庆节，重大体育赛事，周末，一个月的第一个星期六等</li>\n<li>这些因素可能对支出行为产生重大影响</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"空间特征spatial-variables\"> 空间特征（Spatial Variables）</span></h4>\n<ul>\n<li>特点\n<ul>\n<li>空间变量是在空间编码的位置的变量</li>\n<li>例子包括：GPS坐标，城市，国家，地址</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>分类位置（Categorizing location）\n<ul>\n<li>克里金法（Kriging）</li>\n<li>K-means聚类</li>\n<li>原始纬度经度</li>\n<li>将城市转换为纬度经度</li>\n<li>将邮政编码添加到街道名称</li>\n</ul>\n</li>\n<li>与枢纽的接近度（Closeness to Hubs）\n<ul>\n<li>找到位置与主要枢纽之间的紧密程度</li>\n<li>小城镇继承了附近大城市的一些文化/背景</li>\n<li>电话位置可以映射到附近的企业和超市</li>\n</ul>\n</li>\n<li>空间欺诈行为（Spatial fraudulent behavior）\n<ul>\n<li>位置事件数据可以指示可疑行为</li>\n<li>不可能的旅行速度：不同国家的多个同步交易</li>\n<li>在不同的城镇消费，而不是在家或送货地址</li>\n<li>永远不要在同一地点消费</li>\n</ul>\n</li>\n<li>探索（Exploration）\n<ul>\n<li>数据探索可以发现数据健康问题，异常值，噪声，特征工程想法，特征清洗想法</li>\n<li>可以使用：Console, Notebook, Pandas</li>\n<li>尝试简单的统计数据：最小值，最大值</li>\n<li>将目标结合起来，以便在信息之间找到相关性</li>\n</ul>\n</li>\n<li>迭代/调试（Iteration / Debugging）\n<ul>\n<li>特征工程是一个迭代过程：使你的pipelines适合快速迭代</li>\n<li>使用子线性调试：输出有关过程的中间信息，进行伪log</li>\n<li>使用允许快速实验的工具</li>\n<li>失败的想法会多余成功的想法</li>\n</ul>\n</li>\n<li>标签工程（Label Engineering）\n<ul>\n<li>可以将标签/目标/因变量视为数据的一个特征，反之亦然</li>\n<li>对数变换：y  - &gt; log（y + 1）| exp（y_pred） -  1</li>\n<li>Square 变换</li>\n<li>Box-Cox变换</li>\n<li>创建分数，在回归中转换二进制目标</li>\n<li>训练回归器以预测测试集中不可用的特征</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"自然语言处理natural-language-processing\"> 自然语言处理（Natural Language Processing）</span></h4>\n<ul>\n<li>特点\n<ul>\n<li>可以用类别特征中相同的方法</li>\n<li>深度学习（自动特征工程）正在慢慢的吞噬这个领域，但是具有好的特征工程的浅学习仍然具有竞争力</li>\n<li>高稀疏性的数据将带来“维度诅咒”</li>\n<li>特征工程具有很多机会</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>所有方法列表\n<ul>\n<li>转换成小写字母</li>\n<li>删除非字母数字</li>\n<li>修复</li>\n<li>编码标点符号</li>\n<li>符号化</li>\n<li>令牌克（Token-grams）</li>\n<li>skipgrams</li>\n<li>char-grams</li>\n<li>删除停用词</li>\n<li>删除罕见的单词</li>\n<li>非常常见的词</li>\n<li>拼写纠正</li>\n<li>砍字</li>\n<li>词干</li>\n<li>词形还原</li>\n<li>文档特征</li>\n<li>实体的插入和提取</li>\n<li>简化</li>\n<li>Word2Vec 和 GloVe / Doc2Vec</li>\n<li>字符串相似性</li>\n<li>阅读水平</li>\n<li>最近邻居</li>\n<li>TF*IDF</li>\n<li>BayesSVM，矢量化，LDA，LSA</li>\n</ul>\n</li>\n<li>清洗（Cleaning）\n<ul>\n<li>转换成小写字母：使标记独立于大写：“I work at NASA” -&gt; “i work at nasa”.</li>\n<li>转换成Unidecode：将字符转换为ascii-对应物：</li>\n<li>删除非字母数字：删除不在[a-z] [A-Z] [0-9]中的任何内容来清理文本，“Breaking! Amsterdam (2009)” -&gt; “Breaking Amsterdam 2009”</li>\n<li>修复：修复编码问题或修剪intertoken空格。 “C a s a C a f＆eacute;” - &gt;“CasaCafé”</li>\n</ul>\n</li>\n<li>符号化（Tokenizing）\n<ul>\n<li>编码标点符号：硬编码“！”和“？”作为标记。</li>\n<li>符号化（Tokenize）：划分句子标记成单词记号</li>\n<li>N-Grams：将连续的符号编码为符号,  “I like the Beatles” -&gt; [“I like”, “like the”, “the Beatles”]</li>\n<li>Skip-grams: 编码连续的符号，但跳过一些,  “I like the Beatles” -&gt; [“I the”, “like Beatles”]</li>\n<li>Char-grams: 与N-gram相同，但字符级别, “Beatles” - &gt; [“Bea”, “eat”, “atl”, “tle”, “les”]</li>\n<li>Affixes：与char-gram相同，但是仅限于前缀与后缀</li>\n</ul>\n</li>\n<li>删除（Removing）\n<ul>\n<li>停用词：删除停用词列表中出现的单词/标记</li>\n<li>稀有单词：删除仅在训练集中出现几次的单词</li>\n<li>常用词：删除可能不在停用词列表中的极其常见的词</li>\n</ul>\n</li>\n<li>根（Roots）\n<ul>\n<li>拼写纠正：将字符更改为正确的拼写</li>\n<li>切（chop）： 仅取一个单词的前n（8）个字符</li>\n<li>词干：将词/标记减少到其根， “cars” -&gt; “car”</li>\n<li>Lemmatize：找到语义根， “never be late” -&gt; “never are late”</li>\n</ul>\n</li>\n<li>丰富（Enrich）\n<ul>\n<li>文档特征：计算空格数量，tab数量，换行数量，字符数量，以及令牌数量等</li>\n<li>实体插入：向文本中插入更多的通用的规范， “Microsoft releases Windows” -&gt; “Microsoft (company) releases Windows (application)”</li>\n<li>解析树：将句子解析为逻辑形式，“Alice hits Bill” -&gt; Alice/Noun_subject hits/Verb Bill/Noun_object.</li>\n<li>阅读级别：计算文档的阅读级别</li>\n</ul>\n</li>\n<li>相似性（Similarities）\n<ul>\n<li>令牌相似性：计算出现在两个文本中的令牌数</li>\n<li>压缩距离：查看是否可以使用其他文本更好地压缩一个文本</li>\n<li>Levenshtein / Hamming / Jaccard 距离：通过查看在另一个字符串中转换一个字符串所需的操作数来检查两个字符串之间的相似性</li>\n<li>Word2Vec / Glove：检查两个平均向量之间的余弦相似度</li>\n</ul>\n</li>\n<li>TF-IDF\n<ul>\n<li>术语频率：减少对长文档的偏差</li>\n<li>反向文档频率：减少对常见令牌的偏差</li>\n<li>TF-IDF：用于标识文档中最重要的标记，删除不重要的标记，或作为降维的预处理步骤</li>\n</ul>\n</li>\n<li>降维\n<ul>\n<li>PCA：将文本缩小为50或100维向量</li>\n<li>SVD：将文本缩小为50或100维向量</li>\n<li>LDA：TF-IDF，然后是SVD</li>\n<li>LSA：创建主题向量</li>\n</ul>\n</li>\n<li>外部模型\n<ul>\n<li>情绪分析器：为任何文本获取负面或正面情绪的向量</li>\n<li>主题模型：使用另一个数据集为新数据集创建主题向量</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"深度学习神经网络\"> 深度学习/神经网络</span></h4>\n<ul>\n<li>神经网络声称是端到端的自动特征工程。所以特征工程没用了么？并不是这样的。特征工程将重点转移到架构工程。</li>\n<li>尽管承诺：计算机视觉使用的特征， 例如：\n<ul>\n<li>HOG</li>\n<li>SIFT</li>\n<li>whitening</li>\n<li>perturbation</li>\n<li>image pyramids</li>\n<li>rotation</li>\n<li>z-scaling</li>\n<li>log-scaling</li>\n<li>frame- grams</li>\n<li>external semantic data</li>\n<li>…</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"leakage-golden-features\"> Leakage / Golden Features</span></h4>\n<ul>\n<li>特征工程可以帮助利用泄露</li>\n<li>逆向工程\n<ul>\n<li>使用rainbow表反转 MD5 Hash</li>\n<li>将 TF-IDF反转回术语频率</li>\n<li>编码样本数据集的顺序</li>\n<li>编码文件创建日期</li>\n</ul>\n</li>\n<li>规则挖掘\n<ul>\n<li>查找简单的规则（并对其进行编码）以帮助您的模型</li>\n</ul>\n</li>\n</ul>\n<!-- $$\nf(n) = \\begin{cases}\n \\frac{n}{2},\n & \\text{if } n\\text{ is even}\n \\\\ 3n+1, & \\text{if } n\\text{ is odd}\n \\end{cases}\n$$ -->\n<h3><span id=\"参考资料\"> 参考资料</span></h3>\n<p><a href=\"https://www.slideshare.net/HJvanVeen/feature-engineering-72376750\" target=\"_blank\" rel=\"noopener\">Tips &amp; Tricks for Feature Engineering / Applied Machine Learning</a><br>\n[]</p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已<br>\n实践中学习的手艺<br>\nMore data beats clever algorithms, but better data beats more data</p>\n</blockquote>\n<h3 id=\"特征工程包含以下几个方面\"><a class=\"markdownIt-Anchor\" href=\"#特征工程包含以下几个方面\"></a> 特征工程包含以下几个方面：</h3>\n<h4 id=\"特征使用方案\"><a class=\"markdownIt-Anchor\" href=\"#特征使用方案\"></a> 特征使用方案</h4>\n<ul>\n<li>要实现我们的目标需要哪些数据？\n<ul>\n<li>基于业务理解，尽可能找出对因变量有影响的所有自变量</li>\n</ul>\n</li>\n<li>可用性评估\n<ul>\n<li>获取难度</li>\n<li>覆盖率</li>\n<li>准确率</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"特征获取方案\"><a class=\"markdownIt-Anchor\" href=\"#特征获取方案\"></a> 特征获取方案</h4>\n<ul>\n<li>如何获取这些特征？</li>\n<li>如何存储？</li>\n</ul>\n<h4 id=\"特征处理\"><a class=\"markdownIt-Anchor\" href=\"#特征处理\"></a> 特征处理</h4>\n<ul>\n<li>特征特征清洗\n<ul>\n<li>清洗异样样本</li>\n<li>采样\n<ul>\n<li>数据不均衡</li>\n<li>样本权重</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>预处理\n<ul>\n<li>单个特征\n<ul>\n<li>归一化</li>\n<li>离散化</li>\n<li>Dummy Coding</li>\n<li>缺失值</li>\n<li>数据变换\n<ul>\n<li>log</li>\n<li>指数</li>\n<li>Box-Cox</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>多个特征\n<ul>\n<li>Filter\n<ul>\n<li>思路：自变量和目标变量之间的关联</li>\n<li>相关系数</li>\n<li>卡方检验</li>\n<li>信息增益、 互信息</li>\n</ul>\n</li>\n<li>Wrapper\n<ul>\n<li>思路: 通过目标函数（AUC/MSE）来决定是否加入一个变量</li>\n<li>迭代：产生特征子集，评价\n<ul>\n<li>完全搜索</li>\n<li>启发式搜索</li>\n<li>随机搜索\n<ul>\n<li>GA</li>\n<li>SA</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Embedded\n<ul>\n<li>思路：学习期自身自动选择特征</li>\n<li>正则化\n<ul>\n<li>L1 – Lasso</li>\n<li>L2 – Ridge</li>\n</ul>\n</li>\n<li>决策树 – 熵、信息增益</li>\n<li>深度学习</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>衍生变量 – 对原始数据加工，生成有商业意义的变量</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"特征监控\"><a class=\"markdownIt-Anchor\" href=\"#特征监控\"></a> 特征监控</h4>\n<ul>\n<li>特征有效性分析</li>\n<li>特征监控</li>\n</ul>\n<h3 id=\"最基本的特征工程方法\"><a class=\"markdownIt-Anchor\" href=\"#最基本的特征工程方法\"></a> 最基本的特征工程方法</h3>\n<h4 id=\"类别特征\"><a class=\"markdownIt-Anchor\" href=\"#类别特征\"></a> 类别特征</h4>\n<ul>\n<li>特点\n<ul>\n<li>几乎总是需要一些处理</li>\n<li>高基数可以创建非常稀疏的数据</li>\n<li>很难插补缺失数据</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>\n<p>独热编码（One-Hot Encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>在长度为K的数组上做 one-of-k 编码</li>\n<li>基本方法： 与大多数线性算法一起使用</li>\n<li>删除第一列可以避免多重共线性</li>\n<li>稀疏格式对内存友好</li>\n<li>当前大多数实现都没有处理缺失，看不见的数据</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘BR’]\n<table>\n<thead>\n<tr>\n<th>国家  =&gt;</th>\n<th>国家=NL</th>\n<th>国家=BR</th>\n<th>国家=US</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NL</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>BR</td>\n<td>[   0</td>\n<td>1</td>\n<td>0]</td>\n</tr>\n<tr>\n<td>US</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码集：[0, 1, 0]</li>\n<li>编码稀疏： 2:1</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>散列编码（Hash encoding)</p>\n<ul>\n<li>概念\n<ul>\n<li>独热编码是否具有固定长度的数组？</li>\n<li>避免极其稀疏的数据</li>\n<li>可能会引入碰撞</li>\n<li>可以重复的使用不同的哈希函数和包获得小的精度</li>\n<li>碰撞通常会降低结果，但可能会改善结果</li>\n<li>优雅地处理新变量（例如：新用户代理）</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘BR’]</li>\n<li>hash(‘BR’) =&gt;\n<table>\n<thead>\n<tr>\n<th>国家</th>\n<th>hash1</th>\n<th>hash2</th>\n<th>hash3</th>\n<th>hash4</th>\n<th>hash5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NL</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>BR</td>\n<td>[   0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0]</td>\n</tr>\n<tr>\n<td>US</td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码集：[0, 1, 0, 0, 0]</li>\n<li>编码稀疏： 2:1</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>标签编码（Label encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>为每个类别变量提供唯一的数字ID</li>\n<li>对于非线性基于树的算法很有用</li>\n<li>不增加维度</li>\n<li>随机化类别变量到数值变量的映射并重新训练， 平均以获得精度较小的凸凹</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘Queenstown’]\n<table>\n<thead>\n<tr>\n<th>城市  =&gt;</th>\n<th>city</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Cherbourg</td>\n<td>1</td>\n</tr>\n<tr>\n<td>Queenstown</td>\n<td>2</td>\n</tr>\n<tr>\n<td>Southhampton</td>\n<td>3</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [2]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>计数编码（Count encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>将类别变量替换为他们在训练数据中的计数</li>\n<li>适用于线性和非线性算法</li>\n<li>可以对异常值敏感</li>\n<li>可以加入对数转换（log transform）, 与计数一起工作的很好</li>\n<li>用‘1’替换看不见的数据</li>\n<li>可能会发生冲突：相同的编码，不同的变量</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘A6GHBD78’]\n<table>\n<thead>\n<tr>\n<th>teacher_id</th>\n<th>teacher_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n<tr>\n<td>A6GHBD78</td>\n<td>3</td>\n</tr>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n<tr>\n<td>FCKGWRHQ</td>\n<td>1</td>\n</tr>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n<tr>\n<td>A6GHBD78</td>\n<td>3</td>\n</tr>\n<tr>\n<td>A6GHBD78</td>\n<td>3</td>\n</tr>\n<tr>\n<td>DEADB33F</td>\n<td>4</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [3]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>标签计数编码（LabelCount encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>在训练数据中，对类别变量进行排名</li>\n<li>适用于线性和非线性算法</li>\n<li>对异常值不敏感</li>\n<li>不会对不同的变量赋予相同的编码</li>\n<li>两全其美</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘nl’]\n<table>\n<thead>\n<tr>\n<th>tld</th>\n<th>tld</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>nl</td>\n<td>3</td>\n</tr>\n<tr>\n<td>de</td>\n<td>2</td>\n</tr>\n<tr>\n<td>de</td>\n<td>2</td>\n</tr>\n<tr>\n<td>fr</td>\n<td>1</td>\n</tr>\n<tr>\n<td>fr</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [3]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>Target encoding</p>\n<ul>\n<li>概念\n<ul>\n<li>根据目标比率对类别变量进行编码（二进制分类或回归）</li>\n<li>小心避免过度拟合（overfit）</li>\n<li>堆叠的形式：单变量模型，其输出平均目标</li>\n<li>以交叉验证方式进行</li>\n<li>添加平滑以避免将变量编码设置为0</li>\n<li>添加随机噪音以对抗过拟合</li>\n<li>什么时候用：线性和非线性的最佳编码</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>role</th>\n<th>y</th>\n<th>role</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>manager</td>\n<td>1</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>1</td>\n<td>0.66</td>\n</tr>\n<tr>\n<td>scientist</td>\n<td>1</td>\n<td>1.</td>\n</tr>\n<tr>\n<td>manager</td>\n<td>0</td>\n<td>0.5</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>0</td>\n<td>0.66</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>1</td>\n<td>0.66</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>\n<p>类别嵌入（Category Embedding）</p>\n<ul>\n<li>概念\n<ul>\n<li>使用神经网络从类别变量创建密集嵌入</li>\n<li>将函数逼近问题中的类别变量映射到欧几里德空间</li>\n<li>更快的模型训练</li>\n<li>减少内存开销</li>\n<li>可以提供比1-hot 编码更好的精准度</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>role</th>\n<th>role 3-D embedding</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>manager</td>\n<td>[0.05, 0.10, 0.96]</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>[0.72, 0.66, 0.17]</td>\n</tr>\n<tr>\n<td>scientist</td>\n<td>[0.75, 0.62, 0.15]</td>\n</tr>\n<tr>\n<td>manager</td>\n<td>[0.05, 0.10, 0.96]</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>[0.72, 0.66, 0.17]</td>\n</tr>\n<tr>\n<td>engineer</td>\n<td>[0.72, 0.66, 0.17]</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>\n<p>NaN编码（NaN encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>为NaN值提供显式编码而不是忽略</li>\n<li>NaN值可以保存信息</li>\n<li>小心避免过度拟合</li>\n<li>仅在训练数据和测试数据中的Nan值是由相同的原因引起时才可以用，或者本地验证证明它持有信息</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>[‘NaN’]\n<table>\n<thead>\n<tr>\n<th>UA</th>\n<th>UA=mobile</th>\n<th>UA=tablet</th>\n<th>UA=NaN</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>mobile</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>tablet</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>mobile</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>NaN</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>mobile</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n</li>\n<li>编码： [0, 0, 1]</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>多项式编码（Polynomial encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>类别变量之间的编码交互</li>\n<li>没有交互的线性算法无法解决XOR问题</li>\n<li>多项式内核可以解决XOR问题</li>\n<li>爆炸式的特征空间： 使用FS，散列或者VW</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>A</th>\n<th>B</th>\n<th>y</th>\n<th>A=1*B=1</th>\n<th>A=0*B=1</th>\n<th>A=1*B=0</th>\n<th>A=0*B=0</th>\n<th>y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>\n<p>扩展编码（Expansion encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>从单个变量创建多个类别变量</li>\n<li>某些高基数功能（如用户代理）在其中包含更多信息：\n<ul>\n<li>is_mobile?</li>\n<li>is_latest_version?</li>\n<li>Operation_system</li>\n<li>Browser_build</li>\n<li>etc…</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>例子\n<ul>\n<li>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36<br>\n==&gt;\n<table>\n<thead>\n<tr>\n<th>UA1</th>\n<th>UA2</th>\n<th>UA3</th>\n<th>UA4</th>\n<th>UA5</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Chrome</td>\n<td>53.0.2785.143</td>\n<td>Desktop</td>\n<td>Mac</td>\n<td>10_10_4</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>合并编码（Consolidation encoding）</p>\n<ul>\n<li>概念\n<ul>\n<li>将不同的类别变量映射到同一个变量</li>\n<li>拼写错误，略有不同的职位描述，全名 vs 缩写</li>\n<li>真实数据很乱，自由文本尤其如此</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>company_desc   =&gt;</th>\n<th>desc1</th>\n<th>company_desc2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Shell</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>shel</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>SHELL</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>Shell Gasonline</td>\n<td>Shell</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>BP</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>British Petr.</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>B&amp;P</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>BP Gas Station</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>bp</td>\n<td>BP</td>\n<td>Gas station</td>\n</tr>\n<tr>\n<td>Procter&amp;Gamble</td>\n<td>P&amp;G</td>\n<td>Manufacturer</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"数值特征\"><a class=\"markdownIt-Anchor\" href=\"#数值特征\"></a> 数值特征</h4>\n<ul>\n<li>特点\n<ul>\n<li>可以更容易地输入算法</li>\n<li>可以构成浮点数，计数，数字</li>\n<li>更容易估算缺失的数据</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>四舍五入（Rounding）\n<ul>\n<li>概念\n<ul>\n<li>四舍五入数值变量</li>\n<li>有损压缩： 保留数据的最重要特征</li>\n<li>有时太精确只是噪音</li>\n<li>四舍五入的变量可以视为类别变量</li>\n<li>可以在四舍五入之前做对数变换</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>age</th>\n<th>age1</th>\n<th>age2</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>23.6671</td>\n<td>23</td>\n<td>2</td>\n</tr>\n<tr>\n<td>23.8891</td>\n<td>23</td>\n<td>2</td>\n</tr>\n<tr>\n<td>22.1261</td>\n<td>22</td>\n<td>2</td>\n</tr>\n<tr>\n<td>19.5506</td>\n<td>19</td>\n<td>1</td>\n</tr>\n<tr>\n<td>18.2114</td>\n<td>18</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>分档（Binning）\n<ul>\n<li>概念\n<ul>\n<li>将数值变量放入bin（档）中并使用bin-ID进行编码</li>\n<li>分档可以务实设置，通过分位数，均匀的，或使用模型到最佳分档</li>\n<li>可以使得训练集所见范围以外的变量正常工作</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>risk_score</th>\n<th>rs[-inf, 33]</th>\n<th>rs[33,66]</th>\n<th>rs[66,inf]</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>15</td>\n<td>1</td>\n<td>0</td>\n<td>0</td>\n</tr>\n<tr>\n<td>77</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>78</td>\n<td>0</td>\n<td>0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>55</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n</tr>\n<tr>\n<td>42</td>\n<td>0</td>\n<td>1</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>缩放（Scaling）\n<ul>\n<li>概念\n<ul>\n<li>将数值变量扩展到特定范围</li>\n<li>Standard（Z）缩放</li>\n<li>MinMax 缩放</li>\n<li>Root 缩放</li>\n<li>Log 缩放</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>估算（Imputation）\n<ul>\n<li>概念\n<ul>\n<li>估算缺失的变量</li>\n<li>硬编码可与估算相结合</li>\n<li>平均：非常基本的</li>\n<li>中位数：对异常值鲁棒性比较高</li>\n<li>忽略： 只是推迟问题</li>\n<li>使用模型：可以暴露算法的偏差</li>\n</ul>\n</li>\n<li>例子\n<table>\n<thead>\n<tr>\n<th>wage</th>\n<th>hours</th>\n<th>gender</th>\n<th>y =&gt;</th>\n<th>wage</th>\n<th>hours</th>\n<th>gender_y</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1600</td>\n<td>40</td>\n<td>0</td>\n<td>1</td>\n<td>1600</td>\n<td>40</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2200</td>\n<td>50</td>\n<td>1</td>\n<td>1</td>\n<td>2200</td>\n<td>50</td>\n<td>1</td>\n</tr>\n<tr>\n<td>1800</td>\n<td>36</td>\n<td>0</td>\n<td>0</td>\n<td>1800</td>\n<td>36</td>\n<td>0</td>\n</tr>\n<tr>\n<td>2100</td>\n<td>45</td>\n<td>1</td>\n<td>0</td>\n<td>2100</td>\n<td>45</td>\n<td>?</td>\n</tr>\n<tr>\n<td>2050</td>\n<td>60</td>\n<td>NaN</td>\n<td>0</td>\n<td>2050</td>\n<td>60</td>\n<td>?</td>\n</tr>\n<tr>\n<td>1650</td>\n<td>36</td>\n<td>0</td>\n<td>1</td>\n<td>1650</td>\n<td>36</td>\n<td>?</td>\n</tr>\n</tbody>\n</table>\n</li>\n</ul>\n</li>\n<li>相互作用（Interactions）\n<ul>\n<li>概念\n<ul>\n<li>具体来说编码数值变量之间的相互作用</li>\n<li>尝试：减法，加法，乘法，除法</li>\n<li>使用：通过统 计测试选择特征，或训练模型特征重要性</li>\n<li>忽略：人的直觉; 奇怪的相互作用可以带来显著的改进</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>线性算法的非线性编码（No-linear encoding for linear algo’s）\n<ul>\n<li>概念\n<ul>\n<li>硬编码非线性以改进线性算法</li>\n<li>多项式内核</li>\n<li>叶编码（随机森林嵌入）</li>\n<li>遗传算法</li>\n<li>局部线性嵌入，光谱嵌入，t-SNE</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>行统计（Row statistics）\n<ul>\n<li>概念\n<ul>\n<li>创建一行数据的统计信息</li>\n<li>NaN的数量</li>\n<li>0的数量</li>\n<li>负数值的数量</li>\n<li>平均值，最大值，最小值，偏度 等</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"时间特征-temporal-variables\"><a class=\"markdownIt-Anchor\" href=\"#时间特征-temporal-variables\"></a> 时间特征 (Temporal variables)</h4>\n<ul>\n<li>特点\n<ul>\n<li>时间变量，如日期，需要更好的本地验证方案（如回测）</li>\n<li>容易在这里犯错误</li>\n<li>很多机会进行重大改进</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>投射到一个圆\n<ul>\n<li>将单个要素（如day_of_week）转换为圆上的两个坐标</li>\n<li>确保max和min之间的距离与min和min +1相同</li>\n<li>用于day_of_week，day_of_month，hour_of_day等</li>\n</ul>\n</li>\n<li>趋势线（Trendlines）\n<ul>\n<li>不是编码总支出，而是编码上周花费，上个月花费，去年花费。</li>\n<li>给算法的趋势：两个支出相同的客户可能会有截然不同的行为 - 一个客户可能开始花费更多，而另一个客户开始减少支出。</li>\n</ul>\n</li>\n<li>与重大事件的接近度（Closeness to major events）\n<ul>\n<li>硬编码类别特征，如：date_3_days_before_holidays = 1</li>\n<li>尝试：国庆节，重大体育赛事，周末，一个月的第一个星期六等</li>\n<li>这些因素可能对支出行为产生重大影响</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"空间特征spatial-variables\"><a class=\"markdownIt-Anchor\" href=\"#空间特征spatial-variables\"></a> 空间特征（Spatial Variables）</h4>\n<ul>\n<li>特点\n<ul>\n<li>空间变量是在空间编码的位置的变量</li>\n<li>例子包括：GPS坐标，城市，国家，地址</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>分类位置（Categorizing location）\n<ul>\n<li>克里金法（Kriging）</li>\n<li>K-means聚类</li>\n<li>原始纬度经度</li>\n<li>将城市转换为纬度经度</li>\n<li>将邮政编码添加到街道名称</li>\n</ul>\n</li>\n<li>与枢纽的接近度（Closeness to Hubs）\n<ul>\n<li>找到位置与主要枢纽之间的紧密程度</li>\n<li>小城镇继承了附近大城市的一些文化/背景</li>\n<li>电话位置可以映射到附近的企业和超市</li>\n</ul>\n</li>\n<li>空间欺诈行为（Spatial fraudulent behavior）\n<ul>\n<li>位置事件数据可以指示可疑行为</li>\n<li>不可能的旅行速度：不同国家的多个同步交易</li>\n<li>在不同的城镇消费，而不是在家或送货地址</li>\n<li>永远不要在同一地点消费</li>\n</ul>\n</li>\n<li>探索（Exploration）\n<ul>\n<li>数据探索可以发现数据健康问题，异常值，噪声，特征工程想法，特征清洗想法</li>\n<li>可以使用：Console, Notebook, Pandas</li>\n<li>尝试简单的统计数据：最小值，最大值</li>\n<li>将目标结合起来，以便在信息之间找到相关性</li>\n</ul>\n</li>\n<li>迭代/调试（Iteration / Debugging）\n<ul>\n<li>特征工程是一个迭代过程：使你的pipelines适合快速迭代</li>\n<li>使用子线性调试：输出有关过程的中间信息，进行伪log</li>\n<li>使用允许快速实验的工具</li>\n<li>失败的想法会多余成功的想法</li>\n</ul>\n</li>\n<li>标签工程（Label Engineering）\n<ul>\n<li>可以将标签/目标/因变量视为数据的一个特征，反之亦然</li>\n<li>对数变换：y  - &gt; log（y + 1）| exp（y_pred） -  1</li>\n<li>Square 变换</li>\n<li>Box-Cox变换</li>\n<li>创建分数，在回归中转换二进制目标</li>\n<li>训练回归器以预测测试集中不可用的特征</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"自然语言处理natural-language-processing\"><a class=\"markdownIt-Anchor\" href=\"#自然语言处理natural-language-processing\"></a> 自然语言处理（Natural Language Processing）</h4>\n<ul>\n<li>特点\n<ul>\n<li>可以用类别特征中相同的方法</li>\n<li>深度学习（自动特征工程）正在慢慢的吞噬这个领域，但是具有好的特征工程的浅学习仍然具有竞争力</li>\n<li>高稀疏性的数据将带来“维度诅咒”</li>\n<li>特征工程具有很多机会</li>\n</ul>\n</li>\n<li>方法\n<ul>\n<li>所有方法列表\n<ul>\n<li>转换成小写字母</li>\n<li>删除非字母数字</li>\n<li>修复</li>\n<li>编码标点符号</li>\n<li>符号化</li>\n<li>令牌克（Token-grams）</li>\n<li>skipgrams</li>\n<li>char-grams</li>\n<li>删除停用词</li>\n<li>删除罕见的单词</li>\n<li>非常常见的词</li>\n<li>拼写纠正</li>\n<li>砍字</li>\n<li>词干</li>\n<li>词形还原</li>\n<li>文档特征</li>\n<li>实体的插入和提取</li>\n<li>简化</li>\n<li>Word2Vec 和 GloVe / Doc2Vec</li>\n<li>字符串相似性</li>\n<li>阅读水平</li>\n<li>最近邻居</li>\n<li>TF*IDF</li>\n<li>BayesSVM，矢量化，LDA，LSA</li>\n</ul>\n</li>\n<li>清洗（Cleaning）\n<ul>\n<li>转换成小写字母：使标记独立于大写：“I work at NASA” -&gt; “i work at nasa”.</li>\n<li>转换成Unidecode：将字符转换为ascii-对应物：</li>\n<li>删除非字母数字：删除不在[a-z] [A-Z] [0-9]中的任何内容来清理文本，“Breaking! Amsterdam (2009)” -&gt; “Breaking Amsterdam 2009”</li>\n<li>修复：修复编码问题或修剪intertoken空格。 “C a s a C a f＆eacute;” - &gt;“CasaCafé”</li>\n</ul>\n</li>\n<li>符号化（Tokenizing）\n<ul>\n<li>编码标点符号：硬编码“！”和“？”作为标记。</li>\n<li>符号化（Tokenize）：划分句子标记成单词记号</li>\n<li>N-Grams：将连续的符号编码为符号,  “I like the Beatles” -&gt; [“I like”, “like the”, “the Beatles”]</li>\n<li>Skip-grams: 编码连续的符号，但跳过一些,  “I like the Beatles” -&gt; [“I the”, “like Beatles”]</li>\n<li>Char-grams: 与N-gram相同，但字符级别, “Beatles” - &gt; [“Bea”, “eat”, “atl”, “tle”, “les”]</li>\n<li>Affixes：与char-gram相同，但是仅限于前缀与后缀</li>\n</ul>\n</li>\n<li>删除（Removing）\n<ul>\n<li>停用词：删除停用词列表中出现的单词/标记</li>\n<li>稀有单词：删除仅在训练集中出现几次的单词</li>\n<li>常用词：删除可能不在停用词列表中的极其常见的词</li>\n</ul>\n</li>\n<li>根（Roots）\n<ul>\n<li>拼写纠正：将字符更改为正确的拼写</li>\n<li>切（chop）： 仅取一个单词的前n（8）个字符</li>\n<li>词干：将词/标记减少到其根， “cars” -&gt; “car”</li>\n<li>Lemmatize：找到语义根， “never be late” -&gt; “never are late”</li>\n</ul>\n</li>\n<li>丰富（Enrich）\n<ul>\n<li>文档特征：计算空格数量，tab数量，换行数量，字符数量，以及令牌数量等</li>\n<li>实体插入：向文本中插入更多的通用的规范， “Microsoft releases Windows” -&gt; “Microsoft (company) releases Windows (application)”</li>\n<li>解析树：将句子解析为逻辑形式，“Alice hits Bill” -&gt; Alice/Noun_subject hits/Verb Bill/Noun_object.</li>\n<li>阅读级别：计算文档的阅读级别</li>\n</ul>\n</li>\n<li>相似性（Similarities）\n<ul>\n<li>令牌相似性：计算出现在两个文本中的令牌数</li>\n<li>压缩距离：查看是否可以使用其他文本更好地压缩一个文本</li>\n<li>Levenshtein / Hamming / Jaccard 距离：通过查看在另一个字符串中转换一个字符串所需的操作数来检查两个字符串之间的相似性</li>\n<li>Word2Vec / Glove：检查两个平均向量之间的余弦相似度</li>\n</ul>\n</li>\n<li>TF-IDF\n<ul>\n<li>术语频率：减少对长文档的偏差</li>\n<li>反向文档频率：减少对常见令牌的偏差</li>\n<li>TF-IDF：用于标识文档中最重要的标记，删除不重要的标记，或作为降维的预处理步骤</li>\n</ul>\n</li>\n<li>降维\n<ul>\n<li>PCA：将文本缩小为50或100维向量</li>\n<li>SVD：将文本缩小为50或100维向量</li>\n<li>LDA：TF-IDF，然后是SVD</li>\n<li>LSA：创建主题向量</li>\n</ul>\n</li>\n<li>外部模型\n<ul>\n<li>情绪分析器：为任何文本获取负面或正面情绪的向量</li>\n<li>主题模型：使用另一个数据集为新数据集创建主题向量</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"深度学习神经网络\"><a class=\"markdownIt-Anchor\" href=\"#深度学习神经网络\"></a> 深度学习/神经网络</h4>\n<ul>\n<li>神经网络声称是端到端的自动特征工程。所以特征工程没用了么？并不是这样的。特征工程将重点转移到架构工程。</li>\n<li>尽管承诺：计算机视觉使用的特征， 例如：\n<ul>\n<li>HOG</li>\n<li>SIFT</li>\n<li>whitening</li>\n<li>perturbation</li>\n<li>image pyramids</li>\n<li>rotation</li>\n<li>z-scaling</li>\n<li>log-scaling</li>\n<li>frame- grams</li>\n<li>external semantic data</li>\n<li>…</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"leakage-golden-features\"><a class=\"markdownIt-Anchor\" href=\"#leakage-golden-features\"></a> Leakage / Golden Features</h4>\n<ul>\n<li>特征工程可以帮助利用泄露</li>\n<li>逆向工程\n<ul>\n<li>使用rainbow表反转 MD5 Hash</li>\n<li>将 TF-IDF反转回术语频率</li>\n<li>编码样本数据集的顺序</li>\n<li>编码文件创建日期</li>\n</ul>\n</li>\n<li>规则挖掘\n<ul>\n<li>查找简单的规则（并对其进行编码）以帮助您的模型</li>\n</ul>\n</li>\n</ul>\n<!-- $$\nf(n) = \\begin{cases}\n \\frac{n}{2},\n & \\text{if } n\\text{ is even}\n \\\\ 3n+1, & \\text{if } n\\text{ is odd}\n \\end{cases}\n$$ -->\n<h3 id=\"参考资料\"><a class=\"markdownIt-Anchor\" href=\"#参考资料\"></a> 参考资料</h3>\n<p><a href=\"https://www.slideshare.net/HJvanVeen/feature-engineering-72376750\" target=\"_blank\" rel=\"noopener\">Tips &amp; Tricks for Feature Engineering / Applied Machine Learning</a><br>\n[]</p>\n"},{"title":"2019年8月-12月学习计划","catalog":true,"toc_nav_num":true,"date":"2019-07-31T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["学习计划"],"_content":"\n> 学无止境，学海无涯\n>\n> 专注深入的学习技术\n\n### 机器学习基础\n#### 机器学习概念、应用与前沿\n* 内容\n  机器学习技术和应用场景的介绍。常见算法，主流的应用构建方法。主流机器学习框架介绍，针对机器学习场景能够更好的应用相关工具进行分析与处理。\n* 项目\n  - 鸢尾花分类实战\n  - 分类预测实战\n  - 回归预测实战\n\n#### 数学基础-数学概念\n* 内容\n  机器学习中用到的数学基础由浅入深进行详细的梳理与讲解。主要涉及矩阵、导数、概率相关内容。\n  概率论，矩阵和凸优化的介绍，相应算法设计和原理；凸优化理论，流优化手段SGD等优化方法。\n* 项目\n  - 手写识别实战\n  - 文本降维实战\n#### 特征工程 & 可视化\n* 内容\n  Python数据预处理库，原始数据特征构建。特征选择，构建新特征，缺失值填充等特征工程方法学习。\n* 项目\n  - Scikit-learn特征工程，网格搜索，超参数调优。\n  - 泰坦尼克求生预测\n\n### 机器学习算法学习\n#### 决策树与随机森林算法\n* 内容\n  决策树算法的原理，度量指标和算法变种。掌握和了解GBDT，AdaBoost，随机森林等集成学习模型的原理和集成学习算法。\n* 项目\n  - 鸢尾花分类实战\n  - 金融反欺诈预测\n\n#### 分类算法\n* 内容\n  - 了解和掌握KNN、SVM及朴素贝叶斯算法原理。\n  - 熟悉集成学习对于分类算法的优化过程。掌握数据降维方法应用。\n  - 熟悉分类算法调参关键参数\n  - 掌握不同分类算法的过拟合、欠拟合情景与调优\n  - 掌握集成学习调优\n  - 了解不同算法的共性与个性\n* 项目\n  - 手写图形数据降维与分类\n  - 文本向量化实战\n  - 文本分类实战\n\n#### 回归算法\n* 内容\n  - 主流回归模型，线性回归，逻辑回归LR、Softmax及其变种和扩展算法。\n  - 梯度下降，牛顿法等优化方法，逻辑回归最优化问题的求解，正则化方法。\n* 项目\n  - 波士顿房价预测\n  - 股票预测回归实战\n\n### 大数据框架应用\n#### 聚类算法\n* 内容\n  无监督学习模型，了解主流的聚类算法。了解不同相似度计算算法。深入了解不同的数据降维方法。掌握文本降维方法(LDA)。\n  - 掌握 Kmeans 以及其衍生算法\n  - 掌握 modelbased 聚类方法\n  - 掌握无监督降维方法：PCA、ICA、字典学习\n  - 掌握监督降维方法：LDA\n  - 掌握文本降维方法：LDA\n  - 深入理解聚类算法与分类算法的区别\n  - 理解聚类算法的优缺点\n* 项目\n  - 新闻分类实战\n  - 文本降维实战\n\n#### 深度学习框架 TensorFlow\n* 内容\n  通过掌握 Tensorflow 基本概念，计算模型 和原理，能够通过 Tensorflow 进行深度学习和模型构建与训练。学习掌握训练过程优化方法与问题优化。\n  - 学习变量作用域与变量命名\n  - 搭建多层神经网络并完成优化\n  - 正则化优化神经网络\n  - 梯度问题与解决方法\n* 项目\n  - 图片分类实战\n  - 贷款欺诈预测\n\n#### 大数据\n* 内容\n  大数据主流分析框架为例， Spark 内核架构，计算模型和原理，了解分布式机器学习原理，能够处理和解决大规模数据分析预处理和模型训练。\n  - 了解和掌握 Spark 框架上的机器学习库 MLlib 的算法原理，核心数据抽象，以及应 用 MLlib。\n* 项目\n  - 电影推荐案例","source":"_posts/2019-07-31-2019-下半年学习计划.md","raw":"---\ntitle: \"2019年8月-12月学习计划\"\ncatalog: true\ntoc_nav_num: true\n# mathjax: true\ndate: 2019-07-31 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 学习计划\ncatagories:\n- 学习计划\n\n---\n\n> 学无止境，学海无涯\n>\n> 专注深入的学习技术\n\n### 机器学习基础\n#### 机器学习概念、应用与前沿\n* 内容\n  机器学习技术和应用场景的介绍。常见算法，主流的应用构建方法。主流机器学习框架介绍，针对机器学习场景能够更好的应用相关工具进行分析与处理。\n* 项目\n  - 鸢尾花分类实战\n  - 分类预测实战\n  - 回归预测实战\n\n#### 数学基础-数学概念\n* 内容\n  机器学习中用到的数学基础由浅入深进行详细的梳理与讲解。主要涉及矩阵、导数、概率相关内容。\n  概率论，矩阵和凸优化的介绍，相应算法设计和原理；凸优化理论，流优化手段SGD等优化方法。\n* 项目\n  - 手写识别实战\n  - 文本降维实战\n#### 特征工程 & 可视化\n* 内容\n  Python数据预处理库，原始数据特征构建。特征选择，构建新特征，缺失值填充等特征工程方法学习。\n* 项目\n  - Scikit-learn特征工程，网格搜索，超参数调优。\n  - 泰坦尼克求生预测\n\n### 机器学习算法学习\n#### 决策树与随机森林算法\n* 内容\n  决策树算法的原理，度量指标和算法变种。掌握和了解GBDT，AdaBoost，随机森林等集成学习模型的原理和集成学习算法。\n* 项目\n  - 鸢尾花分类实战\n  - 金融反欺诈预测\n\n#### 分类算法\n* 内容\n  - 了解和掌握KNN、SVM及朴素贝叶斯算法原理。\n  - 熟悉集成学习对于分类算法的优化过程。掌握数据降维方法应用。\n  - 熟悉分类算法调参关键参数\n  - 掌握不同分类算法的过拟合、欠拟合情景与调优\n  - 掌握集成学习调优\n  - 了解不同算法的共性与个性\n* 项目\n  - 手写图形数据降维与分类\n  - 文本向量化实战\n  - 文本分类实战\n\n#### 回归算法\n* 内容\n  - 主流回归模型，线性回归，逻辑回归LR、Softmax及其变种和扩展算法。\n  - 梯度下降，牛顿法等优化方法，逻辑回归最优化问题的求解，正则化方法。\n* 项目\n  - 波士顿房价预测\n  - 股票预测回归实战\n\n### 大数据框架应用\n#### 聚类算法\n* 内容\n  无监督学习模型，了解主流的聚类算法。了解不同相似度计算算法。深入了解不同的数据降维方法。掌握文本降维方法(LDA)。\n  - 掌握 Kmeans 以及其衍生算法\n  - 掌握 modelbased 聚类方法\n  - 掌握无监督降维方法：PCA、ICA、字典学习\n  - 掌握监督降维方法：LDA\n  - 掌握文本降维方法：LDA\n  - 深入理解聚类算法与分类算法的区别\n  - 理解聚类算法的优缺点\n* 项目\n  - 新闻分类实战\n  - 文本降维实战\n\n#### 深度学习框架 TensorFlow\n* 内容\n  通过掌握 Tensorflow 基本概念，计算模型 和原理，能够通过 Tensorflow 进行深度学习和模型构建与训练。学习掌握训练过程优化方法与问题优化。\n  - 学习变量作用域与变量命名\n  - 搭建多层神经网络并完成优化\n  - 正则化优化神经网络\n  - 梯度问题与解决方法\n* 项目\n  - 图片分类实战\n  - 贷款欺诈预测\n\n#### 大数据\n* 内容\n  大数据主流分析框架为例， Spark 内核架构，计算模型和原理，了解分布式机器学习原理，能够处理和解决大规模数据分析预处理和模型训练。\n  - 了解和掌握 Spark 框架上的机器学习库 MLlib 的算法原理，核心数据抽象，以及应 用 MLlib。\n* 项目\n  - 电影推荐案例","slug":"2019-07-31-2019-下半年学习计划","published":1,"updated":"2019-08-07T06:14:51.231Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqfa0009z0ovp1u8d9tw","content":"<blockquote>\n<p>学无止境，学海无涯</p>\n<p>专注深入的学习技术</p>\n</blockquote>\n<h3><span id=\"机器学习基础\"> 机器学习基础</span></h3>\n<h4><span id=\"机器学习概念-应用与前沿\"> 机器学习概念、应用与前沿</span></h4>\n<ul>\n<li>内容<br>\n机器学习技术和应用场景的介绍。常见算法，主流的应用构建方法。主流机器学习框架介绍，针对机器学习场景能够更好的应用相关工具进行分析与处理。</li>\n<li>项目\n<ul>\n<li>鸢尾花分类实战</li>\n<li>分类预测实战</li>\n<li>回归预测实战</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"数学基础-数学概念\"> 数学基础-数学概念</span></h4>\n<ul>\n<li>内容<br>\n机器学习中用到的数学基础由浅入深进行详细的梳理与讲解。主要涉及矩阵、导数、概率相关内容。<br>\n概率论，矩阵和凸优化的介绍，相应算法设计和原理；凸优化理论，流优化手段SGD等优化方法。</li>\n<li>项目\n<ul>\n<li>手写识别实战</li>\n<li>文本降维实战</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"特征工程-amp-可视化\"> 特征工程 &amp; 可视化</span></h4>\n<ul>\n<li>内容<br>\nPython数据预处理库，原始数据特征构建。特征选择，构建新特征，缺失值填充等特征工程方法学习。</li>\n<li>项目\n<ul>\n<li>Scikit-learn特征工程，网格搜索，超参数调优。</li>\n<li>泰坦尼克求生预测</li>\n</ul>\n</li>\n</ul>\n<h3><span id=\"机器学习算法学习\"> 机器学习算法学习</span></h3>\n<h4><span id=\"决策树与随机森林算法\"> 决策树与随机森林算法</span></h4>\n<ul>\n<li>内容<br>\n决策树算法的原理，度量指标和算法变种。掌握和了解GBDT，AdaBoost，随机森林等集成学习模型的原理和集成学习算法。</li>\n<li>项目\n<ul>\n<li>鸢尾花分类实战</li>\n<li>金融反欺诈预测</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"分类算法\"> 分类算法</span></h4>\n<ul>\n<li>内容\n<ul>\n<li>了解和掌握KNN、SVM及朴素贝叶斯算法原理。</li>\n<li>熟悉集成学习对于分类算法的优化过程。掌握数据降维方法应用。</li>\n<li>熟悉分类算法调参关键参数</li>\n<li>掌握不同分类算法的过拟合、欠拟合情景与调优</li>\n<li>掌握集成学习调优</li>\n<li>了解不同算法的共性与个性</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>手写图形数据降维与分类</li>\n<li>文本向量化实战</li>\n<li>文本分类实战</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"回归算法\"> 回归算法</span></h4>\n<ul>\n<li>内容\n<ul>\n<li>主流回归模型，线性回归，逻辑回归LR、Softmax及其变种和扩展算法。</li>\n<li>梯度下降，牛顿法等优化方法，逻辑回归最优化问题的求解，正则化方法。</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>波士顿房价预测</li>\n<li>股票预测回归实战</li>\n</ul>\n</li>\n</ul>\n<h3><span id=\"大数据框架应用\"> 大数据框架应用</span></h3>\n<h4><span id=\"聚类算法\"> 聚类算法</span></h4>\n<ul>\n<li>内容<br>\n无监督学习模型，了解主流的聚类算法。了解不同相似度计算算法。深入了解不同的数据降维方法。掌握文本降维方法(LDA)。\n<ul>\n<li>掌握 Kmeans 以及其衍生算法</li>\n<li>掌握 modelbased 聚类方法</li>\n<li>掌握无监督降维方法：PCA、ICA、字典学习</li>\n<li>掌握监督降维方法：LDA</li>\n<li>掌握文本降维方法：LDA</li>\n<li>深入理解聚类算法与分类算法的区别</li>\n<li>理解聚类算法的优缺点</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>新闻分类实战</li>\n<li>文本降维实战</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"深度学习框架-tensorflow\"> 深度学习框架 TensorFlow</span></h4>\n<ul>\n<li>内容<br>\n通过掌握 Tensorflow 基本概念，计算模型 和原理，能够通过 Tensorflow 进行深度学习和模型构建与训练。学习掌握训练过程优化方法与问题优化。\n<ul>\n<li>学习变量作用域与变量命名</li>\n<li>搭建多层神经网络并完成优化</li>\n<li>正则化优化神经网络</li>\n<li>梯度问题与解决方法</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>图片分类实战</li>\n<li>贷款欺诈预测</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"大数据\"> 大数据</span></h4>\n<ul>\n<li>内容<br>\n大数据主流分析框架为例， Spark 内核架构，计算模型和原理，了解分布式机器学习原理，能够处理和解决大规模数据分析预处理和模型训练。\n<ul>\n<li>了解和掌握 Spark 框架上的机器学习库 MLlib 的算法原理，核心数据抽象，以及应 用 MLlib。</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>电影推荐案例</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>学无止境，学海无涯</p>\n<p>专注深入的学习技术</p>\n</blockquote>\n<h3 id=\"机器学习基础\"><a class=\"markdownIt-Anchor\" href=\"#机器学习基础\"></a> 机器学习基础</h3>\n<h4 id=\"机器学习概念-应用与前沿\"><a class=\"markdownIt-Anchor\" href=\"#机器学习概念-应用与前沿\"></a> 机器学习概念、应用与前沿</h4>\n<ul>\n<li>内容<br>\n机器学习技术和应用场景的介绍。常见算法，主流的应用构建方法。主流机器学习框架介绍，针对机器学习场景能够更好的应用相关工具进行分析与处理。</li>\n<li>项目\n<ul>\n<li>鸢尾花分类实战</li>\n<li>分类预测实战</li>\n<li>回归预测实战</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"数学基础-数学概念\"><a class=\"markdownIt-Anchor\" href=\"#数学基础-数学概念\"></a> 数学基础-数学概念</h4>\n<ul>\n<li>内容<br>\n机器学习中用到的数学基础由浅入深进行详细的梳理与讲解。主要涉及矩阵、导数、概率相关内容。<br>\n概率论，矩阵和凸优化的介绍，相应算法设计和原理；凸优化理论，流优化手段SGD等优化方法。</li>\n<li>项目\n<ul>\n<li>手写识别实战</li>\n<li>文本降维实战</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"特征工程-可视化\"><a class=\"markdownIt-Anchor\" href=\"#特征工程-可视化\"></a> 特征工程 &amp; 可视化</h4>\n<ul>\n<li>内容<br>\nPython数据预处理库，原始数据特征构建。特征选择，构建新特征，缺失值填充等特征工程方法学习。</li>\n<li>项目\n<ul>\n<li>Scikit-learn特征工程，网格搜索，超参数调优。</li>\n<li>泰坦尼克求生预测</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"机器学习算法学习\"><a class=\"markdownIt-Anchor\" href=\"#机器学习算法学习\"></a> 机器学习算法学习</h3>\n<h4 id=\"决策树与随机森林算法\"><a class=\"markdownIt-Anchor\" href=\"#决策树与随机森林算法\"></a> 决策树与随机森林算法</h4>\n<ul>\n<li>内容<br>\n决策树算法的原理，度量指标和算法变种。掌握和了解GBDT，AdaBoost，随机森林等集成学习模型的原理和集成学习算法。</li>\n<li>项目\n<ul>\n<li>鸢尾花分类实战</li>\n<li>金融反欺诈预测</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"分类算法\"><a class=\"markdownIt-Anchor\" href=\"#分类算法\"></a> 分类算法</h4>\n<ul>\n<li>内容\n<ul>\n<li>了解和掌握KNN、SVM及朴素贝叶斯算法原理。</li>\n<li>熟悉集成学习对于分类算法的优化过程。掌握数据降维方法应用。</li>\n<li>熟悉分类算法调参关键参数</li>\n<li>掌握不同分类算法的过拟合、欠拟合情景与调优</li>\n<li>掌握集成学习调优</li>\n<li>了解不同算法的共性与个性</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>手写图形数据降维与分类</li>\n<li>文本向量化实战</li>\n<li>文本分类实战</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"回归算法\"><a class=\"markdownIt-Anchor\" href=\"#回归算法\"></a> 回归算法</h4>\n<ul>\n<li>内容\n<ul>\n<li>主流回归模型，线性回归，逻辑回归LR、Softmax及其变种和扩展算法。</li>\n<li>梯度下降，牛顿法等优化方法，逻辑回归最优化问题的求解，正则化方法。</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>波士顿房价预测</li>\n<li>股票预测回归实战</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"大数据框架应用\"><a class=\"markdownIt-Anchor\" href=\"#大数据框架应用\"></a> 大数据框架应用</h3>\n<h4 id=\"聚类算法\"><a class=\"markdownIt-Anchor\" href=\"#聚类算法\"></a> 聚类算法</h4>\n<ul>\n<li>内容<br>\n无监督学习模型，了解主流的聚类算法。了解不同相似度计算算法。深入了解不同的数据降维方法。掌握文本降维方法(LDA)。\n<ul>\n<li>掌握 Kmeans 以及其衍生算法</li>\n<li>掌握 modelbased 聚类方法</li>\n<li>掌握无监督降维方法：PCA、ICA、字典学习</li>\n<li>掌握监督降维方法：LDA</li>\n<li>掌握文本降维方法：LDA</li>\n<li>深入理解聚类算法与分类算法的区别</li>\n<li>理解聚类算法的优缺点</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>新闻分类实战</li>\n<li>文本降维实战</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"深度学习框架-tensorflow\"><a class=\"markdownIt-Anchor\" href=\"#深度学习框架-tensorflow\"></a> 深度学习框架 TensorFlow</h4>\n<ul>\n<li>内容<br>\n通过掌握 Tensorflow 基本概念，计算模型 和原理，能够通过 Tensorflow 进行深度学习和模型构建与训练。学习掌握训练过程优化方法与问题优化。\n<ul>\n<li>学习变量作用域与变量命名</li>\n<li>搭建多层神经网络并完成优化</li>\n<li>正则化优化神经网络</li>\n<li>梯度问题与解决方法</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>图片分类实战</li>\n<li>贷款欺诈预测</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"大数据\"><a class=\"markdownIt-Anchor\" href=\"#大数据\"></a> 大数据</h4>\n<ul>\n<li>内容<br>\n大数据主流分析框架为例， Spark 内核架构，计算模型和原理，了解分布式机器学习原理，能够处理和解决大规模数据分析预处理和模型训练。\n<ul>\n<li>了解和掌握 Spark 框架上的机器学习库 MLlib 的算法原理，核心数据抽象，以及应 用 MLlib。</li>\n</ul>\n</li>\n<li>项目\n<ul>\n<li>电影推荐案例</li>\n</ul>\n</li>\n</ul>\n"},{"title":"工具以及框架总结","catalog":true,"toc_nav_num":true,"date":"2019-08-02T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["工具以及框架总结"],"_content":"\n\n#### 数据科学\n* 语言\n  - python\n* 包\n  - numpy\n  - pandas\n  - sqlalchemy\n  - lxml\n  - html5lib\n  - BeautifulSoup4\n  - [nltk](https://www.nltk.org)\n* 数据\n  - [UCI Data set](https://archive.ics.uci.edu/ml/datasets.php)\n* 数据可视化\n  - Matplotlib\n  - Seaborn\n  - Pandas内嵌数据可视化\n  - Plotly and Cufflinks\n  - Geographical Plotting\n* 框架\n  - Scikit Learn\n  -\n\n* 算法\n  - Linear Regression\n  - Logistic Regression\n  - K Nearest Neighbors\n  - Decision Trees and Random Forests\n  - Support Vector Machines\n  - K Means Clustering\n  - Natural Language Processing\n  - Neural Nets and Deep Learning\n* 方法\n  - Cross Validation and Bias-Variance Trade-off\n  - Principal Component Analysis","source":"_posts/2019-08-02-框架以及工具总结.md","raw":"---\ntitle: \"工具以及框架总结\"\ncatalog: true\ntoc_nav_num: true\n# mathjax: true\ndate: 2019-08-02 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 前端，数据科学\ncatagories:\n- 工具以及框架总结\n\n---\n\n\n#### 数据科学\n* 语言\n  - python\n* 包\n  - numpy\n  - pandas\n  - sqlalchemy\n  - lxml\n  - html5lib\n  - BeautifulSoup4\n  - [nltk](https://www.nltk.org)\n* 数据\n  - [UCI Data set](https://archive.ics.uci.edu/ml/datasets.php)\n* 数据可视化\n  - Matplotlib\n  - Seaborn\n  - Pandas内嵌数据可视化\n  - Plotly and Cufflinks\n  - Geographical Plotting\n* 框架\n  - Scikit Learn\n  -\n\n* 算法\n  - Linear Regression\n  - Logistic Regression\n  - K Nearest Neighbors\n  - Decision Trees and Random Forests\n  - Support Vector Machines\n  - K Means Clustering\n  - Natural Language Processing\n  - Neural Nets and Deep Learning\n* 方法\n  - Cross Validation and Bias-Variance Trade-off\n  - Principal Component Analysis","slug":"2019-08-02-框架以及工具总结","published":1,"updated":"2019-08-13T03:16:15.220Z","_id":"cjz3pnqfc000cz0ovzv9xe34x","comments":1,"layout":"post","photos":[],"link":"","content":"<h4><span id=\"数据科学\"> 数据科学</span></h4>\n<ul>\n<li>\n<p>语言</p>\n<ul>\n<li>python</li>\n</ul>\n</li>\n<li>\n<p>包</p>\n<ul>\n<li>numpy</li>\n<li>pandas</li>\n<li>sqlalchemy</li>\n<li>lxml</li>\n<li>html5lib</li>\n<li>BeautifulSoup4</li>\n<li><a href=\"https://www.nltk.org\" target=\"_blank\" rel=\"noopener\">nltk</a></li>\n</ul>\n</li>\n<li>\n<p>数据</p>\n<ul>\n<li><a href=\"https://archive.ics.uci.edu/ml/datasets.php\" target=\"_blank\" rel=\"noopener\">UCI Data set</a></li>\n</ul>\n</li>\n<li>\n<p>数据可视化</p>\n<ul>\n<li>Matplotlib</li>\n<li>Seaborn</li>\n<li>Pandas内嵌数据可视化</li>\n<li>Plotly and Cufflinks</li>\n<li>Geographical Plotting</li>\n</ul>\n</li>\n<li>\n<p>框架</p>\n<ul>\n<li>Scikit Learn</li>\n<li></li>\n</ul>\n</li>\n<li>\n<p>算法</p>\n<ul>\n<li>Linear Regression</li>\n<li>Logistic Regression</li>\n<li>K Nearest Neighbors</li>\n<li>Decision Trees and Random Forests</li>\n<li>Support Vector Machines</li>\n<li>K Means Clustering</li>\n<li>Natural Language Processing</li>\n<li>Neural Nets and Deep Learning</li>\n</ul>\n</li>\n<li>\n<p>方法</p>\n<ul>\n<li>Cross Validation and Bias-Variance Trade-off</li>\n<li>Principal Component Analysis</li>\n</ul>\n</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"数据科学\"><a class=\"markdownIt-Anchor\" href=\"#数据科学\"></a> 数据科学</h4>\n<ul>\n<li>\n<p>语言</p>\n<ul>\n<li>python</li>\n</ul>\n</li>\n<li>\n<p>包</p>\n<ul>\n<li>numpy</li>\n<li>pandas</li>\n<li>sqlalchemy</li>\n<li>lxml</li>\n<li>html5lib</li>\n<li>BeautifulSoup4</li>\n<li><a href=\"https://www.nltk.org\" target=\"_blank\" rel=\"noopener\">nltk</a></li>\n</ul>\n</li>\n<li>\n<p>数据</p>\n<ul>\n<li><a href=\"https://archive.ics.uci.edu/ml/datasets.php\" target=\"_blank\" rel=\"noopener\">UCI Data set</a></li>\n</ul>\n</li>\n<li>\n<p>数据可视化</p>\n<ul>\n<li>Matplotlib</li>\n<li>Seaborn</li>\n<li>Pandas内嵌数据可视化</li>\n<li>Plotly and Cufflinks</li>\n<li>Geographical Plotting</li>\n</ul>\n</li>\n<li>\n<p>框架</p>\n<ul>\n<li>Scikit Learn</li>\n<li></li>\n</ul>\n</li>\n<li>\n<p>算法</p>\n<ul>\n<li>Linear Regression</li>\n<li>Logistic Regression</li>\n<li>K Nearest Neighbors</li>\n<li>Decision Trees and Random Forests</li>\n<li>Support Vector Machines</li>\n<li>K Means Clustering</li>\n<li>Natural Language Processing</li>\n<li>Neural Nets and Deep Learning</li>\n</ul>\n</li>\n<li>\n<p>方法</p>\n<ul>\n<li>Cross Validation and Bias-Variance Trade-off</li>\n<li>Principal Component Analysis</li>\n</ul>\n</li>\n</ul>\n"},{"title":"线性回归 Linear Regression","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-06T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"\n#### 概念\n回归是为了预测数值型的目标值。\n线性回归假设特征和结果之间满足线性关系。\n求解回归方程的回归系数的过程就是回归。\n回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。\n\n#### 数学理论\n$$\ny=WX\n$$\n$W$ 是回归系数向量.\n给定输入是$X_{1}$, 那么预测结果就是 $y=X_{1}^{T}W$.\n\n现在有一些$X$和$y$, 我们的目的就是找到$W$。 常用的方法就是找出使误差最小的$W$。即预测值$y$与真实值$y$之间的差值。采用最小二乘法：\n$$\n\\sum_{i=1}^{n}(y_{i} - x_{i}^Tw)^2\n$$\n对$w$求导，得到$X^T(y-Xw)$, 令其等于零，得到\n$$\n\\hat{w} = (X^TX)^{-1}X^Ty\n$$\n#### 评估指标\n\n* **Mean Absolute Error** (MAE)：误差绝对值的平均值\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n* **Mean Squared Error** (MSE): 误差平方的均值\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n* **Root Mean Squared Error** (RMSE): 均方根误差\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n\n#### ScikitLearn 中的线性回归用法\n\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\nlm = LinearRegression()\n\nlm.fit(X_train, y_train)\nprint(lm.intercept_)\nprint(lm.coef_)\n\npredictions = lm.predict(X_test)\n\nprint('MAE: ', metrics.mean_absolute_error(y_test, predictions)\nprint('MSE: ', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, predictions))\n~~~\nlm.intercept_: 是 $y=WX + w_{0}$ 中的$w_{0}$\n\nlm.coef_： 是 $y=WX + w_{0}$ 中的$W$\n\n\n预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)：y_test - predictions 应该符合正态分布\n~~~\nsns.distplot((y_test - predictions), bins=50)\n~~~\n\n#### 数学知识补充\n最小二乘法估计\n$$\ny = m_{0} + m_{1}x + e\n$$\n其中误差项 $e$ 引入用以解释不确定性的因素。\n\n基本假设：\n1. 零均值假设：误差项是期望为零的随机变量，即 $E(e)=0$\n2. 不变方差假设：误差项 $e$ 的方差（用 $\\sigma^2$ 表示）是常数且与 $x_{1}$, $x_{2}$,…. 的值无关\n3. 独立性假设：$e$ 的变量是相互独立的\n4. 正态性假设：误差项 $e$ 是正态随机变量,也即：误差项 $e$ 的值是独立的正态分布随机变量，带有均值0和不变方差 $\\sigma^2$","source":"_posts/2019-08-06-线性回归.md","raw":"---\ntitle: \"线性回归 Linear Regression\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-06 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 回归算法\ncatagories:\n- ML 算法\n\n---\n\n#### 概念\n回归是为了预测数值型的目标值。\n线性回归假设特征和结果之间满足线性关系。\n求解回归方程的回归系数的过程就是回归。\n回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。\n\n#### 数学理论\n$$\ny=WX\n$$\n$W$ 是回归系数向量.\n给定输入是$X_{1}$, 那么预测结果就是 $y=X_{1}^{T}W$.\n\n现在有一些$X$和$y$, 我们的目的就是找到$W$。 常用的方法就是找出使误差最小的$W$。即预测值$y$与真实值$y$之间的差值。采用最小二乘法：\n$$\n\\sum_{i=1}^{n}(y_{i} - x_{i}^Tw)^2\n$$\n对$w$求导，得到$X^T(y-Xw)$, 令其等于零，得到\n$$\n\\hat{w} = (X^TX)^{-1}X^Ty\n$$\n#### 评估指标\n\n* **Mean Absolute Error** (MAE)：误差绝对值的平均值\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n* **Mean Squared Error** (MSE): 误差平方的均值\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n* **Root Mean Squared Error** (RMSE): 均方根误差\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\n\n#### ScikitLearn 中的线性回归用法\n\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\nlm = LinearRegression()\n\nlm.fit(X_train, y_train)\nprint(lm.intercept_)\nprint(lm.coef_)\n\npredictions = lm.predict(X_test)\n\nprint('MAE: ', metrics.mean_absolute_error(y_test, predictions)\nprint('MSE: ', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, predictions))\n~~~\nlm.intercept_: 是 $y=WX + w_{0}$ 中的$w_{0}$\n\nlm.coef_： 是 $y=WX + w_{0}$ 中的$W$\n\n\n预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)：y_test - predictions 应该符合正态分布\n~~~\nsns.distplot((y_test - predictions), bins=50)\n~~~\n\n#### 数学知识补充\n最小二乘法估计\n$$\ny = m_{0} + m_{1}x + e\n$$\n其中误差项 $e$ 引入用以解释不确定性的因素。\n\n基本假设：\n1. 零均值假设：误差项是期望为零的随机变量，即 $E(e)=0$\n2. 不变方差假设：误差项 $e$ 的方差（用 $\\sigma^2$ 表示）是常数且与 $x_{1}$, $x_{2}$,…. 的值无关\n3. 独立性假设：$e$ 的变量是相互独立的\n4. 正态性假设：误差项 $e$ 是正态随机变量,也即：误差项 $e$ 的值是独立的正态分布随机变量，带有均值0和不变方差 $\\sigma^2$","slug":"2019-08-06-线性回归","published":1,"updated":"2019-08-09T07:10:14.397Z","_id":"cjz3pnqff000dz0ovnj5493xa","comments":1,"layout":"post","photos":[],"link":"","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<p>回归是为了预测数值型的目标值。<br>\n线性回归假设特征和结果之间满足线性关系。<br>\n求解回归方程的回归系数的过程就是回归。<br>\n回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。</p>\n<h4><span id=\"数学理论\"> 数学理论</span></h4>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">y=WX\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 是回归系数向量.<br>\n给定输入是<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>X</mi><mrow><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">X_{1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.07847em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>, 那么预测结果就是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>X</mi><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></msubsup><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">y=X_{1}^{T}W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8413309999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.0894389999999998em;vertical-align:-0.24810799999999997em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:0.24810799999999997em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span>.</p>\n<p>现在有一些<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span>和<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, 我们的目的就是找到<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span>。 常用的方法就是找出使误差最小的<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span>。即预测值<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>与真实值<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>之间的差值。采用最小二乘法：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub><mo>−</mo><msubsup><mi>x</mi><mrow><mi>i</mi></mrow><mi>T</mi></msubsup><mi>w</mi><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{n}(y_{i} - x_{i}^Tw)^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathit\">n</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.247em;margin-left:0em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span style=\"top:-0.4129999999999999em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<p>对<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span></span></span></span>求导，得到<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>X</mi><mi>T</mi></msup><mo>(</mo><mi>y</mi><mo>−</mo><mi>X</mi><mi>w</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">X^T(y-Xw)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8413309999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span>, 令其等于零，得到</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mrow><mi>w</mi></mrow><mo>^</mo></mover><mo>=</mo><mo>(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo>)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">\\hat{w} = (X^TX)^{-1}X^Ty\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8913309999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.1413309999999999em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span></span></span><span style=\"top:0em;margin-left:0.16668em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.41300000000000003em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord\">−</span><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span></p>\n<h4><span id=\"评估指标\"> 评估指标</span></h4>\n<ul>\n<li><strong>Mean Absolute Error</strong> (MAE)：误差绝对值的平均值</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi mathvariant=\"normal\">∣</mi><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent=\"true\"><mrow><mi>y</mi></mrow><mo>^</mo></mover><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\">n</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:0em;margin-left:0.11112em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span></span></span></span></span></p>\n<ul>\n<li><strong>Mean Squared Error</strong> (MSE): 误差平方的均值</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent=\"true\"><mrow><mi>y</mi></mrow><mo>^</mo></mover><mi>i</mi></msub><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\">n</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:0em;margin-left:0.11112em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<ul>\n<li><strong>Root Mean Squared Error</strong> (RMSE): 均方根误差</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent=\"true\"><mrow><mi>y</mi></mrow><mo>^</mo></mover><mi>i</mi></msub><msup><mo>)</mo><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:2.128249em;\"></span><span class=\"strut bottom\" style=\"height:3.6550199999999995em;vertical-align:-1.5267709999999999em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"sqrt mord\"><span class=\"sqrt-sign\" style=\"top:-0.023254000000000108em;\"><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing mult\"><span class=\"vlist\"><span style=\"top:0.665005em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎷</span></span></span><span style=\"top:-0.24999499999999997em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span></span></span></span><span style=\"top:-0.854995em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span></span></span></span><span style=\"top:-1.459995em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord reset-textstyle displaystyle textstyle cramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:0em;margin-left:0.11112em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span style=\"top:-2.0482489999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped sqrt-line\"></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<h4><span id=\"scikitlearn-中的线性回归用法\"> ScikitLearn 中的线性回归用法</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.linear_model import LinearRegression</span><br><span class=\"line\">from sklearn import metrics</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)</span><br><span class=\"line\"></span><br><span class=\"line\">lm = LinearRegression()</span><br><span class=\"line\"></span><br><span class=\"line\">lm.fit(X_train, y_train)</span><br><span class=\"line\">print(lm.intercept_)</span><br><span class=\"line\">print(lm.coef_)</span><br><span class=\"line\"></span><br><span class=\"line\">predictions = lm.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&apos;MAE: &apos;, metrics.mean_absolute_error(y_test, predictions)</span><br><span class=\"line\">print(&apos;MSE: &apos;, metrics.mean_squared_error(y_test, predictions))</span><br><span class=\"line\">print(&apos;RMSE: &apos;, np.sqrt(metrics.mean_squared_error(y_test, predictions))</span><br></pre></td></tr></table></figure>\n<p>lm.intercept_: 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>X</mi><mo>+</mo><msub><mi>w</mi><mrow><mn>0</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y=WX + w_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.02691em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 中的<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mrow><mn>0</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.02691em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></p>\n<p>lm.coef_： 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>X</mi><mo>+</mo><msub><mi>w</mi><mrow><mn>0</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y=WX + w_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.02691em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 中的<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span></p>\n<p>预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)：y_test - predictions 应该符合正态分布</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sns.distplot((y_test - predictions), bins=50)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"数学知识补充\"> 数学知识补充</span></h4>\n<p>最小二乘法估计</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>m</mi><mrow><mn>0</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mn>1</mn></mrow></msub><mi>x</mi><mo>+</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">y = m_{0} + m_{1}x + e\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.58333em;\"></span><span class=\"strut bottom\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\">m</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">m</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span><span class=\"mbin\">+</span><span class=\"mord mathit\">e</span></span></span></span></span></p>\n<p>其中误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 引入用以解释不确定性的因素。</p>\n<p>基本假设：</p>\n<ol>\n<li>零均值假设：误差项是期望为零的随机变量，即 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo>(</mo><mi>e</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">E(e)=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathit\">e</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">0</span></span></span></span></li>\n<li>不变方差假设：误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 的方差（用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8141079999999999em;\"></span><span class=\"strut bottom\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 表示）是常数且与 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mn>2</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>,…. 的值无关</li>\n<li>独立性假设：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 的变量是相互独立的</li>\n<li>正态性假设：误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 是正态随机变量,也即：误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 的值是独立的正态分布随机变量，带有均值0和不变方差 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8141079999999999em;\"></span><span class=\"strut bottom\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></li>\n</ol>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<p>回归是为了预测数值型的目标值。<br>\n线性回归假设特征和结果之间满足线性关系。<br>\n求解回归方程的回归系数的过程就是回归。<br>\n回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。</p>\n<h4 id=\"数学理论\"><a class=\"markdownIt-Anchor\" href=\"#数学理论\"></a> 数学理论</h4>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">y=WX\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span> 是回归系数向量.<br>\n给定输入是<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>X</mi><mrow><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">X_{1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.07847em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>, 那么预测结果就是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>X</mi><mrow><mn>1</mn></mrow><mrow><mi>T</mi></mrow></msubsup><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">y=X_{1}^{T}W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8413309999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.0894389999999998em;vertical-align:-0.24810799999999997em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:0.24810799999999997em;margin-left:-0.07847em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span>.</p>\n<p>现在有一些<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span>和<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>, 我们的目的就是找到<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span>。 常用的方法就是找出使误差最小的<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span>。即预测值<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>与真实值<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>之间的差值。采用最小二乘法：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi></mrow></msubsup><mo>(</mo><msub><mi>y</mi><mrow><mi>i</mi></mrow></msub><mo>−</mo><msubsup><mi>x</mi><mrow><mi>i</mi></mrow><mi>T</mi></msubsup><mi>w</mi><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sum_{i=1}^{n}(y_{i} - x_{i}^Tw)^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathit\">n</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.247em;margin-left:0em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span></span><span style=\"top:-0.4129999999999999em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<p>对<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span></span></span></span>求导，得到<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>X</mi><mi>T</mi></msup><mo>(</mo><mi>y</mi><mo>−</mo><mi>X</mi><mi>w</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">X^T(y-Xw)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8413309999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.0913309999999998em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"mclose\">)</span></span></span></span>, 令其等于零，得到</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mrow><mi>w</mi></mrow><mo>^</mo></mover><mo>=</mo><mo>(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><msup><mo>)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">\\hat{w} = (X^TX)^{-1}X^Ty\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8913309999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.1413309999999999em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span></span></span><span style=\"top:0em;margin-left:0.16668em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.41300000000000003em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord\">−</span><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span></span></p>\n<h4 id=\"评估指标\"><a class=\"markdownIt-Anchor\" href=\"#评估指标\"></a> 评估指标</h4>\n<ul>\n<li><strong>Mean Absolute Error</strong> (MAE)：误差绝对值的平均值</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi mathvariant=\"normal\">∣</mi><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent=\"true\"><mrow><mi>y</mi></mrow><mo>^</mo></mover><mi>i</mi></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\">n</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:0em;margin-left:0.11112em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathrm\">∣</span></span></span></span></span></p>\n<ul>\n<li><strong>Mean Squared Error</strong> (MSE): 误差平方的均值</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent=\"true\"><mrow><mi>y</mi></mrow><mo>^</mo></mover><mi>i</mi></msub><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\">n</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:0em;margin-left:0.11112em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<ul>\n<li><strong>Root Mean Squared Error</strong> (RMSE): 均方根误差</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent=\"true\"><mrow><mi>y</mi></mrow><mo>^</mo></mover><mi>i</mi></msub><msup><mo>)</mo><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:2.128249em;\"></span><span class=\"strut bottom\" style=\"height:3.6550199999999995em;vertical-align:-1.5267709999999999em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"sqrt mord\"><span class=\"sqrt-sign\" style=\"top:-0.023254000000000108em;\"><span class=\"style-wrap reset-textstyle textstyle uncramped\"><span class=\"delimsizing mult\"><span class=\"vlist\"><span style=\"top:0.665005em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span>⎷</span></span></span><span style=\"top:-0.24999499999999997em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span></span></span></span><span style=\"top:-0.854995em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span></span></span></span><span style=\"top:-1.459995em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"delimsizinginner delim-size4\"><span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord reset-textstyle displaystyle textstyle cramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord mathrm\">1</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">n</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span><span style=\"top:0em;margin-left:0.11112em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">i</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span><span style=\"top:-2.0482489999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped sqrt-line\"></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:1em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<h4 id=\"scikitlearn-中的线性回归用法\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中的线性回归用法\"></a> ScikitLearn 中的线性回归用法</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.linear_model import LinearRegression</span><br><span class=\"line\">from sklearn import metrics</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)</span><br><span class=\"line\"></span><br><span class=\"line\">lm = LinearRegression()</span><br><span class=\"line\"></span><br><span class=\"line\">lm.fit(X_train, y_train)</span><br><span class=\"line\">print(lm.intercept_)</span><br><span class=\"line\">print(lm.coef_)</span><br><span class=\"line\"></span><br><span class=\"line\">predictions = lm.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(&apos;MAE: &apos;, metrics.mean_absolute_error(y_test, predictions)</span><br><span class=\"line\">print(&apos;MSE: &apos;, metrics.mean_squared_error(y_test, predictions))</span><br><span class=\"line\">print(&apos;RMSE: &apos;, np.sqrt(metrics.mean_squared_error(y_test, predictions))</span><br></pre></td></tr></table></figure>\n<p>lm.intercept_: 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>X</mi><mo>+</mo><msub><mi>w</mi><mrow><mn>0</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y=WX + w_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.02691em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 中的<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mrow><mn>0</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">w_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.02691em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></p>\n<p>lm.coef_： 是 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mi>W</mi><mi>X</mi><mo>+</mo><msub><mi>w</mi><mrow><mn>0</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">y=WX + w_{0}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.8777699999999999em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.02691em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 中的<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">W</span></span></span></span></p>\n<p>预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)：y_test - predictions 应该符合正态分布</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sns.distplot((y_test - predictions), bins=50)</span><br></pre></td></tr></table></figure>\n<h4 id=\"数学知识补充\"><a class=\"markdownIt-Anchor\" href=\"#数学知识补充\"></a> 数学知识补充</h4>\n<p>最小二乘法估计</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>m</mi><mrow><mn>0</mn></mrow></msub><mo>+</mo><msub><mi>m</mi><mrow><mn>1</mn></mrow></msub><mi>x</mi><mo>+</mo><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">y = m_{0} + m_{1}x + e\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.58333em;\"></span><span class=\"strut bottom\" style=\"height:0.7777700000000001em;vertical-align:-0.19444em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\">m</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">m</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span><span class=\"mbin\">+</span><span class=\"mord mathit\">e</span></span></span></span></span></p>\n<p>其中误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 引入用以解释不确定性的因素。</p>\n<p>基本假设：</p>\n<ol>\n<li>零均值假设：误差项是期望为零的随机变量，即 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo>(</mo><mi>e</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">E(e)=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathit\">e</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">0</span></span></span></span></li>\n<li>不变方差假设：误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 的方差（用 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8141079999999999em;\"></span><span class=\"strut bottom\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span> 表示）是常数且与 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>x</mi><mrow><mn>2</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">x_{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>,…. 的值无关</li>\n<li>独立性假设：<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 的变量是相互独立的</li>\n<li>正态性假设：误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 是正态随机变量,也即：误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>e</mi></mrow><annotation encoding=\"application/x-tex\">e</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">e</span></span></span></span> 的值是独立的正态分布随机变量，带有均值0和不变方差 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">\\sigma^2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8141079999999999em;\"></span><span class=\"strut bottom\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></li>\n</ol>\n"},{"title":"Bias-Variance 平衡","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-07T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"\n> 不同的误差源如何导致偏差和方差有助于我们改进数据拟合过程，从而产生更准确的模型。三种方式定义偏差和方差：概念上，图形上和数学上。\n\n#### 概念上\n* Error due to Bias: 表示我们的模型预测的期望值（或者叫平均值）与模型想要努力接近真实值的difference。期望值是指，你可以通过多个数据集（随机性）来训练多个模型（参数会不同），这些模型的预测值与真实值的偏差叫Bias。不可以简单认为一个模型的多个测量算得的。\n\n* Error due to Variance: 表示模型对于给定数据点预测的可变性。当然可变性的统计也是基于一系列模型产生的。Variance是对于给定点的预测在模型的不同实现之间的变化性。\n\n#### 图形上\n![Bias 与 Variance 图形表示](/img/article/2019-08-07-bias-variance.png)\n\n#### 数学上\n假设预测值 $Y$ 与协变量 $X$ 之间满足 $Y=f(X) + \\epsilon$, 误差项 $\\epsilon$ 以0为均值的正态分布，$\\epsilon \\backsim N(0, \\sigma_{\\epsilon})$.\n我们用线性模型或者其他模型估测$f(X)$的模型$\\hat{f}(X)$, $x$点处的预期平方预测误差为:\n$$\nErr(x) = E\\lbrack(Y - \\hat{f}(x))^2\\rbrack\n$$\n然后可以将该错误分解为偏差和方差分量:\n$$\nErr(x) = (E[\\hat{f}(x)] - f(x))^2 + E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + \\sigma_{e}^2\n$$\n$$\nErr(x)=Bias^2 + Variance + Irreducible Error\n$$\n$Irreducible Error$ 是真实关系中的噪声项，任何模型都不能从根本上减少。\n鉴于真实模型和无限数据来校准它，我们应该能够将偏差和方差项减少到0.然而，在一个模型不完善和数据有限的世界中，在最小化偏差和最小化方差之间存在权衡。\n\n\n#### 参考\n[Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)","source":"_posts/2019-08-07-Bias-Variance.md","raw":"---\ntitle: \"Bias-Variance 平衡\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-07 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 模型选择\ncatagories:\n- ML 算法\n\n---\n\n> 不同的误差源如何导致偏差和方差有助于我们改进数据拟合过程，从而产生更准确的模型。三种方式定义偏差和方差：概念上，图形上和数学上。\n\n#### 概念上\n* Error due to Bias: 表示我们的模型预测的期望值（或者叫平均值）与模型想要努力接近真实值的difference。期望值是指，你可以通过多个数据集（随机性）来训练多个模型（参数会不同），这些模型的预测值与真实值的偏差叫Bias。不可以简单认为一个模型的多个测量算得的。\n\n* Error due to Variance: 表示模型对于给定数据点预测的可变性。当然可变性的统计也是基于一系列模型产生的。Variance是对于给定点的预测在模型的不同实现之间的变化性。\n\n#### 图形上\n![Bias 与 Variance 图形表示](/img/article/2019-08-07-bias-variance.png)\n\n#### 数学上\n假设预测值 $Y$ 与协变量 $X$ 之间满足 $Y=f(X) + \\epsilon$, 误差项 $\\epsilon$ 以0为均值的正态分布，$\\epsilon \\backsim N(0, \\sigma_{\\epsilon})$.\n我们用线性模型或者其他模型估测$f(X)$的模型$\\hat{f}(X)$, $x$点处的预期平方预测误差为:\n$$\nErr(x) = E\\lbrack(Y - \\hat{f}(x))^2\\rbrack\n$$\n然后可以将该错误分解为偏差和方差分量:\n$$\nErr(x) = (E[\\hat{f}(x)] - f(x))^2 + E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + \\sigma_{e}^2\n$$\n$$\nErr(x)=Bias^2 + Variance + Irreducible Error\n$$\n$Irreducible Error$ 是真实关系中的噪声项，任何模型都不能从根本上减少。\n鉴于真实模型和无限数据来校准它，我们应该能够将偏差和方差项减少到0.然而，在一个模型不完善和数据有限的世界中，在最小化偏差和最小化方差之间存在权衡。\n\n\n#### 参考\n[Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)","slug":"2019-08-07-Bias-Variance","published":1,"updated":"2019-08-07T09:42:30.567Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqfh000fz0ov19lgbl59","content":"<blockquote>\n<p>不同的误差源如何导致偏差和方差有助于我们改进数据拟合过程，从而产生更准确的模型。三种方式定义偏差和方差：概念上，图形上和数学上。</p>\n</blockquote>\n<h4><span id=\"概念上\"> 概念上</span></h4>\n<ul>\n<li>\n<p>Error due to Bias: 表示我们的模型预测的期望值（或者叫平均值）与模型想要努力接近真实值的difference。期望值是指，你可以通过多个数据集（随机性）来训练多个模型（参数会不同），这些模型的预测值与真实值的偏差叫Bias。不可以简单认为一个模型的多个测量算得的。</p>\n</li>\n<li>\n<p>Error due to Variance: 表示模型对于给定数据点预测的可变性。当然可变性的统计也是基于一系列模型产生的。Variance是对于给定点的预测在模型的不同实现之间的变化性。</p>\n</li>\n</ul>\n<h4><span id=\"图形上\"> 图形上</span></h4>\n<p><img src=\"/img/article/2019-08-07-bias-variance.png\" alt=\"Bias 与 Variance 图形表示\"></p>\n<h4><span id=\"数学上\"> 数学上</span></h4>\n<p>假设预测值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> 与协变量 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 之间满足 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">Y=f(X) + \\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mord mathit\">ϵ</span></span></span></span>, 误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">ϵ</span></span></span></span> 以0为均值的正态分布，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi><mo>∽</mo><mi>N</mi><mo>(</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>σ</mi><mrow><mi>ϵ</mi></mrow></msub><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon \\backsim N(0, \\sigma_{\\epsilon})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">ϵ</span><span class=\"mrel amsrm\">∽</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">0</span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">ϵ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span>.<br>\n我们用线性模型或者其他模型估测<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">f(X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span>的模型<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{f}(X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9578799999999998em;\"></span><span class=\"strut bottom\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span>点处的预期平方预测误差为:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>E</mi><mo>[</mo><mo>(</mo><mi>Y</mi><mo>−</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">Err(x) = E\\lbrack(Y - \\hat{f}(x))^2\\rbrack\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9578799999999998em;\"></span><span class=\"strut bottom\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mbin\">−</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p>然后可以将该错误分解为偏差和方差分量:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>E</mi><mo>[</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>]</mo><mo>−</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup><mo>+</mo><mi>E</mi><mo>[</mo><mo>(</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>E</mi><mo>[</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>]</mo><msup><mo>)</mo><mn>2</mn></msup><mo>]</mo><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>e</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">Err(x) = (E[\\hat{f}(x)] - f(x))^2 + E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + \\sigma_{e}^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9578799999999998em;\"></span><span class=\"strut bottom\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">]</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:0.247em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">e</span></span></span></span><span style=\"top:-0.4129999999999999em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>B</mi><mi>i</mi><mi>a</mi><msup><mi>s</mi><mn>2</mn></msup><mo>+</mo><mi>V</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo>+</mo><mi>I</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>i</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>E</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">Err(x)=Bias^2 + Variance + Irreducible Error\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8641079999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">a</span><span class=\"mord\"><span class=\"mord mathit\">s</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">e</span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>I</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>i</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>E</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">Irreducible Error</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span></span></span></span> 是真实关系中的噪声项，任何模型都不能从根本上减少。<br>\n鉴于真实模型和无限数据来校准它，我们应该能够将偏差和方差项减少到0.然而，在一个模型不完善和数据有限的世界中，在最小化偏差和最小化方差之间存在权衡。</p>\n<h4><span id=\"参考\"> 参考</span></h4>\n<p><a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\" target=\"_blank\" rel=\"noopener\">Understanding the Bias-Variance Tradeoff</a></p>\n","site":{"data":{}},"excerpt":"","more":"<blockquote>\n<p>不同的误差源如何导致偏差和方差有助于我们改进数据拟合过程，从而产生更准确的模型。三种方式定义偏差和方差：概念上，图形上和数学上。</p>\n</blockquote>\n<h4 id=\"概念上\"><a class=\"markdownIt-Anchor\" href=\"#概念上\"></a> 概念上</h4>\n<ul>\n<li>\n<p>Error due to Bias: 表示我们的模型预测的期望值（或者叫平均值）与模型想要努力接近真实值的difference。期望值是指，你可以通过多个数据集（随机性）来训练多个模型（参数会不同），这些模型的预测值与真实值的偏差叫Bias。不可以简单认为一个模型的多个测量算得的。</p>\n</li>\n<li>\n<p>Error due to Variance: 表示模型对于给定数据点预测的可变性。当然可变性的统计也是基于一系列模型产生的。Variance是对于给定点的预测在模型的不同实现之间的变化性。</p>\n</li>\n</ul>\n<h4 id=\"图形上\"><a class=\"markdownIt-Anchor\" href=\"#图形上\"></a> 图形上</h4>\n<p><img src=\"/img/article/2019-08-07-bias-variance.png\" alt=\"Bias 与 Variance 图形表示\"></p>\n<h4 id=\"数学上\"><a class=\"markdownIt-Anchor\" href=\"#数学上\"></a> 数学上</h4>\n<p>假设预测值 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span></span></span></span> 与协变量 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span> 之间满足 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>+</mo><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">Y=f(X) + \\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mord mathit\">ϵ</span></span></span></span>, 误差项 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">ϵ</span></span></span></span> 以0为均值的正态分布，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi><mo>∽</mo><mi>N</mi><mo>(</mo><mn>0</mn><mo separator=\"true\">,</mo><msub><mi>σ</mi><mrow><mi>ϵ</mi></mrow></msub><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\epsilon \\backsim N(0, \\sigma_{\\epsilon})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">ϵ</span><span class=\"mrel amsrm\">∽</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">0</span><span class=\"mpunct\">,</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">ϵ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span>.<br>\n我们用线性模型或者其他模型估测<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">f(X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span>的模型<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat{f}(X)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9578799999999998em;\"></span><span class=\"strut bottom\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span>点处的预期平方预测误差为:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>E</mi><mo>[</mo><mo>(</mo><mi>Y</mi><mo>−</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">Err(x) = E\\lbrack(Y - \\hat{f}(x))^2\\rbrack\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9578799999999998em;\"></span><span class=\"strut bottom\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mbin\">−</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p>然后可以将该错误分解为偏差和方差分量:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>E</mi><mo>[</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>]</mo><mo>−</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup><mo>+</mo><mi>E</mi><mo>[</mo><mo>(</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>−</mo><mi>E</mi><mo>[</mo><mover accent=\"true\"><mrow><mi>f</mi></mrow><mo>^</mo></mover><mo>(</mo><mi>x</mi><mo>)</mo><mo>]</mo><msup><mo>)</mo><mn>2</mn></msup><mo>]</mo><mo>+</mo><msubsup><mi>σ</mi><mrow><mi>e</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding=\"application/x-tex\">Err(x) = (E[\\hat{f}(x)] - f(x))^2 + E[(\\hat{f}(x) - E[\\hat{f}(x)])^2] + \\sigma_{e}^2\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.9578799999999998em;\"></span><span class=\"strut bottom\" style=\"height:1.2078799999999998em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mopen\">(</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">[</span><span class=\"mord accent\"><span class=\"vlist\"><span style=\"top:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"mord displaystyle textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span></span></span><span style=\"top:-0.26343999999999995em;margin-left:0.33334em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"accent-body\"><span>^</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">]</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">σ</span><span class=\"vlist\"><span style=\"top:0.247em;margin-left:-0.03588em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">e</span></span></span></span><span style=\"top:-0.4129999999999999em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>r</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>B</mi><mi>i</mi><mi>a</mi><msup><mi>s</mi><mn>2</mn></msup><mo>+</mo><mi>V</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mo>+</mo><mi>I</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>i</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>E</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">Err(x)=Bias^2 + Variance + Irreducible Error\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8641079999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.1141079999999999em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">a</span><span class=\"mord\"><span class=\"mord mathit\">s</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">e</span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>I</mi><mi>r</mi><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>i</mi><mi>b</mi><mi>l</mi><mi>e</mi><mi>E</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">Irreducible Error</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span></span></span></span> 是真实关系中的噪声项，任何模型都不能从根本上减少。<br>\n鉴于真实模型和无限数据来校准它，我们应该能够将偏差和方差项减少到0.然而，在一个模型不完善和数据有限的世界中，在最小化偏差和最小化方差之间存在权衡。</p>\n<h4 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h4>\n<p><a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\" target=\"_blank\" rel=\"noopener\">Understanding the Bias-Variance Tradeoff</a></p>\n"},{"title":"逻辑回归 Logistic Regression","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-08T06:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"\n#### 概念\n根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。用于估计某种事物的可能性。用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。\n\n#### 逻辑回归与线性回归的关系\n* 相同点：逻辑回归与线性回归都是一种广义线性模型。去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。\n* 不同点：逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。\n\n逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。\n\n#### 数学理论\nSigmoid函数，也称为逻辑函数（Logistic function）：\n$$\ng(z) = \\frac{1}{1 + e^{-z}}\n$$\n![Sigmoid ](/img/article/2019-08-08-sigmoid-function.png)\nsigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。\n\n逻辑回归的假设函数为：\n$$\nh_{\\theta}(x) = g(\\theta^Tx),\ng(z) = \\frac{1}{1 + e^{-z}}\n$$\n所以，\n$$\nh_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n$$\n其中 $x$ 是我们的输入，$\\theta$ 为我们要求取的参数.\n逻辑回归模型所做的假设是:\n$$\nP(y=1|x;\\theta) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}\n$$\n即在给定 $x$ 和 $\\theta$ 的条件下 $y=1$ 的概率，\n$g(h)$ 就是上面提到的sigmoid函数, 与之相对应的决策函数为：\n$$\ny^* = 1, if P(y=1|x) > 0.5\n$$\n实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。\n#### 评估指标\n最常用的是代价函数是交叉熵(Cross Entropy), 交叉熵是对「出乎意料」的度量. 交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。\n$$\nJ(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^m (y_{(i)}logh_{\\theta}(x^{(i)}) + (1 - y^{(i)}log(1 - h_{\\theta}(x^{(i)})))]\n$$\n\n$m$训练样本的个数, $h_{\\theta}(x^{(i)})$ 用参数 $\\theta$ 和 $x$ 预测出来的 $y$值, $y$ 原训练样本中的 $y$ 值，也就是标准答案, 上角标 ${i}$ 第 ${i}$ 个样本\n#### ScikitLearn 中的逻辑回归用法\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\npredictions = logmodel.predict(X_test)\n\nprint(classification_report(y_test, predictions))\nconfusion_matrix(y_test, predictions)\n~~~\n\n#### 混淆矩阵 Confusion Matrix\n\n|n=165|Predicted: NO|Predicted: Yes||\n|---|---|---|---|---|\n|Actual: NO|TN=50|FP=10|60|\n|Actual: Yes|FN=5|TP=100|105|\n||55|110||\nTP: True Positives\nTN: True Negatives\nFP: False Positives (Type I error)\nFN: False Negatives (Type II error)\n\n正确率为：\n$$\\frac{TN + TP}{total} =\\frac{50+100}{165}$$\n错误率为：\n$$\\frac{FP + FN}{total} =\\frac{5+10}{165}$$\n#### 数学知识补充\n* 伯努利分布亦称“零一分布”、“两点分布”。称随机变量X有伯努利分布, 参数为p($0 < p < 1$) 如果它分别以概率p和1-p取1和0为值。$E(X)=p$, $D(X)=p(1-p)$。\n\n\n#### 参考\n[逻辑回归（Logistic Regression)（一）](https://zhuanlan.zhihu.com/p/28408516)","source":"_posts/2019-08-08-逻辑回归.md","raw":"---\ntitle: \"逻辑回归 Logistic Regression\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-08 14:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 分类算法\ncatagories:\n- ML 算法\n\n---\n\n#### 概念\n根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。用于估计某种事物的可能性。用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。\n\n#### 逻辑回归与线性回归的关系\n* 相同点：逻辑回归与线性回归都是一种广义线性模型。去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。\n* 不同点：逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。\n\n逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。\n\n#### 数学理论\nSigmoid函数，也称为逻辑函数（Logistic function）：\n$$\ng(z) = \\frac{1}{1 + e^{-z}}\n$$\n![Sigmoid ](/img/article/2019-08-08-sigmoid-function.png)\nsigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。\n\n逻辑回归的假设函数为：\n$$\nh_{\\theta}(x) = g(\\theta^Tx),\ng(z) = \\frac{1}{1 + e^{-z}}\n$$\n所以，\n$$\nh_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n$$\n其中 $x$ 是我们的输入，$\\theta$ 为我们要求取的参数.\n逻辑回归模型所做的假设是:\n$$\nP(y=1|x;\\theta) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}\n$$\n即在给定 $x$ 和 $\\theta$ 的条件下 $y=1$ 的概率，\n$g(h)$ 就是上面提到的sigmoid函数, 与之相对应的决策函数为：\n$$\ny^* = 1, if P(y=1|x) > 0.5\n$$\n实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。\n#### 评估指标\n最常用的是代价函数是交叉熵(Cross Entropy), 交叉熵是对「出乎意料」的度量. 交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。\n$$\nJ(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^m (y_{(i)}logh_{\\theta}(x^{(i)}) + (1 - y^{(i)}log(1 - h_{\\theta}(x^{(i)})))]\n$$\n\n$m$训练样本的个数, $h_{\\theta}(x^{(i)})$ 用参数 $\\theta$ 和 $x$ 预测出来的 $y$值, $y$ 原训练样本中的 $y$ 值，也就是标准答案, 上角标 ${i}$ 第 ${i}$ 个样本\n#### ScikitLearn 中的逻辑回归用法\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, y_train)\npredictions = logmodel.predict(X_test)\n\nprint(classification_report(y_test, predictions))\nconfusion_matrix(y_test, predictions)\n~~~\n\n#### 混淆矩阵 Confusion Matrix\n\n|n=165|Predicted: NO|Predicted: Yes||\n|---|---|---|---|---|\n|Actual: NO|TN=50|FP=10|60|\n|Actual: Yes|FN=5|TP=100|105|\n||55|110||\nTP: True Positives\nTN: True Negatives\nFP: False Positives (Type I error)\nFN: False Negatives (Type II error)\n\n正确率为：\n$$\\frac{TN + TP}{total} =\\frac{50+100}{165}$$\n错误率为：\n$$\\frac{FP + FN}{total} =\\frac{5+10}{165}$$\n#### 数学知识补充\n* 伯努利分布亦称“零一分布”、“两点分布”。称随机变量X有伯努利分布, 参数为p($0 < p < 1$) 如果它分别以概率p和1-p取1和0为值。$E(X)=p$, $D(X)=p(1-p)$。\n\n\n#### 参考\n[逻辑回归（Logistic Regression)（一）](https://zhuanlan.zhihu.com/p/28408516)","slug":"2019-08-08-逻辑回归","published":1,"updated":"2019-08-08T07:28:40.062Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqfi000hz0ov7bva1k7x","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<p>根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。用于估计某种事物的可能性。用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。</p>\n<h4><span id=\"逻辑回归与线性回归的关系\"> 逻辑回归与线性回归的关系</span></h4>\n<ul>\n<li>相同点：逻辑回归与线性回归都是一种广义线性模型。去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。</li>\n<li>不同点：逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。</li>\n</ul>\n<p>逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。</p>\n<h4><span id=\"数学理论\"> 数学理论</span></h4>\n<p>Sigmoid函数，也称为逻辑函数（Logistic function）：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">g(z) = \\frac{1}{1 + e^{-z}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.09077em;vertical-align:-0.7693300000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p><img src=\"/img/article/2019-08-08-sigmoid-function.png\" alt=\"Sigmoid \"><br>\nsigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。</p>\n<p>逻辑回归的假设函数为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>g</mi><mo>(</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi><mo>)</mo><mo separator=\"true\">,</mo><mi>g</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(x) = g(\\theta^Tx),\ng(z) = \\frac{1}{1 + e^{-z}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.09077em;vertical-align:-0.7693300000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>所以，</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.127735em;vertical-align:-0.806295em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.722965em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.07142857142857144em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span> 是我们的输入，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 为我们要求取的参数.<br>\n逻辑回归模型所做的假设是:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">;</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mi>g</mi><mo>(</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">P(y=1|x;\\theta) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.127735em;vertical-align:-0.806295em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\">x</span><span class=\"mpunct\">;</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.722965em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.07142857142857144em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>即在给定 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的条件下 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">y=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.64444em;\"></span><span class=\"strut bottom\" style=\"height:0.8388800000000001em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span> 的概率，<br>\n<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi><mo>(</mo><mi>h</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">g(h)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathit\">h</span><span class=\"mclose\">)</span></span></span></span> 就是上面提到的sigmoid函数, 与之相对应的决策函数为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>y</mi><mo>∗</mo></msup><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>i</mi><mi>f</mi><mi>P</mi><mo>(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo>)</mo><mo>&gt;</mo><mn>0</mn><mi mathvariant=\"normal\">.</mi><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">y^* = 1, if P(y=1|x) &gt; 0.5\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord\">∗</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span><span class=\"mpunct\">,</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">&gt;</span><span class=\"mord mathrm\">0</span><span class=\"mord mathrm\">.</span><span class=\"mord mathrm\">5</span></span></span></span></span></p>\n<p>实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。</p>\n<h4><span id=\"评估指标\"> 评估指标</span></h4>\n<p>最常用的是代价函数是交叉熵(Cross Entropy), 交叉熵是对「出乎意料」的度量. 交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mo>−</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>m</mi></mrow></mfrac><mo>[</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>(</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mo>)</mo><mo>)</mo><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^m (y_{(i)}logh_{\\theta}(x^{(i)}) + (1 - y^{(i)}log(1 - h_{\\theta}(x^{(i)})))]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.09618em;\">J</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord\">−</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">m</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mopen\">[</span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\">m</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.18019999999999992em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">1</span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">1</span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">m</span></span></span></span>训练样本的个数, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(x^{(i)})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8879999999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.138em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span> 用参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span> 预测出来的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>值, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 原训练样本中的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 值，也就是标准答案, 上角标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mrow><mi>i</mi></mrow></mrow><annotation encoding=\"application/x-tex\">{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.65952em;\"></span><span class=\"strut bottom\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">i</span></span></span></span></span> 第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mrow><mi>i</mi></mrow></mrow><annotation encoding=\"application/x-tex\">{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.65952em;\"></span><span class=\"strut bottom\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">i</span></span></span></span></span> 个样本</p>\n<h4><span id=\"scikitlearn-中的逻辑回归用法\"> ScikitLearn 中的逻辑回归用法</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.linear_model import LogisticRegression</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)</span><br><span class=\"line\">logmodel = LogisticRegression()</span><br><span class=\"line\">logmodel.fit(X_train, y_train)</span><br><span class=\"line\">predictions = logmodel.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, predictions))</span><br><span class=\"line\">confusion_matrix(y_test, predictions)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"混淆矩阵-confusion-matrix\"> 混淆矩阵 Confusion Matrix</span></h4>\n<table>\n<thead>\n<tr>\n<th>n=165</th>\n<th>Predicted: NO</th>\n<th>Predicted: Yes</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Actual: NO</td>\n<td>TN=50</td>\n<td>FP=10</td>\n<td>60</td>\n</tr>\n<tr>\n<td>Actual: Yes</td>\n<td>FN=5</td>\n<td>TP=100</td>\n<td>105</td>\n</tr>\n<tr>\n<td></td>\n<td>55</td>\n<td>110</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>TP: True Positives<br>\nTN: True Negatives<br>\nFP: False Positives (Type I error)<br>\nFN: False Negatives (Type II error)</p>\n<p>正确率为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi></mrow><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>5</mn><mn>0</mn><mo>+</mo><mn>1</mn><mn>0</mn><mn>0</mn></mrow><mrow><mn>1</mn><mn>6</mn><mn>5</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{TN + TP}{total} =\\frac{50+100}{165}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.36033em;\"></span><span class=\"strut bottom\" style=\"height:2.04633em;vertical-align:-0.686em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">t</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">6</span><span class=\"mord mathrm\">5</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">5</span><span class=\"mord mathrm\">0</span><span class=\"mbin\">+</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">0</span><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>错误率为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>5</mn><mo>+</mo><mn>1</mn><mn>0</mn></mrow><mrow><mn>1</mn><mn>6</mn><mn>5</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{FP + FN}{total} =\\frac{5+10}{165}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.36033em;\"></span><span class=\"strut bottom\" style=\"height:2.04633em;vertical-align:-0.686em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">t</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">6</span><span class=\"mord mathrm\">5</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">5</span><span class=\"mbin\">+</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<h4><span id=\"数学知识补充\"> 数学知识补充</span></h4>\n<ul>\n<li>伯努利分布亦称“零一分布”、“两点分布”。称随机变量X有伯努利分布, 参数为p(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>p</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 &lt; p &lt; 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.64444em;\"></span><span class=\"strut bottom\" style=\"height:0.8388800000000001em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathrm\">0</span><span class=\"mrel\">&lt;</span><span class=\"mord mathit\">p</span><span class=\"mrel\">&lt;</span><span class=\"mord mathrm\">1</span></span></span></span>) 如果它分别以概率p和1-p取1和0为值。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">E(X)=p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\">p</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>p</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">D(X)=p(1-p)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\">p</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">1</span><span class=\"mbin\">−</span><span class=\"mord mathit\">p</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n<h4><span id=\"参考\"> 参考</span></h4>\n<p><a href=\"https://zhuanlan.zhihu.com/p/28408516\" target=\"_blank\" rel=\"noopener\">逻辑回归（Logistic Regression)（一）</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<p>根据现有数据对分类边界线(Decision Boundary)建立回归公式，以此进行分类。用于估计某种事物的可能性。用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。</p>\n<h4 id=\"逻辑回归与线性回归的关系\"><a class=\"markdownIt-Anchor\" href=\"#逻辑回归与线性回归的关系\"></a> 逻辑回归与线性回归的关系</h4>\n<ul>\n<li>相同点：逻辑回归与线性回归都是一种广义线性模型。去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。</li>\n<li>不同点：逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。</li>\n</ul>\n<p>逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。</p>\n<h4 id=\"数学理论\"><a class=\"markdownIt-Anchor\" href=\"#数学理论\"></a> 数学理论</h4>\n<p>Sigmoid函数，也称为逻辑函数（Logistic function）：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">g(z) = \\frac{1}{1 + e^{-z}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.09077em;vertical-align:-0.7693300000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p><img src=\"/img/article/2019-08-08-sigmoid-function.png\" alt=\"Sigmoid \"><br>\nsigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0或者1。它的这个特性对于解决二分类问题十分重要。</p>\n<p>逻辑回归的假设函数为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>g</mi><mo>(</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi><mo>)</mo><mo separator=\"true\">,</mo><mi>g</mi><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(x) = g(\\theta^Tx),\ng(z) = \\frac{1}{1 + e^{-z}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.09077em;vertical-align:-0.7693300000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mpunct\">,</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>所以，</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.127735em;vertical-align:-0.806295em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.722965em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.07142857142857144em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>其中 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span> 是我们的输入，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 为我们要求取的参数.<br>\n逻辑回归模型所做的假设是:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">;</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mi>g</mi><mo>(</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><msup><mi>θ</mi><mi>T</mi></msup><mi>x</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">P(y=1|x;\\theta) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.32144em;\"></span><span class=\"strut bottom\" style=\"height:2.127735em;vertical-align:-0.806295em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\">x</span><span class=\"mpunct\">;</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.722965em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mbin\">+</span><span class=\"mord\"><span class=\"mord mathit\">e</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.07142857142857144em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">x</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>即在给定 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 的条件下 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">y=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.64444em;\"></span><span class=\"strut bottom\" style=\"height:0.8388800000000001em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span> 的概率，<br>\n<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>g</mi><mo>(</mo><mi>h</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">g(h)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathit\">h</span><span class=\"mclose\">)</span></span></span></span> 就是上面提到的sigmoid函数, 与之相对应的决策函数为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>y</mi><mo>∗</mo></msup><mo>=</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>i</mi><mi>f</mi><mi>P</mi><mo>(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo>)</mo><mo>&gt;</mo><mn>0</mn><mi mathvariant=\"normal\">.</mi><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">y^* = 1, if P(y=1|x) &gt; 0.5\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord\">∗</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span><span class=\"mpunct\">,</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mrel\">&gt;</span><span class=\"mord mathrm\">0</span><span class=\"mord mathrm\">.</span><span class=\"mord mathrm\">5</span></span></span></span></span></p>\n<p>实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。</p>\n<h4 id=\"评估指标\"><a class=\"markdownIt-Anchor\" href=\"#评估指标\"></a> 评估指标</h4>\n<p>最常用的是代价函数是交叉熵(Cross Entropy), 交叉熵是对「出乎意料」的度量. 交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>J</mi><mo>(</mo><mi>θ</mi><mo>)</mo><mo>=</mo><mo>−</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>m</mi></mrow></mfrac><mo>[</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mo>(</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi>y</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>1</mn><mo>−</mo><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo><mo>)</mo><mo>)</mo><mo>]</mo></mrow><annotation encoding=\"application/x-tex\">J(\\theta) = -\\frac{1}{m}[\\sum_{i=1}^m (y_{(i)}logh_{\\theta}(x^{(i)}) + (1 - y^{(i)}log(1 - h_{\\theta}(x^{(i)})))]\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.6513970000000002em;\"></span><span class=\"strut bottom\" style=\"height:2.929066em;vertical-align:-1.277669em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.09618em;\">J</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord\">−</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">m</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">1</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mopen\">[</span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.1776689999999999em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000143778em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.2500050000000003em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathit\">m</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:0.18019999999999992em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mbin\">+</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">1</span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">1</span><span class=\"mbin\">−</span><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">]</span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">m</span></span></span></span>训练样本的个数, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>θ</mi></mrow></msub><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">h_{\\theta}(x^{(i)})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.8879999999999999em;\"></span><span class=\"strut bottom\" style=\"height:1.138em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mopen\">(</span><span class=\"mord mathit\">i</span><span class=\"mclose\">)</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mclose\">)</span></span></span></span> 用参数 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding=\"application/x-tex\">\\theta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.69444em;\"></span><span class=\"strut bottom\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">θ</span></span></span></span> 和 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\">x</span></span></span></span> 预测出来的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span>值, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 原训练样本中的 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span></span></span></span> 值，也就是标准答案, 上角标 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mrow><mi>i</mi></mrow></mrow><annotation encoding=\"application/x-tex\">{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.65952em;\"></span><span class=\"strut bottom\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">i</span></span></span></span></span> 第 <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mrow><mi>i</mi></mrow></mrow><annotation encoding=\"application/x-tex\">{i}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.65952em;\"></span><span class=\"strut bottom\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">i</span></span></span></span></span> 个样本</p>\n<h4 id=\"scikitlearn-中的逻辑回归用法\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中的逻辑回归用法\"></a> ScikitLearn 中的逻辑回归用法</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.linear_model import LogisticRegression</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)</span><br><span class=\"line\">logmodel = LogisticRegression()</span><br><span class=\"line\">logmodel.fit(X_train, y_train)</span><br><span class=\"line\">predictions = logmodel.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, predictions))</span><br><span class=\"line\">confusion_matrix(y_test, predictions)</span><br></pre></td></tr></table></figure>\n<h4 id=\"混淆矩阵-confusion-matrix\"><a class=\"markdownIt-Anchor\" href=\"#混淆矩阵-confusion-matrix\"></a> 混淆矩阵 Confusion Matrix</h4>\n<table>\n<thead>\n<tr>\n<th>n=165</th>\n<th>Predicted: NO</th>\n<th>Predicted: Yes</th>\n<th></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Actual: NO</td>\n<td>TN=50</td>\n<td>FP=10</td>\n<td>60</td>\n</tr>\n<tr>\n<td>Actual: Yes</td>\n<td>FN=5</td>\n<td>TP=100</td>\n<td>105</td>\n</tr>\n<tr>\n<td></td>\n<td>55</td>\n<td>110</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>TP: True Positives<br>\nTN: True Negatives<br>\nFP: False Positives (Type I error)<br>\nFN: False Negatives (Type II error)</p>\n<p>正确率为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>T</mi><mi>N</mi><mo>+</mo><mi>T</mi><mi>P</mi></mrow><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>5</mn><mn>0</mn><mo>+</mo><mn>1</mn><mn>0</mn><mn>0</mn></mrow><mrow><mn>1</mn><mn>6</mn><mn>5</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{TN + TP}{total} =\\frac{50+100}{165}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.36033em;\"></span><span class=\"strut bottom\" style=\"height:2.04633em;vertical-align:-0.686em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">t</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">6</span><span class=\"mord mathrm\">5</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">5</span><span class=\"mord mathrm\">0</span><span class=\"mbin\">+</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">0</span><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>错误率为：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>F</mi><mi>P</mi><mo>+</mo><mi>F</mi><mi>N</mi></mrow><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>5</mn><mo>+</mo><mn>1</mn><mn>0</mn></mrow><mrow><mn>1</mn><mn>6</mn><mn>5</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{FP + FN}{total} =\\frac{5+10}{165}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.36033em;\"></span><span class=\"strut bottom\" style=\"height:2.04633em;vertical-align:-0.686em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\">t</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mbin\">+</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">6</span><span class=\"mord mathrm\">5</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\">5</span><span class=\"mbin\">+</span><span class=\"mord mathrm\">1</span><span class=\"mord mathrm\">0</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<h4 id=\"数学知识补充\"><a class=\"markdownIt-Anchor\" href=\"#数学知识补充\"></a> 数学知识补充</h4>\n<ul>\n<li>伯努利分布亦称“零一分布”、“两点分布”。称随机变量X有伯努利分布, 参数为p(<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>0</mn><mo>&lt;</mo><mi>p</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 &lt; p &lt; 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.64444em;\"></span><span class=\"strut bottom\" style=\"height:0.8388800000000001em;vertical-align:-0.19444em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathrm\">0</span><span class=\"mrel\">&lt;</span><span class=\"mord mathit\">p</span><span class=\"mrel\">&lt;</span><span class=\"mord mathrm\">1</span></span></span></span>) 如果它分别以概率p和1-p取1和0为值。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">E(X)=p</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\">p</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>D</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>=</mo><mi>p</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">D(X)=p(1-p)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\">p</span><span class=\"mopen\">(</span><span class=\"mord mathrm\">1</span><span class=\"mbin\">−</span><span class=\"mord mathit\">p</span><span class=\"mclose\">)</span></span></span></span>。</li>\n</ul>\n<h4 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h4>\n<p><a href=\"https://zhuanlan.zhihu.com/p/28408516\" target=\"_blank\" rel=\"noopener\">逻辑回归（Logistic Regression)（一）</a></p>\n"},{"title":"决策树与随机森林","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-09T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"\n#### 决策树概念\n决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。\n\n分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。\n* 结点：根据某一属性值拆分数据\n* 边：拆分结果到下一个结点\n* 根结点：执行第一次拆分的结点\n* 叶子结点：终端节点，预测结果\n![决策树](/img/article/2019-08-09-decision-tree.png)\n\n\n#### 随机森林概念\n是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。\n\n#### 随机森林的构造过程\n1. 假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。\n2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。\n3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。\n4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了\n\n#### 随机森林随机性\n随机森林对输入的数据的行,列进行随机采样:\n* 对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。\n* 列采样，从M个feature中，选择m个（m << M）\n两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting\n\n#### 随机森林的优点\n1. 在数据集上表现良好，随机森林不容易陷入过拟合\n2. 随机森林具有很好的抗噪声能力\n3. 够处理很高维度（feature很多）的数据，并且不用做特征选择\n4. 既能处理离散型数据，也能处理连续型数据，数据集无需规范化\n5. 可生成一个$Proximities=p_{ij}$矩阵，用于度量样本之间的相似性： $p_{ij}=\\frac{a_{ij}}{N}$, $a_{ij}$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数\n6. 创建随机森林的使用的是无偏估计\n7. 训练速度快\n8. 训练过程中能够检测到feature间的互相影响\n9. 容易做成并行化方法\n10. 实现简单\n\n#### 信息熵 & 信息增益 & 信息增益率\n* 信息熵(Information Entropy)\n$$\nEnt(D) = -\\sum_{k=1}^{|y|}p_{k}log_{2}p_{k}\n$$\n$|y|$ 表示类别的个数，信息熵越小的话，表明这个数据集越纯粹，即类别越少。$Ent(D)=0$,则数据集中只有一个类别\n\n* 条件熵\n条件熵表示在条件X下Y的信息熵\n$$\nEnt(Y|X)=\\sum_{x \\in X}p(x)H(Y|X=x)\n$$\n\n* 信息增益(Information Gain)\n信息增益 = 信息熵 - 条件熵\n* 信息增益率(Information Gain Ratio)\n\n#### ScikitLearn 中决策树\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX = df.drop('Kyphosis', axis = 1)\ny = df['Kyphosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\npredictions = dtree.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n```\n#### ScikitLearn 中随机森林\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)\nprint(classification_report(y_test, rfc_pred))\nprint(confusion_matrix(y_test, rfc_pred))\n```\n#### 参考链接\n[说说随机森林](https://zhuanlan.zhihu.com/p/22097796)\n\n","source":"_posts/2019-08-09-决策树-随机森林.md","raw":"---\ntitle: \"决策树与随机森林\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-09 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 分类算法\ncatagories:\n- ML 算法\n\n---\n\n#### 决策树概念\n决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。\n\n分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。\n* 结点：根据某一属性值拆分数据\n* 边：拆分结果到下一个结点\n* 根结点：执行第一次拆分的结点\n* 叶子结点：终端节点，预测结果\n![决策树](/img/article/2019-08-09-decision-tree.png)\n\n\n#### 随机森林概念\n是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。\n\n#### 随机森林的构造过程\n1. 假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。\n2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。\n3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。\n4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了\n\n#### 随机森林随机性\n随机森林对输入的数据的行,列进行随机采样:\n* 对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。\n* 列采样，从M个feature中，选择m个（m << M）\n两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting\n\n#### 随机森林的优点\n1. 在数据集上表现良好，随机森林不容易陷入过拟合\n2. 随机森林具有很好的抗噪声能力\n3. 够处理很高维度（feature很多）的数据，并且不用做特征选择\n4. 既能处理离散型数据，也能处理连续型数据，数据集无需规范化\n5. 可生成一个$Proximities=p_{ij}$矩阵，用于度量样本之间的相似性： $p_{ij}=\\frac{a_{ij}}{N}$, $a_{ij}$表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数\n6. 创建随机森林的使用的是无偏估计\n7. 训练速度快\n8. 训练过程中能够检测到feature间的互相影响\n9. 容易做成并行化方法\n10. 实现简单\n\n#### 信息熵 & 信息增益 & 信息增益率\n* 信息熵(Information Entropy)\n$$\nEnt(D) = -\\sum_{k=1}^{|y|}p_{k}log_{2}p_{k}\n$$\n$|y|$ 表示类别的个数，信息熵越小的话，表明这个数据集越纯粹，即类别越少。$Ent(D)=0$,则数据集中只有一个类别\n\n* 条件熵\n条件熵表示在条件X下Y的信息熵\n$$\nEnt(Y|X)=\\sum_{x \\in X}p(x)H(Y|X=x)\n$$\n\n* 信息增益(Information Gain)\n信息增益 = 信息熵 - 条件熵\n* 信息增益率(Information Gain Ratio)\n\n#### ScikitLearn 中决策树\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX = df.drop('Kyphosis', axis = 1)\ny = df['Kyphosis']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\npredictions = dtree.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))\n```\n#### ScikitLearn 中随机森林\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)\nprint(classification_report(y_test, rfc_pred))\nprint(confusion_matrix(y_test, rfc_pred))\n```\n#### 参考链接\n[说说随机森林](https://zhuanlan.zhihu.com/p/22097796)\n\n","slug":"2019-08-09-决策树-随机森林","published":1,"updated":"2019-08-09T06:09:48.051Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqfk000jz0ov0g5q3rrw","content":"<h4><span id=\"决策树概念\"> 决策树概念</span></h4>\n<p>决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。</p>\n<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>\n<ul>\n<li>结点：根据某一属性值拆分数据</li>\n<li>边：拆分结果到下一个结点</li>\n<li>根结点：执行第一次拆分的结点</li>\n<li>叶子结点：终端节点，预测结果<br>\n<img src=\"/img/article/2019-08-09-decision-tree.png\" alt=\"决策树\"></li>\n</ul>\n<h4><span id=\"随机森林概念\"> 随机森林概念</span></h4>\n<p>是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。</p>\n<h4><span id=\"随机森林的构造过程\"> 随机森林的构造过程</span></h4>\n<ol>\n<li>假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。</li>\n<li>当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &lt;&lt; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。</li>\n<li>决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>\n<li>按照步骤1~3建立大量的决策树，这样就构成了随机森林了</li>\n</ol>\n<h4><span id=\"随机森林随机性\"> 随机森林随机性</span></h4>\n<p>随机森林对输入的数据的行,列进行随机采样:</p>\n<ul>\n<li>对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。</li>\n<li>列采样，从M个feature中，选择m个（m &lt;&lt; M）<br>\n两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting</li>\n</ul>\n<h4><span id=\"随机森林的优点\"> 随机森林的优点</span></h4>\n<ol>\n<li>在数据集上表现良好，随机森林不容易陷入过拟合</li>\n<li>随机森林具有很好的抗噪声能力</li>\n<li>够处理很高维度（feature很多）的数据，并且不用做特征选择</li>\n<li>既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>\n<li>可生成一个<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>s</mi><mo>=</mo><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Proximities=p_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">x</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">s</span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>矩阵，用于度量样本之间的相似性： <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">p_{ij}=\\frac{a_{ij}}{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.813612em;\"></span><span class=\"strut bottom\" style=\"height:1.158612em;vertical-align:-0.345em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.345em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.51222em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">a</span><span class=\"vlist\"><span style=\"top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord scriptscriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">a_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">a</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数</li>\n<li>创建随机森林的使用的是无偏估计</li>\n<li>训练速度快</li>\n<li>训练过程中能够检测到feature间的互相影响</li>\n<li>容易做成并行化方法</li>\n<li>实现简单</li>\n</ol>\n<h4><span id=\"信息熵-amp-信息增益-amp-信息增益率\"> 信息熵 &amp; 信息增益 &amp; 信息增益率</span></h4>\n<ul>\n<li>信息熵(Information Entropy)</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>D</mi><mo>)</mo><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mi mathvariant=\"normal\">∣</mi></mrow></msubsup><msub><mi>p</mi><mrow><mi>k</mi></mrow></msub><mi>l</mi><mi>o</mi><msub><mi>g</mi><mrow><mn>2</mn></mrow></msub><msub><mi>p</mi><mrow><mi>k</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Ent(D) = -\\sum_{k=1}^{|y|}p_{k}log_{2}p_{k}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.961005em;\"></span><span class=\"strut bottom\" style=\"height:3.2631180000000004em;vertical-align:-1.302113em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord\">−</span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.202113em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000032756em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.336005em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathrm\">∣</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|y|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathrm\">∣</span></span></span></span> 表示类别的个数，信息熵越小的话，表明这个数据集越纯粹，即类别越少。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>D</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">Ent(D)=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">0</span></span></span></span>,则数据集中只有一个类别</p>\n<ul>\n<li>条件熵<br>\n条件熵表示在条件X下Y的信息熵</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></msub><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi>H</mi><mo>(</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">Ent(Y|X)=\\sum_{x \\in X}p(x)H(Y|X=x)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.050005em;\"></span><span class=\"strut bottom\" style=\"height:2.3717110000000003em;vertical-align:-1.321706em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.194336em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">x</span><span class=\"mrel\">∈</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span style=\"top:-0.000005000000000032756em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">p</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mord mathit\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mrel\">=</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<ul>\n<li>信息增益(Information Gain)<br>\n信息增益 = 信息熵 - 条件熵</li>\n<li>信息增益率(Information Gain Ratio)</li>\n</ul>\n<h4><span id=\"scikitlearn-中决策树\"> ScikitLearn 中决策树</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.tree import DecisionTreeClassifier</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">X = df.drop(&apos;Kyphosis&apos;, axis = 1)</span><br><span class=\"line\">y = df[&apos;Kyphosis&apos;]</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span><br><span class=\"line\">dtree = DecisionTreeClassifier()</span><br><span class=\"line\">dtree.fit(X_train, y_train)</span><br><span class=\"line\">predictions = dtree.predict(X_test)</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, predictions))</span><br><span class=\"line\">print(confusion_matrix(y_test, predictions))</span><br></pre></td></tr></table></figure>\n<h4><span id=\"scikitlearn-中随机森林\"> ScikitLearn 中随机森林</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.ensemble import RandomForestClassifier</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">rfc = RandomForestClassifier(n_estimators=200)</span><br><span class=\"line\">rfc.fit(X_train, y_train)</span><br><span class=\"line\">rfc_pred = rfc.predict(X_test)</span><br><span class=\"line\">print(classification_report(y_test, rfc_pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, rfc_pred))</span><br></pre></td></tr></table></figure>\n<h4><span id=\"参考链接\"> 参考链接</span></h4>\n<p><a href=\"https://zhuanlan.zhihu.com/p/22097796\" target=\"_blank\" rel=\"noopener\">说说随机森林</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"决策树概念\"><a class=\"markdownIt-Anchor\" href=\"#决策树概念\"></a> 决策树概念</h4>\n<p>决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二。</p>\n<p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性(features)，叶结点表示一个类(labels)。</p>\n<ul>\n<li>结点：根据某一属性值拆分数据</li>\n<li>边：拆分结果到下一个结点</li>\n<li>根结点：执行第一次拆分的结点</li>\n<li>叶子结点：终端节点，预测结果<br>\n<img src=\"/img/article/2019-08-09-decision-tree.png\" alt=\"决策树\"></li>\n</ul>\n<h4 id=\"随机森林概念\"><a class=\"markdownIt-Anchor\" href=\"#随机森林概念\"></a> 随机森林概念</h4>\n<p>是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。</p>\n<h4 id=\"随机森林的构造过程\"><a class=\"markdownIt-Anchor\" href=\"#随机森林的构造过程\"></a> 随机森林的构造过程</h4>\n<ol>\n<li>假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。</li>\n<li>当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &lt;&lt; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。</li>\n<li>决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>\n<li>按照步骤1~3建立大量的决策树，这样就构成了随机森林了</li>\n</ol>\n<h4 id=\"随机森林随机性\"><a class=\"markdownIt-Anchor\" href=\"#随机森林随机性\"></a> 随机森林随机性</h4>\n<p>随机森林对输入的数据的行,列进行随机采样:</p>\n<ul>\n<li>对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。</li>\n<li>列采样，从M个feature中，选择m个（m &lt;&lt; M）<br>\n两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting</li>\n</ul>\n<h4 id=\"随机森林的优点\"><a class=\"markdownIt-Anchor\" href=\"#随机森林的优点\"></a> 随机森林的优点</h4>\n<ol>\n<li>在数据集上表现良好，随机森林不容易陷入过拟合</li>\n<li>随机森林具有很好的抗噪声能力</li>\n<li>够处理很高维度（feature很多）的数据，并且不用做特征选择</li>\n<li>既能处理离散型数据，也能处理连续型数据，数据集无需规范化</li>\n<li>可生成一个<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>P</mi><mi>r</mi><mi>o</mi><mi>x</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>e</mi><mi>s</mi><mo>=</mo><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Proximities=p_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.68333em;\"></span><span class=\"strut bottom\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">P</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">x</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">s</span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>矩阵，用于度量样本之间的相似性： <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>p</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mrow><mi>N</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">p_{ij}=\\frac{a_{ij}}{N}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.813612em;\"></span><span class=\"strut bottom\" style=\"height:1.158612em;vertical-align:-0.345em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.345em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.51222em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">a</span><span class=\"vlist\"><span style=\"top:0.15000000000000002em;margin-right:0.07142857142857144em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-scriptstyle scriptscriptstyle cramped\"><span class=\"mord scriptscriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">a_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.43056em;\"></span><span class=\"strut bottom\" style=\"height:0.716668em;vertical-align:-0.286108em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathit\">a</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">i</span><span class=\"mord mathit\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span>表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数</li>\n<li>创建随机森林的使用的是无偏估计</li>\n<li>训练速度快</li>\n<li>训练过程中能够检测到feature间的互相影响</li>\n<li>容易做成并行化方法</li>\n<li>实现简单</li>\n</ol>\n<h4 id=\"信息熵-信息增益-信息增益率\"><a class=\"markdownIt-Anchor\" href=\"#信息熵-信息增益-信息增益率\"></a> 信息熵 &amp; 信息增益 &amp; 信息增益率</h4>\n<ul>\n<li>信息熵(Information Entropy)</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>D</mi><mo>)</mo><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mi mathvariant=\"normal\">∣</mi></mrow></msubsup><msub><mi>p</mi><mrow><mi>k</mi></mrow></msub><mi>l</mi><mi>o</mi><msub><mi>g</mi><mrow><mn>2</mn></mrow></msub><msub><mi>p</mi><mrow><mi>k</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">Ent(D) = -\\sum_{k=1}^{|y|}p_{k}log_{2}p_{k}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.961005em;\"></span><span class=\"strut bottom\" style=\"height:3.2631180000000004em;vertical-align:-1.302113em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord\">−</span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.202113em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">1</span></span></span></span><span style=\"top:-0.000005000000000032756em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span style=\"top:-1.336005em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord scriptstyle uncramped\"><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathrm\">∣</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord\"><span class=\"mord mathit\">p</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.03148em;\">k</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span></span></p>\n<p><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|y|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mathrm\">∣</span></span></span></span> 表示类别的个数，信息熵越小的话，表明这个数据集越纯粹，即类别越少。<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>D</mi><mo>)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">Ent(D)=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:0.75em;\"></span><span class=\"strut bottom\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"base textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathrm\">0</span></span></span></span>,则数据集中只有一个类别</p>\n<ul>\n<li>条件熵<br>\n条件熵表示在条件X下Y的信息熵</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mi>n</mi><mi>t</mi><mo>(</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></msub><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi>H</mi><mo>(</mo><mi>Y</mi><mi mathvariant=\"normal\">∣</mi><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">Ent(Y|X)=\\sum_{x \\in X}p(x)H(Y|X=x)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.050005em;\"></span><span class=\"strut bottom\" style=\"height:2.3717110000000003em;vertical-align:-1.321706em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.05764em;\">E</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mop op-limits\"><span class=\"vlist\"><span style=\"top:1.194336em;margin-left:0em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord scriptstyle cramped\"><span class=\"mord mathit\">x</span><span class=\"mrel\">∈</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span></span></span></span><span style=\"top:-0.000005000000000032756em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span><span class=\"op-symbol large-op mop\">∑</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">p</span><span class=\"mopen\">(</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span><span class=\"mord mathit\" style=\"margin-right:0.08125em;\">H</span><span class=\"mopen\">(</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">Y</span><span class=\"mord mathrm\">∣</span><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">X</span><span class=\"mrel\">=</span><span class=\"mord mathit\">x</span><span class=\"mclose\">)</span></span></span></span></span></p>\n<ul>\n<li>信息增益(Information Gain)<br>\n信息增益 = 信息熵 - 条件熵</li>\n<li>信息增益率(Information Gain Ratio)</li>\n</ul>\n<h4 id=\"scikitlearn-中决策树\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中决策树\"></a> ScikitLearn 中决策树</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.tree import DecisionTreeClassifier</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">X = df.drop(&apos;Kyphosis&apos;, axis = 1)</span><br><span class=\"line\">y = df[&apos;Kyphosis&apos;]</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span><br><span class=\"line\">dtree = DecisionTreeClassifier()</span><br><span class=\"line\">dtree.fit(X_train, y_train)</span><br><span class=\"line\">predictions = dtree.predict(X_test)</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, predictions))</span><br><span class=\"line\">print(confusion_matrix(y_test, predictions))</span><br></pre></td></tr></table></figure>\n<h4 id=\"scikitlearn-中随机森林\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中随机森林\"></a> ScikitLearn 中随机森林</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.ensemble import RandomForestClassifier</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">rfc = RandomForestClassifier(n_estimators=200)</span><br><span class=\"line\">rfc.fit(X_train, y_train)</span><br><span class=\"line\">rfc_pred = rfc.predict(X_test)</span><br><span class=\"line\">print(classification_report(y_test, rfc_pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, rfc_pred))</span><br></pre></td></tr></table></figure>\n<h4 id=\"参考链接\"><a class=\"markdownIt-Anchor\" href=\"#参考链接\"></a> 参考链接</h4>\n<p><a href=\"https://zhuanlan.zhihu.com/p/22097796\" target=\"_blank\" rel=\"noopener\">说说随机森林</a></p>\n"},{"title":"K-近邻 kNN, k-NearestNeighbor","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-08T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"\n#### 概念\n给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。\n* 优点：\n  - 算法简单\n  - 训练简单\n  - 适用于任意数量的分类\n  - 容易添加更多数据\n  - 精度高\n  - 对异常值不敏感\n  - 无数据输入假定\n  - 参数少：\n    + K\n    + distance metric\n* 缺点：\n  - 高预测成本（大的数据集更糟）\n  - 高维数据不太好\n  - 计算复杂度高\n  - 空间复杂度高\n#### 原理\n```\n1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系;\n2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。\n  - 计算新数据与样本数据集中每条数据的距离。\n  - 对求得的所有距离进行排序（从小到大，越小表示越相似）。\n3. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。\n4. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。\n```\n#### 归一化\n* 定义\n> 归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。\n\n* 方法\n  - 线性函数转换，表达式如下：\n\n    $$y=\\frac{x-MinValue}{MaxValue-MinValue}$$\n    　\n    说明：x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。　\n\n  - 对数函数转换，表达式如下:\n    $$\\frac{\\partial u}{\\partial t} = h^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + \\frac{\\partial^2 u}{\\partial z^2}\\right)$$\n\n#### 评估指标\n\n\n#### ScikitLearn 中的kNN\n* 归一化数值\n```\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(df.drop('TARGET CLASS', axis=1))\nscaled_features = scaler.transform(df.drop('TARGET CLASS', axis=1))\ndf_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n```\n* 使用算法\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX = df_feat\ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\n```\n\n* 比较K值对算法的影响\n```\nerror_rate = []\n\nfor i in range(1, 40):\n  knn = KNeighborsClassifier(n_neighbors=i)\n  knn.fit(X_train, y_train)\n  pred_i = knn.predict(X_test)\n  error_rate.append(np.mean(pred_i != y_test))\n\nplt.Figure(figsize=(10, 6))\nplt.plot(range(1, 40), error_rate, color='blue', ls='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate with K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\n```\n\n#### 参考\n","source":"_posts/2019-08-08-K-近邻算法.md","raw":"---\ntitle: \"K-近邻 kNN, k-NearestNeighbor\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-08 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 分类算法\ncatagories:\n- ML 算法\n\n---\n\n#### 概念\n给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。\n* 优点：\n  - 算法简单\n  - 训练简单\n  - 适用于任意数量的分类\n  - 容易添加更多数据\n  - 精度高\n  - 对异常值不敏感\n  - 无数据输入假定\n  - 参数少：\n    + K\n    + distance metric\n* 缺点：\n  - 高预测成本（大的数据集更糟）\n  - 高维数据不太好\n  - 计算复杂度高\n  - 空间复杂度高\n#### 原理\n```\n1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系;\n2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。\n  - 计算新数据与样本数据集中每条数据的距离。\n  - 对求得的所有距离进行排序（从小到大，越小表示越相似）。\n3. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。\n4. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。\n```\n#### 归一化\n* 定义\n> 归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。\n\n* 方法\n  - 线性函数转换，表达式如下：\n\n    $$y=\\frac{x-MinValue}{MaxValue-MinValue}$$\n    　\n    说明：x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。　\n\n  - 对数函数转换，表达式如下:\n    $$\\frac{\\partial u}{\\partial t} = h^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + \\frac{\\partial^2 u}{\\partial z^2}\\right)$$\n\n#### 评估指标\n\n\n#### ScikitLearn 中的kNN\n* 归一化数值\n```\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(df.drop('TARGET CLASS', axis=1))\nscaled_features = scaler.transform(df.drop('TARGET CLASS', axis=1))\ndf_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])\n```\n* 使用算法\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX = df_feat\ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\n```\n\n* 比较K值对算法的影响\n```\nerror_rate = []\n\nfor i in range(1, 40):\n  knn = KNeighborsClassifier(n_neighbors=i)\n  knn.fit(X_train, y_train)\n  pred_i = knn.predict(X_test)\n  error_rate.append(np.mean(pred_i != y_test))\n\nplt.Figure(figsize=(10, 6))\nplt.plot(range(1, 40), error_rate, color='blue', ls='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Error Rate with K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\n```\n\n#### 参考\n","slug":"2019-08-08-K-近邻算法","published":1,"updated":"2019-08-08T09:14:27.528Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz3pnqfm000nz0ovqsryc9ek","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<p>给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。</p>\n<ul>\n<li>优点：\n<ul>\n<li>算法简单</li>\n<li>训练简单</li>\n<li>适用于任意数量的分类</li>\n<li>容易添加更多数据</li>\n<li>精度高</li>\n<li>对异常值不敏感</li>\n<li>无数据输入假定</li>\n<li>参数少：\n<ul>\n<li>K</li>\n<li>distance metric</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>缺点：\n<ul>\n<li>高预测成本（大的数据集更糟）</li>\n<li>高维数据不太好</li>\n<li>计算复杂度高</li>\n<li>空间复杂度高</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"原理\"> 原理</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系;</span><br><span class=\"line\">2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。</span><br><span class=\"line\">  - 计算新数据与样本数据集中每条数据的距离。</span><br><span class=\"line\">  - 对求得的所有距离进行排序（从小到大，越小表示越相似）。</span><br><span class=\"line\">3. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。</span><br><span class=\"line\">4. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。</span><br></pre></td></tr></table></figure>\n<h4><span id=\"归一化\"> 归一化</span></h4>\n<ul>\n<li>定义</li>\n</ul>\n<blockquote>\n<p>归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。</p>\n</blockquote>\n<ul>\n<li>方法\n<ul>\n<li>\n<p>线性函数转换，表达式如下：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>M</mi><mi>i</mi><mi>n</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow><mrow><mi>M</mi><mi>a</mi><mi>x</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi><mo>−</mo><mi>M</mi><mi>i</mi><mi>n</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">y=\\frac{x-MinValue}{MaxValue-MinValue}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.37144em;\"></span><span class=\"strut bottom\" style=\"height:2.14077em;vertical-align:-0.7693300000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">x</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">e</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">e</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">x</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">e</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>说明：x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。</p>\n</li>\n<li>\n<p>对数函数转换，表达式如下:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><msup><mi>h</mi><mn>2</mn></msup><mrow><mo fence=\"true\">(</mo><mfrac><mrow><msup><mi mathvariant=\"normal\">∂</mi><mn>2</mn></msup><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>x</mi><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><msup><mi mathvariant=\"normal\">∂</mi><mn>2</mn></msup><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>y</mi><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><msup><mi mathvariant=\"normal\">∂</mi><mn>2</mn></msup><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>z</mi><mn>2</mn></msup></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial u}{\\partial t} = h^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + \\frac{\\partial^2 u}{\\partial z^2}\\right)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.491108em;\"></span><span class=\"strut bottom\" style=\"height:2.441138em;vertical-align:-0.95003em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathit\">t</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"minner displaystyle textstyle uncramped\"><span class=\"style-wrap reset-textstyle textstyle uncramped\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mbin\">+</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.6860000000000002em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mbin\">+</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"style-wrap reset-textstyle textstyle uncramped\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span></span></span></span></span></p>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"评估指标\"> 评估指标</span></h4>\n<h4><span id=\"scikitlearn-中的knn\"> ScikitLearn 中的kNN</span></h4>\n<ul>\n<li>归一化数值</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.preprocessing import StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">scaler.fit(df.drop(&apos;TARGET CLASS&apos;, axis=1))</span><br><span class=\"line\">scaled_features = scaler.transform(df.drop(&apos;TARGET CLASS&apos;, axis=1))</span><br><span class=\"line\">df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用算法</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.neighbors import KNeighborsClassifier</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">X = df_feat</span><br><span class=\"line\">y = df[&apos;TARGET CLASS&apos;]</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)</span><br><span class=\"line\">from sklearn.neighbors import KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\">knn = KNeighborsClassifier(n_neighbors=1)</span><br><span class=\"line\">knn.fit(X_train, y_train)</span><br><span class=\"line\">pred = knn.predict(X_test)</span><br><span class=\"line\">print(confusion_matrix(y_test, pred))</span><br><span class=\"line\">print(classification_report(y_test, pred))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>比较K值对算法的影响</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">error_rate = []</span><br><span class=\"line\"></span><br><span class=\"line\">for i in range(1, 40):</span><br><span class=\"line\">  knn = KNeighborsClassifier(n_neighbors=i)</span><br><span class=\"line\">  knn.fit(X_train, y_train)</span><br><span class=\"line\">  pred_i = knn.predict(X_test)</span><br><span class=\"line\">  error_rate.append(np.mean(pred_i != y_test))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.Figure(figsize=(10, 6))</span><br><span class=\"line\">plt.plot(range(1, 40), error_rate, color=&apos;blue&apos;, ls=&apos;dashed&apos;, marker=&apos;o&apos;, markerfacecolor=&apos;red&apos;, markersize=10)</span><br><span class=\"line\">plt.title(&apos;Error Rate with K Value&apos;)</span><br><span class=\"line\">plt.xlabel(&apos;K&apos;)</span><br><span class=\"line\">plt.ylabel(&apos;Error Rate&apos;)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"参考\"> 参考</span></h4>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<p>给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。</p>\n<ul>\n<li>优点：\n<ul>\n<li>算法简单</li>\n<li>训练简单</li>\n<li>适用于任意数量的分类</li>\n<li>容易添加更多数据</li>\n<li>精度高</li>\n<li>对异常值不敏感</li>\n<li>无数据输入假定</li>\n<li>参数少：\n<ul>\n<li>K</li>\n<li>distance metric</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>缺点：\n<ul>\n<li>高预测成本（大的数据集更糟）</li>\n<li>高维数据不太好</li>\n<li>计算复杂度高</li>\n<li>空间复杂度高</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"原理\"><a class=\"markdownIt-Anchor\" href=\"#原理\"></a> 原理</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系;</span><br><span class=\"line\">2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。</span><br><span class=\"line\">  - 计算新数据与样本数据集中每条数据的距离。</span><br><span class=\"line\">  - 对求得的所有距离进行排序（从小到大，越小表示越相似）。</span><br><span class=\"line\">3. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。</span><br><span class=\"line\">4. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。</span><br></pre></td></tr></table></figure>\n<h4 id=\"归一化\"><a class=\"markdownIt-Anchor\" href=\"#归一化\"></a> 归一化</h4>\n<ul>\n<li>定义</li>\n</ul>\n<blockquote>\n<p>归一化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内。首先归一化是为了后面数据处理的方便，其次是保正程序运行时收敛加快。</p>\n</blockquote>\n<ul>\n<li>方法\n<ul>\n<li>\n<p>线性函数转换，表达式如下：</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>M</mi><mi>i</mi><mi>n</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow><mrow><mi>M</mi><mi>a</mi><mi>x</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi><mo>−</mo><mi>M</mi><mi>i</mi><mi>n</mi><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">y=\\frac{x-MinValue}{MaxValue-MinValue}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.37144em;\"></span><span class=\"strut bottom\" style=\"height:2.14077em;vertical-align:-0.7693300000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">x</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">e</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">e</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\">x</span><span class=\"mbin\">−</span><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">M</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\" style=\"margin-right:0.22222em;\">V</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">e</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<p>说明：x、y分别为转换前、后的值，MaxValue、MinValue分别为样本的最大值和最小值。</p>\n</li>\n<li>\n<p>对数函数转换，表达式如下:</p>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi mathvariant=\"normal\">∂</mi><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><msup><mi>h</mi><mn>2</mn></msup><mrow><mo fence=\"true\">(</mo><mfrac><mrow><msup><mi mathvariant=\"normal\">∂</mi><mn>2</mn></msup><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>x</mi><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><msup><mi mathvariant=\"normal\">∂</mi><mn>2</mn></msup><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>y</mi><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><msup><mi mathvariant=\"normal\">∂</mi><mn>2</mn></msup><mi>u</mi></mrow><mrow><mi mathvariant=\"normal\">∂</mi><msup><mi>z</mi><mn>2</mn></msup></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\frac{\\partial u}{\\partial t} = h^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} + \\frac{\\partial^2 u}{\\partial z^2}\\right)\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.491108em;\"></span><span class=\"strut bottom\" style=\"height:2.441138em;vertical-align:-0.95003em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathit\">t</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mrel\">=</span><span class=\"mord\"><span class=\"mord mathit\">h</span><span class=\"vlist\"><span style=\"top:-0.413em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"minner displaystyle textstyle uncramped\"><span class=\"style-wrap reset-textstyle textstyle uncramped\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathit\">x</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mbin\">+</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.6860000000000002em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">y</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mbin\">+</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.686em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.04398em;\">z</span><span class=\"vlist\"><span style=\"top:-0.289em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord\"><span class=\"mord mathrm\" style=\"margin-right:0.05556em;\">∂</span><span class=\"vlist\"><span style=\"top:-0.363em;margin-right:0.05em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle uncramped\"><span class=\"mord mathrm\">2</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mord mathit\">u</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"style-wrap reset-textstyle textstyle uncramped\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span></span></span></span></span></p>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"评估指标\"><a class=\"markdownIt-Anchor\" href=\"#评估指标\"></a> 评估指标</h4>\n<h4 id=\"scikitlearn-中的knn\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中的knn\"></a> ScikitLearn 中的kNN</h4>\n<ul>\n<li>归一化数值</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.preprocessing import StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">scaler.fit(df.drop(&apos;TARGET CLASS&apos;, axis=1))</span><br><span class=\"line\">scaled_features = scaler.transform(df.drop(&apos;TARGET CLASS&apos;, axis=1))</span><br><span class=\"line\">df_feat = pd.DataFrame(scaled_features, columns=df.columns[:-1])</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用算法</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.neighbors import KNeighborsClassifier</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">X = df_feat</span><br><span class=\"line\">y = df[&apos;TARGET CLASS&apos;]</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)</span><br><span class=\"line\">from sklearn.neighbors import KNeighborsClassifier</span><br><span class=\"line\"></span><br><span class=\"line\">knn = KNeighborsClassifier(n_neighbors=1)</span><br><span class=\"line\">knn.fit(X_train, y_train)</span><br><span class=\"line\">pred = knn.predict(X_test)</span><br><span class=\"line\">print(confusion_matrix(y_test, pred))</span><br><span class=\"line\">print(classification_report(y_test, pred))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>比较K值对算法的影响</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">error_rate = []</span><br><span class=\"line\"></span><br><span class=\"line\">for i in range(1, 40):</span><br><span class=\"line\">  knn = KNeighborsClassifier(n_neighbors=i)</span><br><span class=\"line\">  knn.fit(X_train, y_train)</span><br><span class=\"line\">  pred_i = knn.predict(X_test)</span><br><span class=\"line\">  error_rate.append(np.mean(pred_i != y_test))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.Figure(figsize=(10, 6))</span><br><span class=\"line\">plt.plot(range(1, 40), error_rate, color=&apos;blue&apos;, ls=&apos;dashed&apos;, marker=&apos;o&apos;, markerfacecolor=&apos;red&apos;, markersize=10)</span><br><span class=\"line\">plt.title(&apos;Error Rate with K Value&apos;)</span><br><span class=\"line\">plt.xlabel(&apos;K&apos;)</span><br><span class=\"line\">plt.ylabel(&apos;Error Rate&apos;)</span><br></pre></td></tr></table></figure>\n<h4 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h4>\n"},{"title":"支持向量机 SVM","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-10T06:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"#### 概念\n* 将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。\n* 基本模型是定义在特征空间上的间隔最大的线性分类器\n* SVM的的学习算法就是求解凸二次规划的最优化算法。\n#### 数学理论\n\n\n#### 评估指标\n\n\n#### ScikitLearn 中的线性回归用法\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX = df_feat\ny = cancer['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nmodel = SVC()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n~~~\n* 使用GridSearchCV进行自动调参\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\n\ngrid.fit(X_train, y_train)\ngrid.best_params_\ngrid.best_estimator_\ngrid_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, grid_pred))\nprint(confusion_matrix(y_test, grid_pred))\n~~~\n\n#### 数学知识补充","source":"_posts/2019-09-10-支持向量机.md","raw":"---\ntitle: \"支持向量机 SVM\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-10 14:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 分类算法\ncatagories:\n- ML 算法\n\n---\n#### 概念\n* 将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。\n* 基本模型是定义在特征空间上的间隔最大的线性分类器\n* SVM的的学习算法就是求解凸二次规划的最优化算法。\n#### 数学理论\n\n\n#### 评估指标\n\n\n#### ScikitLearn 中的线性回归用法\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX = df_feat\ny = cancer['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nmodel = SVC()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n~~~\n* 使用GridSearchCV进行自动调参\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\n\ngrid.fit(X_train, y_train)\ngrid.best_params_\ngrid.best_estimator_\ngrid_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, grid_pred))\nprint(confusion_matrix(y_test, grid_pred))\n~~~\n\n#### 数学知识补充","slug":"2019-09-10-支持向量机","published":1,"updated":"2019-08-09T09:27:13.022Z","_id":"cjz3rrdb10014z0ovl76ycqdt","comments":1,"layout":"post","photos":[],"link":"","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<ul>\n<li>将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。</li>\n<li>基本模型是定义在特征空间上的间隔最大的线性分类器</li>\n<li>SVM的的学习算法就是求解凸二次规划的最优化算法。</li>\n</ul>\n<h4><span id=\"数学理论\"> 数学理论</span></h4>\n<h4><span id=\"评估指标\"> 评估指标</span></h4>\n<h4><span id=\"scikitlearn-中的线性回归用法\"> ScikitLearn 中的线性回归用法</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">X = df_feat</span><br><span class=\"line\">y = cancer[&apos;target&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span><br><span class=\"line\">model = SVC()</span><br><span class=\"line\">model.fit(X_train, y_train)</span><br><span class=\"line\">pred = model.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\">print(classification_report(y_test, pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, pred))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用GridSearchCV进行自动调参</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">from sklearn.model_selection import GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\">param_grid = &#123;&apos;C&apos;: [0.1, 1, 10, 100, 1000], &apos;gamma&apos;: [1, 0.1, 0.01, 0.001, 0.0001]&#125;</span><br><span class=\"line\">grid = GridSearchCV(SVC(), param_grid, verbose=3)</span><br><span class=\"line\"></span><br><span class=\"line\">grid.fit(X_train, y_train)</span><br><span class=\"line\">grid.best_params_</span><br><span class=\"line\">grid.best_estimator_</span><br><span class=\"line\">grid_pred = grid.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, grid_pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, grid_pred))</span><br></pre></td></tr></table></figure>\n<h4><span id=\"数学知识补充\"> 数学知识补充</span></h4>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<ul>\n<li>将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。</li>\n<li>基本模型是定义在特征空间上的间隔最大的线性分类器</li>\n<li>SVM的的学习算法就是求解凸二次规划的最优化算法。</li>\n</ul>\n<h4 id=\"数学理论\"><a class=\"markdownIt-Anchor\" href=\"#数学理论\"></a> 数学理论</h4>\n<h4 id=\"评估指标\"><a class=\"markdownIt-Anchor\" href=\"#评估指标\"></a> 评估指标</h4>\n<h4 id=\"scikitlearn-中的线性回归用法\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中的线性回归用法\"></a> ScikitLearn 中的线性回归用法</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">X = df_feat</span><br><span class=\"line\">y = cancer[&apos;target&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span><br><span class=\"line\">model = SVC()</span><br><span class=\"line\">model.fit(X_train, y_train)</span><br><span class=\"line\">pred = model.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\">print(classification_report(y_test, pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, pred))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用GridSearchCV进行自动调参</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">from sklearn.model_selection import GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\">param_grid = &#123;&apos;C&apos;: [0.1, 1, 10, 100, 1000], &apos;gamma&apos;: [1, 0.1, 0.01, 0.001, 0.0001]&#125;</span><br><span class=\"line\">grid = GridSearchCV(SVC(), param_grid, verbose=3)</span><br><span class=\"line\"></span><br><span class=\"line\">grid.fit(X_train, y_train)</span><br><span class=\"line\">grid.best_params_</span><br><span class=\"line\">grid.best_estimator_</span><br><span class=\"line\">grid_pred = grid.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, grid_pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, grid_pred))</span><br></pre></td></tr></table></figure>\n<h4 id=\"数学知识补充\"><a class=\"markdownIt-Anchor\" href=\"#数学知识补充\"></a> 数学知识补充</h4>\n"},{"title":"K均值聚类，K Means Clustering","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-12T06:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"\n#### 概念\nK均值聚类是一种无监督学习算法，尝试在数据中将类似的群集组合在一起。\n* 典型的聚类问题是什么样的呢？\n  * 集群类似文档\n  * 基于特征的客户群\n  * 市场细分\n  * 识别类似的物理组\n\n* 算法过程\n  1. 选择集群的个数 ‘K’\n  2. 随机将每个点分配给一个集群\n  3. 直到集群停止改变，重复以下：\n    - 对于每个群集，通过获取群集中点的平均向量来计算群集质心\n    - 将每个数据点分配最接近的群集质心\n\n* K的选择\n  - 没有简单的方法选择K\n  - 一种方式用elbow方\n    + 首先，计算某些k值（例如2,4,6,8等）的平方误差之和（SSE）\n    + SSE定义为群集中每个成员与其质心之间的平方距离的总和\n    + 如果您对SSE绘制k，您将看到随着k变大，误差减小; 这是因为当簇的数量增加时，它们应该更小，因此失真也更小。\n    + elbow 方法的idea是选取SSE突然减小的K\n#### 数学理论\n#### 评估指标\n#### ScikitLearn 中的线性回归用法\n~~~\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\ndata = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.8, random_state=101)\n\nplt.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap='rainbow')\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(data[0])\nkmeans.cluster_centers_\nkmeans.labels_\n\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 6))\n\nax1.set_title('K Means')\nax1.scatter(data[0][:,0], data[0][:, 1], c=kmeans.labels_, cmap='rainbow')\n\nax2.set_title('Original')\nax2.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap='rainbow')\n~~~\n\n#### 数学知识补充","source":"_posts/2019-08-12-K-均值聚类.md","raw":"---\ntitle: \"K均值聚类，K Means Clustering\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-12 14:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 分类算法\ncatagories:\n- ML 算法\n\n---\n\n#### 概念\nK均值聚类是一种无监督学习算法，尝试在数据中将类似的群集组合在一起。\n* 典型的聚类问题是什么样的呢？\n  * 集群类似文档\n  * 基于特征的客户群\n  * 市场细分\n  * 识别类似的物理组\n\n* 算法过程\n  1. 选择集群的个数 ‘K’\n  2. 随机将每个点分配给一个集群\n  3. 直到集群停止改变，重复以下：\n    - 对于每个群集，通过获取群集中点的平均向量来计算群集质心\n    - 将每个数据点分配最接近的群集质心\n\n* K的选择\n  - 没有简单的方法选择K\n  - 一种方式用elbow方\n    + 首先，计算某些k值（例如2,4,6,8等）的平方误差之和（SSE）\n    + SSE定义为群集中每个成员与其质心之间的平方距离的总和\n    + 如果您对SSE绘制k，您将看到随着k变大，误差减小; 这是因为当簇的数量增加时，它们应该更小，因此失真也更小。\n    + elbow 方法的idea是选取SSE突然减小的K\n#### 数学理论\n#### 评估指标\n#### ScikitLearn 中的线性回归用法\n~~~\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\ndata = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.8, random_state=101)\n\nplt.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap='rainbow')\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(data[0])\nkmeans.cluster_centers_\nkmeans.labels_\n\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 6))\n\nax1.set_title('K Means')\nax1.scatter(data[0][:,0], data[0][:, 1], c=kmeans.labels_, cmap='rainbow')\n\nax2.set_title('Original')\nax2.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap='rainbow')\n~~~\n\n#### 数学知识补充","slug":"2019-08-12-K-均值聚类","published":1,"updated":"2019-08-12T06:27:54.979Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz87p1ca0000z2ovakimkter","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<p>K均值聚类是一种无监督学习算法，尝试在数据中将类似的群集组合在一起。</p>\n<ul>\n<li>\n<p>典型的聚类问题是什么样的呢？</p>\n<ul>\n<li>集群类似文档</li>\n<li>基于特征的客户群</li>\n<li>市场细分</li>\n<li>识别类似的物理组</li>\n</ul>\n</li>\n<li>\n<p>算法过程</p>\n<ol>\n<li>选择集群的个数 ‘K’</li>\n<li>随机将每个点分配给一个集群</li>\n<li>直到集群停止改变，重复以下：</li>\n</ol>\n<ul>\n<li>对于每个群集，通过获取群集中点的平均向量来计算群集质心</li>\n<li>将每个数据点分配最接近的群集质心</li>\n</ul>\n</li>\n<li>\n<p>K的选择</p>\n<ul>\n<li>没有简单的方法选择K</li>\n<li>一种方式用elbow方\n<ul>\n<li>首先，计算某些k值（例如2,4,6,8等）的平方误差之和（SSE）</li>\n<li>SSE定义为群集中每个成员与其质心之间的平方距离的总和</li>\n<li>如果您对SSE绘制k，您将看到随着k变大，误差减小; 这是因为当簇的数量增加时，它们应该更小，因此失真也更小。</li>\n<li>elbow 方法的idea是选取SSE突然减小的K</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4><span id=\"数学理论\"> 数学理论</span></h4>\n<h4><span id=\"评估指标\"> 评估指标</span></h4>\n<h4><span id=\"scikitlearn-中的线性回归用法\"> ScikitLearn 中的线性回归用法</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import seaborn as sns</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">sns.set_style(&apos;whitegrid&apos;)</span><br><span class=\"line\">from sklearn.datasets import make_blobs</span><br><span class=\"line\">from sklearn.cluster import KMeans</span><br><span class=\"line\">data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.8, random_state=101)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap=&apos;rainbow&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">kmeans = KMeans(n_clusters=4)</span><br><span class=\"line\">kmeans.fit(data[0])</span><br><span class=\"line\">kmeans.cluster_centers_</span><br><span class=\"line\">kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\">fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 6))</span><br><span class=\"line\"></span><br><span class=\"line\">ax1.set_title(&apos;K Means&apos;)</span><br><span class=\"line\">ax1.scatter(data[0][:,0], data[0][:, 1], c=kmeans.labels_, cmap=&apos;rainbow&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">ax2.set_title(&apos;Original&apos;)</span><br><span class=\"line\">ax2.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap=&apos;rainbow&apos;)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"数学知识补充\"> 数学知识补充</span></h4>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<p>K均值聚类是一种无监督学习算法，尝试在数据中将类似的群集组合在一起。</p>\n<ul>\n<li>\n<p>典型的聚类问题是什么样的呢？</p>\n<ul>\n<li>集群类似文档</li>\n<li>基于特征的客户群</li>\n<li>市场细分</li>\n<li>识别类似的物理组</li>\n</ul>\n</li>\n<li>\n<p>算法过程</p>\n<ol>\n<li>选择集群的个数 ‘K’</li>\n<li>随机将每个点分配给一个集群</li>\n<li>直到集群停止改变，重复以下：</li>\n</ol>\n<ul>\n<li>对于每个群集，通过获取群集中点的平均向量来计算群集质心</li>\n<li>将每个数据点分配最接近的群集质心</li>\n</ul>\n</li>\n<li>\n<p>K的选择</p>\n<ul>\n<li>没有简单的方法选择K</li>\n<li>一种方式用elbow方\n<ul>\n<li>首先，计算某些k值（例如2,4,6,8等）的平方误差之和（SSE）</li>\n<li>SSE定义为群集中每个成员与其质心之间的平方距离的总和</li>\n<li>如果您对SSE绘制k，您将看到随着k变大，误差减小; 这是因为当簇的数量增加时，它们应该更小，因此失真也更小。</li>\n<li>elbow 方法的idea是选取SSE突然减小的K</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"数学理论\"><a class=\"markdownIt-Anchor\" href=\"#数学理论\"></a> 数学理论</h4>\n<h4 id=\"评估指标\"><a class=\"markdownIt-Anchor\" href=\"#评估指标\"></a> 评估指标</h4>\n<h4 id=\"scikitlearn-中的线性回归用法\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中的线性回归用法\"></a> ScikitLearn 中的线性回归用法</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import seaborn as sns</span><br><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">sns.set_style(&apos;whitegrid&apos;)</span><br><span class=\"line\">from sklearn.datasets import make_blobs</span><br><span class=\"line\">from sklearn.cluster import KMeans</span><br><span class=\"line\">data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.8, random_state=101)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap=&apos;rainbow&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">kmeans = KMeans(n_clusters=4)</span><br><span class=\"line\">kmeans.fit(data[0])</span><br><span class=\"line\">kmeans.cluster_centers_</span><br><span class=\"line\">kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\">fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 6))</span><br><span class=\"line\"></span><br><span class=\"line\">ax1.set_title(&apos;K Means&apos;)</span><br><span class=\"line\">ax1.scatter(data[0][:,0], data[0][:, 1], c=kmeans.labels_, cmap=&apos;rainbow&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">ax2.set_title(&apos;Original&apos;)</span><br><span class=\"line\">ax2.scatter(data[0][:,0], data[0][:, 1], c=data[1], cmap=&apos;rainbow&apos;)</span><br></pre></td></tr></table></figure>\n<h4 id=\"数学知识补充\"><a class=\"markdownIt-Anchor\" href=\"#数学知识补充\"></a> 数学知识补充</h4>\n"},{"title":"支持向量机 SVM","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-10T06:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML 算法"],"_content":"#### 概念\n* 将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。\n* 基本模型是定义在特征空间上的间隔最大的线性分类器\n* SVM的的学习算法就是求解凸二次规划的最优化算法。\n#### 数学理论\n\n\n#### 评估指标\n\n\n#### ScikitLearn 中的线性回归用法\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX = df_feat\ny = cancer['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nmodel = SVC()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n~~~\n* 使用GridSearchCV进行自动调参\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\n\ngrid.fit(X_train, y_train)\ngrid.best_params_\ngrid.best_estimator_\ngrid_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, grid_pred))\nprint(confusion_matrix(y_test, grid_pred))\n~~~\n\n#### 数学知识补充","source":"_posts/2019-08-10-支持向量机.md","raw":"---\ntitle: \"支持向量机 SVM\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-10 14:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 分类算法\ncatagories:\n- ML 算法\n\n---\n#### 概念\n* 将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。\n* 基本模型是定义在特征空间上的间隔最大的线性分类器\n* SVM的的学习算法就是求解凸二次规划的最优化算法。\n#### 数学理论\n\n\n#### 评估指标\n\n\n#### ScikitLearn 中的线性回归用法\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX = df_feat\ny = cancer['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nmodel = SVC()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, pred))\nprint(confusion_matrix(y_test, pred))\n~~~\n* 使用GridSearchCV进行自动调参\n~~~\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\ngrid = GridSearchCV(SVC(), param_grid, verbose=3)\n\ngrid.fit(X_train, y_train)\ngrid.best_params_\ngrid.best_estimator_\ngrid_pred = grid.predict(X_test)\n\nprint(classification_report(y_test, grid_pred))\nprint(confusion_matrix(y_test, grid_pred))\n~~~\n\n#### 数学知识补充","slug":"2019-08-10-支持向量机","published":1,"updated":"2019-08-12T02:03:34.491Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjz87p1ck0003z2ovuu5qsupz","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<ul>\n<li>将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。</li>\n<li>基本模型是定义在特征空间上的间隔最大的线性分类器</li>\n<li>SVM的的学习算法就是求解凸二次规划的最优化算法。</li>\n</ul>\n<h4><span id=\"数学理论\"> 数学理论</span></h4>\n<h4><span id=\"评估指标\"> 评估指标</span></h4>\n<h4><span id=\"scikitlearn-中的线性回归用法\"> ScikitLearn 中的线性回归用法</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">X = df_feat</span><br><span class=\"line\">y = cancer[&apos;target&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span><br><span class=\"line\">model = SVC()</span><br><span class=\"line\">model.fit(X_train, y_train)</span><br><span class=\"line\">pred = model.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\">print(classification_report(y_test, pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, pred))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用GridSearchCV进行自动调参</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">from sklearn.model_selection import GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\">param_grid = &#123;&apos;C&apos;: [0.1, 1, 10, 100, 1000], &apos;gamma&apos;: [1, 0.1, 0.01, 0.001, 0.0001]&#125;</span><br><span class=\"line\">grid = GridSearchCV(SVC(), param_grid, verbose=3)</span><br><span class=\"line\"></span><br><span class=\"line\">grid.fit(X_train, y_train)</span><br><span class=\"line\">grid.best_params_</span><br><span class=\"line\">grid.best_estimator_</span><br><span class=\"line\">grid_pred = grid.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, grid_pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, grid_pred))</span><br></pre></td></tr></table></figure>\n<h4><span id=\"数学知识补充\"> 数学知识补充</span></h4>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<ul>\n<li>将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。</li>\n<li>基本模型是定义在特征空间上的间隔最大的线性分类器</li>\n<li>SVM的的学习算法就是求解凸二次规划的最优化算法。</li>\n</ul>\n<h4 id=\"数学理论\"><a class=\"markdownIt-Anchor\" href=\"#数学理论\"></a> 数学理论</h4>\n<h4 id=\"评估指标\"><a class=\"markdownIt-Anchor\" href=\"#评估指标\"></a> 评估指标</h4>\n<h4 id=\"scikitlearn-中的线性回归用法\"><a class=\"markdownIt-Anchor\" href=\"#scikitlearn-中的线性回归用法\"></a> ScikitLearn 中的线性回归用法</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">X = df_feat</span><br><span class=\"line\">y = cancer[&apos;target&apos;]</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span><br><span class=\"line\">model = SVC()</span><br><span class=\"line\">model.fit(X_train, y_train)</span><br><span class=\"line\">pred = model.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\">print(classification_report(y_test, pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, pred))</span><br></pre></td></tr></table></figure>\n<ul>\n<li>使用GridSearchCV进行自动调参</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from sklearn.model_selection import train_test_split</span><br><span class=\"line\">from sklearn.svm import SVC</span><br><span class=\"line\">from sklearn.model_selection import GridSearchCV</span><br><span class=\"line\"></span><br><span class=\"line\">param_grid = &#123;&apos;C&apos;: [0.1, 1, 10, 100, 1000], &apos;gamma&apos;: [1, 0.1, 0.01, 0.001, 0.0001]&#125;</span><br><span class=\"line\">grid = GridSearchCV(SVC(), param_grid, verbose=3)</span><br><span class=\"line\"></span><br><span class=\"line\">grid.fit(X_train, y_train)</span><br><span class=\"line\">grid.best_params_</span><br><span class=\"line\">grid.best_estimator_</span><br><span class=\"line\">grid_pred = grid.predict(X_test)</span><br><span class=\"line\"></span><br><span class=\"line\">print(classification_report(y_test, grid_pred))</span><br><span class=\"line\">print(confusion_matrix(y_test, grid_pred))</span><br></pre></td></tr></table></figure>\n<h4 id=\"数学知识补充\"><a class=\"markdownIt-Anchor\" href=\"#数学知识补充\"></a> 数学知识补充</h4>\n"},{"title":"推荐系统","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-13T06:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML"],"_content":"\n#### 背景\n\n#### 概念\n\n\n#### 应用例子\n","source":"_posts/2019-08-13-推荐系统.md","raw":"---\ntitle: \"推荐系统\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-13 14:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 应用\ncatagories:\n- ML\n---\n\n#### 背景\n\n#### 概念\n\n\n#### 应用例子\n","slug":"2019-08-13-推荐系统","published":1,"updated":"2019-08-12T10:10:37.952Z","_id":"cjz87xn610007z2ov0sa4omo3","comments":1,"layout":"post","photos":[],"link":"","content":"<h4><span id=\"背景\"> 背景</span></h4>\n<h4><span id=\"概念\"> 概念</span></h4>\n<h4><span id=\"应用例子\"> 应用例子</span></h4>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"背景\"><a class=\"markdownIt-Anchor\" href=\"#背景\"></a> 背景</h4>\n<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<h4 id=\"应用例子\"><a class=\"markdownIt-Anchor\" href=\"#应用例子\"></a> 应用例子</h4>\n"},{"title":"主成分分析，PCA","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-12T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML"],"_content":"\n#### 概念\n\n* 多重共线性 multi-collinearity\n  多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。\n* PCA\n  通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量\n* 降维\n  就是找到k个超平面，把原n维空间的数据投影到这k个超平面上，使原n维数据降为k维。\n* PCA核心思想\n  降维后的数据尽可能分散\n* PCA的目的\n  是找到一个投影矩阵，使得样本点投影后尽可能分散，即使得样本点投影后的方差最大，并且为了避免投影的堆叠，投影矩阵中的各个向量应该是正交的。\n* 中心化\n\n#### PCA过程总结\n1. 初始化X，使得所有样本之间的特征值均值为0，同时应用feature scaling，缩放到-0.5～0.5\n2. 计算X的协方差矩阵S\n3. 对S进行SVD分解，U即我们要求的新坐标系集合， Σ \\SigmaΣ 为特征值集合（计算时特征值都会大于0，且结果会从小到大排列)\n4. 按照特征值从大到小排序，要降低为k维，那么取前k个特征值对应的特征向量，就是新的k个坐标轴\n5. 把X映射到新的坐标系中，完整降维操作\n#### 主成份数量的选择\nPCA使得数据从n维降低为k维度。\n一般选择标准为：投影前后方差比例值，作为k值的选择标准。\n\n#### Scikit Learn\n~~~\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ncancer = load_breast_cancer()\ndf = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n\nscaler = StandardScaler()\nscaler.fit(df)\nscaled_data = scaler.transform(df)\n\npca = PCA(n_components=2)\npca.fit(scaled_data)\nx_pca = pca.transform(scaled_data)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x_pca[:,0], x_pca[:, 1], c=cancer['target'], cmap='plasma')\nplt.xlabel('First Principle Component')\nplt.ylabel('Second Principle Component')\n~~~\n#### 数学理论\n* 矩阵换基底\n  坐标变换地目标是，找到一组新的正交单位向量，替换原来的正交单位向量。\n* 拉格朗日乘子法\n  拉格朗日乘子法主要提供了一种求解函数在约束条件下极值的方法。\n* 协方差矩阵\n  协方差研究的目的是变量（特征）之间的关系\n  每个样本点到样本中心距离的平方和的平均 = 样本各个特征方差和（自身协方差），样本的方差\n* 特征向量\n* 奇异值分解\n  奇异值分解（svd: singular value decomposition）定义：对于任意的矩阵A，存在：\n* 特征向量和奇异值分解的关系\n\n#### 参考文章\n[详细推导PCA算法](https://zhuanlan.zhihu.com/p/55297233)\n","source":"_posts/2019-08-12-主成分分析.md","raw":"---\ntitle: \"主成分分析，PCA\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-12 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 数据降维\ncatagories:\n- ML\n\n---\n\n#### 概念\n\n* 多重共线性 multi-collinearity\n  多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。\n* PCA\n  通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量\n* 降维\n  就是找到k个超平面，把原n维空间的数据投影到这k个超平面上，使原n维数据降为k维。\n* PCA核心思想\n  降维后的数据尽可能分散\n* PCA的目的\n  是找到一个投影矩阵，使得样本点投影后尽可能分散，即使得样本点投影后的方差最大，并且为了避免投影的堆叠，投影矩阵中的各个向量应该是正交的。\n* 中心化\n\n#### PCA过程总结\n1. 初始化X，使得所有样本之间的特征值均值为0，同时应用feature scaling，缩放到-0.5～0.5\n2. 计算X的协方差矩阵S\n3. 对S进行SVD分解，U即我们要求的新坐标系集合， Σ \\SigmaΣ 为特征值集合（计算时特征值都会大于0，且结果会从小到大排列)\n4. 按照特征值从大到小排序，要降低为k维，那么取前k个特征值对应的特征向量，就是新的k个坐标轴\n5. 把X映射到新的坐标系中，完整降维操作\n#### 主成份数量的选择\nPCA使得数据从n维降低为k维度。\n一般选择标准为：投影前后方差比例值，作为k值的选择标准。\n\n#### Scikit Learn\n~~~\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nsns.set_style('whitegrid')\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ncancer = load_breast_cancer()\ndf = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])\n\nscaler = StandardScaler()\nscaler.fit(df)\nscaled_data = scaler.transform(df)\n\npca = PCA(n_components=2)\npca.fit(scaled_data)\nx_pca = pca.transform(scaled_data)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x_pca[:,0], x_pca[:, 1], c=cancer['target'], cmap='plasma')\nplt.xlabel('First Principle Component')\nplt.ylabel('Second Principle Component')\n~~~\n#### 数学理论\n* 矩阵换基底\n  坐标变换地目标是，找到一组新的正交单位向量，替换原来的正交单位向量。\n* 拉格朗日乘子法\n  拉格朗日乘子法主要提供了一种求解函数在约束条件下极值的方法。\n* 协方差矩阵\n  协方差研究的目的是变量（特征）之间的关系\n  每个样本点到样本中心距离的平方和的平均 = 样本各个特征方差和（自身协方差），样本的方差\n* 特征向量\n* 奇异值分解\n  奇异值分解（svd: singular value decomposition）定义：对于任意的矩阵A，存在：\n* 特征向量和奇异值分解的关系\n\n#### 参考文章\n[详细推导PCA算法](https://zhuanlan.zhihu.com/p/55297233)\n","slug":"2019-08-12-主成分分析","published":1,"updated":"2019-08-12T09:52:50.596Z","_id":"cjz87xsza0008z2ovewboc383","comments":1,"layout":"post","photos":[],"link":"","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<ul>\n<li>多重共线性 multi-collinearity<br>\n多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。</li>\n<li>PCA<br>\n通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量</li>\n<li>降维<br>\n就是找到k个超平面，把原n维空间的数据投影到这k个超平面上，使原n维数据降为k维。</li>\n<li>PCA核心思想<br>\n降维后的数据尽可能分散</li>\n<li>PCA的目的<br>\n是找到一个投影矩阵，使得样本点投影后尽可能分散，即使得样本点投影后的方差最大，并且为了避免投影的堆叠，投影矩阵中的各个向量应该是正交的。</li>\n<li>中心化</li>\n</ul>\n<h4><span id=\"pca过程总结\"> PCA过程总结</span></h4>\n<ol>\n<li>初始化X，使得所有样本之间的特征值均值为0，同时应用feature scaling，缩放到-0.5～0.5</li>\n<li>计算X的协方差矩阵S</li>\n<li>对S进行SVD分解，U即我们要求的新坐标系集合， Σ \\SigmaΣ 为特征值集合（计算时特征值都会大于0，且结果会从小到大排列)</li>\n<li>按照特征值从大到小排序，要降低为k维，那么取前k个特征值对应的特征向量，就是新的k个坐标轴</li>\n<li>把X映射到新的坐标系中，完整降维操作</li>\n</ol>\n<h4><span id=\"主成份数量的选择\"> 主成份数量的选择</span></h4>\n<p>PCA使得数据从n维降低为k维度。<br>\n一般选择标准为：投影前后方差比例值，作为k值的选择标准。</p>\n<h4><span id=\"scikit-learn\"> Scikit Learn</span></h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">import seaborn as sns</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">sns.set_style(&apos;whitegrid&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn.datasets import load_breast_cancer</span><br><span class=\"line\">from sklearn.preprocessing import StandardScaler</span><br><span class=\"line\">from sklearn.decomposition import PCA</span><br><span class=\"line\"></span><br><span class=\"line\">cancer = load_breast_cancer()</span><br><span class=\"line\">df = pd.DataFrame(cancer[&apos;data&apos;], columns=cancer[&apos;feature_names&apos;])</span><br><span class=\"line\"></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">scaler.fit(df)</span><br><span class=\"line\">scaled_data = scaler.transform(df)</span><br><span class=\"line\"></span><br><span class=\"line\">pca = PCA(n_components=2)</span><br><span class=\"line\">pca.fit(scaled_data)</span><br><span class=\"line\">x_pca = pca.transform(scaled_data)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(8, 6))</span><br><span class=\"line\">plt.scatter(x_pca[:,0], x_pca[:, 1], c=cancer[&apos;target&apos;], cmap=&apos;plasma&apos;)</span><br><span class=\"line\">plt.xlabel(&apos;First Principle Component&apos;)</span><br><span class=\"line\">plt.ylabel(&apos;Second Principle Component&apos;)</span><br></pre></td></tr></table></figure>\n<h4><span id=\"数学理论\"> 数学理论</span></h4>\n<ul>\n<li>矩阵换基底<br>\n坐标变换地目标是，找到一组新的正交单位向量，替换原来的正交单位向量。</li>\n<li>拉格朗日乘子法<br>\n拉格朗日乘子法主要提供了一种求解函数在约束条件下极值的方法。</li>\n<li>协方差矩阵<br>\n协方差研究的目的是变量（特征）之间的关系<br>\n每个样本点到样本中心距离的平方和的平均 = 样本各个特征方差和（自身协方差），样本的方差</li>\n<li>特征向量</li>\n<li>奇异值分解<br>\n奇异值分解（svd: singular value decomposition）定义：对于任意的矩阵A，存在：</li>\n<li>特征向量和奇异值分解的关系</li>\n</ul>\n<h4><span id=\"参考文章\"> 参考文章</span></h4>\n<p><a href=\"https://zhuanlan.zhihu.com/p/55297233\" target=\"_blank\" rel=\"noopener\">详细推导PCA算法</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<ul>\n<li>多重共线性 multi-collinearity<br>\n多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。</li>\n<li>PCA<br>\n通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量</li>\n<li>降维<br>\n就是找到k个超平面，把原n维空间的数据投影到这k个超平面上，使原n维数据降为k维。</li>\n<li>PCA核心思想<br>\n降维后的数据尽可能分散</li>\n<li>PCA的目的<br>\n是找到一个投影矩阵，使得样本点投影后尽可能分散，即使得样本点投影后的方差最大，并且为了避免投影的堆叠，投影矩阵中的各个向量应该是正交的。</li>\n<li>中心化</li>\n</ul>\n<h4 id=\"pca过程总结\"><a class=\"markdownIt-Anchor\" href=\"#pca过程总结\"></a> PCA过程总结</h4>\n<ol>\n<li>初始化X，使得所有样本之间的特征值均值为0，同时应用feature scaling，缩放到-0.5～0.5</li>\n<li>计算X的协方差矩阵S</li>\n<li>对S进行SVD分解，U即我们要求的新坐标系集合， Σ \\SigmaΣ 为特征值集合（计算时特征值都会大于0，且结果会从小到大排列)</li>\n<li>按照特征值从大到小排序，要降低为k维，那么取前k个特征值对应的特征向量，就是新的k个坐标轴</li>\n<li>把X映射到新的坐标系中，完整降维操作</li>\n</ol>\n<h4 id=\"主成份数量的选择\"><a class=\"markdownIt-Anchor\" href=\"#主成份数量的选择\"></a> 主成份数量的选择</h4>\n<p>PCA使得数据从n维降低为k维度。<br>\n一般选择标准为：投影前后方差比例值，作为k值的选择标准。</p>\n<h4 id=\"scikit-learn\"><a class=\"markdownIt-Anchor\" href=\"#scikit-learn\"></a> Scikit Learn</h4>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import matplotlib.pyplot as plt</span><br><span class=\"line\">import pandas as pd</span><br><span class=\"line\">import numpy as np</span><br><span class=\"line\">import seaborn as sns</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\">sns.set_style(&apos;whitegrid&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\">from sklearn.datasets import load_breast_cancer</span><br><span class=\"line\">from sklearn.preprocessing import StandardScaler</span><br><span class=\"line\">from sklearn.decomposition import PCA</span><br><span class=\"line\"></span><br><span class=\"line\">cancer = load_breast_cancer()</span><br><span class=\"line\">df = pd.DataFrame(cancer[&apos;data&apos;], columns=cancer[&apos;feature_names&apos;])</span><br><span class=\"line\"></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">scaler.fit(df)</span><br><span class=\"line\">scaled_data = scaler.transform(df)</span><br><span class=\"line\"></span><br><span class=\"line\">pca = PCA(n_components=2)</span><br><span class=\"line\">pca.fit(scaled_data)</span><br><span class=\"line\">x_pca = pca.transform(scaled_data)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.figure(figsize=(8, 6))</span><br><span class=\"line\">plt.scatter(x_pca[:,0], x_pca[:, 1], c=cancer[&apos;target&apos;], cmap=&apos;plasma&apos;)</span><br><span class=\"line\">plt.xlabel(&apos;First Principle Component&apos;)</span><br><span class=\"line\">plt.ylabel(&apos;Second Principle Component&apos;)</span><br></pre></td></tr></table></figure>\n<h4 id=\"数学理论\"><a class=\"markdownIt-Anchor\" href=\"#数学理论\"></a> 数学理论</h4>\n<ul>\n<li>矩阵换基底<br>\n坐标变换地目标是，找到一组新的正交单位向量，替换原来的正交单位向量。</li>\n<li>拉格朗日乘子法<br>\n拉格朗日乘子法主要提供了一种求解函数在约束条件下极值的方法。</li>\n<li>协方差矩阵<br>\n协方差研究的目的是变量（特征）之间的关系<br>\n每个样本点到样本中心距离的平方和的平均 = 样本各个特征方差和（自身协方差），样本的方差</li>\n<li>特征向量</li>\n<li>奇异值分解<br>\n奇异值分解（svd: singular value decomposition）定义：对于任意的矩阵A，存在：</li>\n<li>特征向量和奇异值分解的关系</li>\n</ul>\n<h4 id=\"参考文章\"><a class=\"markdownIt-Anchor\" href=\"#参考文章\"></a> 参考文章</h4>\n<p><a href=\"https://zhuanlan.zhihu.com/p/55297233\" target=\"_blank\" rel=\"noopener\">详细推导PCA算法</a></p>\n"},{"title":"自然语言处理，NLP","catalog":true,"toc_nav_num":true,"math":true,"date":"2019-08-13T10:30:24.000Z","subtitle":null,"header-img":"/img/article_header/article_header.png","catagories":["ML"],"_content":"\n#### 概念\n自然语言处理是计算机处理人类语言的一门技术：\n  - **句法语义分析**：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n  - **信息抽取**：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n  - **文本挖掘（或者文本数据挖掘）**：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n  - **机器翻译**：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n  - **信息检索**：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n  - **问答系统**： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案.\n  - **对话系统**：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复.\n\n#### 算法\n  - 朴素贝叶斯\n\n#### 练习\n##### 工具\n  - scikitLearn\n  - [nltk](https://www.nltk.org/data.html)\n##### 数据\n  - [UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)\n\n##### 步骤\n1. 安装 scikitLearn\n~~~\npip3 install -U scikit-learn\n~~~\n2. 安装 nltk\n~~~\npip3 install -U nltk\n~~~\n3. 下载stopwords包\n~~~\npython3 -m nltk.downloader -d ./venv/share/nltk_data stopwords\n~~~\n4. 下载数据\n  到[UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)下载数据，解压文件夹放在项目的根目录\n\n5. 处理数据 & 训练模型\n  - 1. 文本预处理\n    我们的数据的主要问题是它都是文本格式（字符串）。 到目前为止我们学到的分类算法需要某种数值特征向量才能执行分类任务。 实际上有很多方法可以将语料库转换为矢量格式。 最简单的是词袋方法，bag-of-words, 即文本中的每个唯一词将由一个数字表示。 我们将原始消息（字符序列）转换为向量（数字序列）\n  - 2. 规范化文本\n    有很多方法可以范化， 例如Stemming。 NLTK有很多内置工具和方法规范化文本。因为许多人倾向于使用缩写或简写的方式，有时这些方法不是很有用。\n  - 3. 矢量化，使用词袋模型分三步完成：\n    - 计算每个消息中出现一个term的次数（称为term频率）\n    - 权重计数，以便频繁的令牌获得较低的权重（逆文档频率）\n    - 将向量标准化为单位长度，从原始文本长度（L2标准）中抽象出来\n\n~~~\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nmessages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t', names=['label', 'message'])\nmessages['length'] = messages['message'].apply(len)\n\ndef text_process(mess):\n  '''\n  Takes in a string of text, then performs the following:\n  1. Remove all punctuation\n  2. Remove all stopwords\n  3. Returns a list of the cleaned text\n  '''\n  # Check characters to see if they are in punctuation\n  nopunc = [char for char in mess if char not in string.punctuation]\n\n  # Join the characters again to form the string.\n  nopunc = ''.join(nopunc)\n\n  # Now just remove any stopwords\n  return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n\nmsg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'])\n\npipeline = Pipeline([\n  ('bow', CountVectorizer(analyzer=(text_process))),\n  ('tfidf', TfidfTransformer()),\n  ('classifier', MultinomialNB())\n  ])\n\npipeline.fit(msg_train, label_train)\npredictions = pipeline.predict(msg_test)\nprint(classification_report(label_test, predictions))\nprint(confusion_matrix(label_test, predictions))\n~~~\n#### 什么是TF-IDF？\n- TF-IDF是term frequency-inverse document frequency的缩写\n- TF: Term Frequency，衡量一个term在文档中出现的频率。 由于每个文档的长度不同，因此长文档中的term可能比较短的文档出现的次数多得多。 因此，术语频率通常除以文档长度（也就是文档中的术语总数）作为标准化的方式：\n$$\nTF(t) = \\frac{Number of times term t appears in a document}{Total number of terms in the document}\n$$\n- IDF: Inverse Document Frequency，衡量一个term的重要性。 在计算TF时，所有term都被视为同等重要。 然而，众所周知，某些term，例如 \"is\", \"of\", 以及 \"that\"，可能会出现很多次但不重要。通过计算以下内容，扩大罕见词的大小：\n  $$\n  IDF(t) = log_e(\\frac{Total number of documents}{Number of documents with term t in it})\n  $$\n#### 参考\n[微软亚洲研究院 周明博士](https://www.zhihu.com/question/19895141/answer/149475410)","source":"_posts/2019-08-14-自然语言处理.md","raw":"---\ntitle: \"自然语言处理，NLP\"\ncatalog: true\ntoc_nav_num: true\nmath: true\ndate: 2019-08-13 18:30:24\nsubtitle:\nheader-img: \"/img/article_header/article_header.png\"\ntags:\n- 应用\ncatagories:\n- ML\n---\n\n#### 概念\n自然语言处理是计算机处理人类语言的一门技术：\n  - **句法语义分析**：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。\n  - **信息抽取**：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。\n  - **文本挖掘（或者文本数据挖掘）**：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。\n  - **机器翻译**：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。\n  - **信息检索**：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。\n  - **问答系统**： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案.\n  - **对话系统**：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复.\n\n#### 算法\n  - 朴素贝叶斯\n\n#### 练习\n##### 工具\n  - scikitLearn\n  - [nltk](https://www.nltk.org/data.html)\n##### 数据\n  - [UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)\n\n##### 步骤\n1. 安装 scikitLearn\n~~~\npip3 install -U scikit-learn\n~~~\n2. 安装 nltk\n~~~\npip3 install -U nltk\n~~~\n3. 下载stopwords包\n~~~\npython3 -m nltk.downloader -d ./venv/share/nltk_data stopwords\n~~~\n4. 下载数据\n  到[UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)下载数据，解压文件夹放在项目的根目录\n\n5. 处理数据 & 训练模型\n  - 1. 文本预处理\n    我们的数据的主要问题是它都是文本格式（字符串）。 到目前为止我们学到的分类算法需要某种数值特征向量才能执行分类任务。 实际上有很多方法可以将语料库转换为矢量格式。 最简单的是词袋方法，bag-of-words, 即文本中的每个唯一词将由一个数字表示。 我们将原始消息（字符序列）转换为向量（数字序列）\n  - 2. 规范化文本\n    有很多方法可以范化， 例如Stemming。 NLTK有很多内置工具和方法规范化文本。因为许多人倾向于使用缩写或简写的方式，有时这些方法不是很有用。\n  - 3. 矢量化，使用词袋模型分三步完成：\n    - 计算每个消息中出现一个term的次数（称为term频率）\n    - 权重计数，以便频繁的令牌获得较低的权重（逆文档频率）\n    - 将向量标准化为单位长度，从原始文本长度（L2标准）中抽象出来\n\n~~~\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nmessages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t', names=['label', 'message'])\nmessages['length'] = messages['message'].apply(len)\n\ndef text_process(mess):\n  '''\n  Takes in a string of text, then performs the following:\n  1. Remove all punctuation\n  2. Remove all stopwords\n  3. Returns a list of the cleaned text\n  '''\n  # Check characters to see if they are in punctuation\n  nopunc = [char for char in mess if char not in string.punctuation]\n\n  # Join the characters again to form the string.\n  nopunc = ''.join(nopunc)\n\n  # Now just remove any stopwords\n  return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n\nmsg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'])\n\npipeline = Pipeline([\n  ('bow', CountVectorizer(analyzer=(text_process))),\n  ('tfidf', TfidfTransformer()),\n  ('classifier', MultinomialNB())\n  ])\n\npipeline.fit(msg_train, label_train)\npredictions = pipeline.predict(msg_test)\nprint(classification_report(label_test, predictions))\nprint(confusion_matrix(label_test, predictions))\n~~~\n#### 什么是TF-IDF？\n- TF-IDF是term frequency-inverse document frequency的缩写\n- TF: Term Frequency，衡量一个term在文档中出现的频率。 由于每个文档的长度不同，因此长文档中的term可能比较短的文档出现的次数多得多。 因此，术语频率通常除以文档长度（也就是文档中的术语总数）作为标准化的方式：\n$$\nTF(t) = \\frac{Number of times term t appears in a document}{Total number of terms in the document}\n$$\n- IDF: Inverse Document Frequency，衡量一个term的重要性。 在计算TF时，所有term都被视为同等重要。 然而，众所周知，某些term，例如 \"is\", \"of\", 以及 \"that\"，可能会出现很多次但不重要。通过计算以下内容，扩大罕见词的大小：\n  $$\n  IDF(t) = log_e(\\frac{Total number of documents}{Number of documents with term t in it})\n  $$\n#### 参考\n[微软亚洲研究院 周明博士](https://www.zhihu.com/question/19895141/answer/149475410)","slug":"2019-08-14-自然语言处理","published":1,"updated":"2019-08-13T07:21:32.331Z","_id":"cjz89mp3e000cz2ovdb1deanh","comments":1,"layout":"post","photos":[],"link":"","content":"<h4><span id=\"概念\"> 概念</span></h4>\n<p>自然语言处理是计算机处理人类语言的一门技术：</p>\n<ul>\n<li><strong>句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</li>\n<li><strong>信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。</li>\n<li><strong>文本挖掘（或者文本数据挖掘）</strong>：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。</li>\n<li><strong>机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。</li>\n<li><strong>信息检索</strong>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。</li>\n<li><strong>问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案.</li>\n<li><strong>对话系统</strong>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复.</li>\n</ul>\n<h4><span id=\"算法\"> 算法</span></h4>\n<ul>\n<li>朴素贝叶斯</li>\n</ul>\n<h4><span id=\"练习\"> 练习</span></h4>\n<h5><span id=\"工具\"> 工具</span></h5>\n<ul>\n<li>scikitLearn</li>\n<li><a href=\"https://www.nltk.org/data.html\" target=\"_blank\" rel=\"noopener\">nltk</a></li>\n</ul>\n<h5><span id=\"数据\"> 数据</span></h5>\n<ul>\n<li><a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\" target=\"_blank\" rel=\"noopener\">UCI SMS Spam Collection Data Set</a></li>\n</ul>\n<h5><span id=\"步骤\"> 步骤</span></h5>\n<ol>\n<li>安装 scikitLearn</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install -U scikit-learn</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>安装 nltk</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install -U nltk</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>下载stopwords包</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m nltk.downloader -d ./venv/share/nltk_data stopwords</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>\n<p>下载数据<br>\n到<a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\" target=\"_blank\" rel=\"noopener\">UCI SMS Spam Collection Data Set</a>下载数据，解压文件夹放在项目的根目录</p>\n</li>\n<li>\n<p>处理数据 &amp; 训练模型</p>\n</li>\n</ol>\n<ul>\n<li>\n<ol>\n<li>文本预处理<br>\n我们的数据的主要问题是它都是文本格式（字符串）。 到目前为止我们学到的分类算法需要某种数值特征向量才能执行分类任务。 实际上有很多方法可以将语料库转换为矢量格式。 最简单的是词袋方法，bag-of-words, 即文本中的每个唯一词将由一个数字表示。 我们将原始消息（字符序列）转换为向量（数字序列）</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>规范化文本<br>\n有很多方法可以范化， 例如Stemming。 NLTK有很多内置工具和方法规范化文本。因为许多人倾向于使用缩写或简写的方式，有时这些方法不是很有用。</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>矢量化，使用词袋模型分三步完成：</li>\n</ol>\n<ul>\n<li>计算每个消息中出现一个term的次数（称为term频率）</li>\n<li>权重计数，以便频繁的令牌获得较低的权重（逆文档频率）</li>\n<li>将向量标准化为单位长度，从原始文本长度（L2标准）中抽象出来</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class=\"line\">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class=\"line\">from sklearn.naive_bayes import MultinomialNB</span><br><span class=\"line\">from sklearn.pipeline import Pipeline</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">messages = pd.read_csv(&apos;smsspamcollection/SMSSpamCollection&apos;, sep=&apos;\\t&apos;, names=[&apos;label&apos;, &apos;message&apos;])</span><br><span class=\"line\">messages[&apos;length&apos;] = messages[&apos;message&apos;].apply(len)</span><br><span class=\"line\"></span><br><span class=\"line\">def text_process(mess):</span><br><span class=\"line\">  &apos;&apos;&apos;</span><br><span class=\"line\">  Takes in a string of text, then performs the following:</span><br><span class=\"line\">  1. Remove all punctuation</span><br><span class=\"line\">  2. Remove all stopwords</span><br><span class=\"line\">  3. Returns a list of the cleaned text</span><br><span class=\"line\">  &apos;&apos;&apos;</span><br><span class=\"line\">  # Check characters to see if they are in punctuation</span><br><span class=\"line\">  nopunc = [char for char in mess if char not in string.punctuation]</span><br><span class=\"line\"></span><br><span class=\"line\">  # Join the characters again to form the string.</span><br><span class=\"line\">  nopunc = &apos;&apos;.join(nopunc)</span><br><span class=\"line\"></span><br><span class=\"line\">  # Now just remove any stopwords</span><br><span class=\"line\">  return [word for word in nopunc.split() if word.lower() not in stopwords.words(&apos;english&apos;)]</span><br><span class=\"line\"></span><br><span class=\"line\">msg_train, msg_test, label_train, label_test = train_test_split(messages[&apos;message&apos;], messages[&apos;label&apos;])</span><br><span class=\"line\"></span><br><span class=\"line\">pipeline = Pipeline([</span><br><span class=\"line\">  (&apos;bow&apos;, CountVectorizer(analyzer=(text_process))),</span><br><span class=\"line\">  (&apos;tfidf&apos;, TfidfTransformer()),</span><br><span class=\"line\">  (&apos;classifier&apos;, MultinomialNB())</span><br><span class=\"line\">  ])</span><br><span class=\"line\"></span><br><span class=\"line\">pipeline.fit(msg_train, label_train)</span><br><span class=\"line\">predictions = pipeline.predict(msg_test)</span><br><span class=\"line\">print(classification_report(label_test, predictions))</span><br><span class=\"line\">print(confusion_matrix(label_test, predictions))</span><br></pre></td></tr></table></figure>\n<h4><span id=\"什么是tf-idf\"> 什么是TF-IDF？</span></h4>\n<ul>\n<li>TF-IDF是term frequency-inverse document frequency的缩写</li>\n<li>TF: Term Frequency，衡量一个term在文档中出现的频率。 由于每个文档的长度不同，因此长文档中的term可能比较短的文档出现的次数多得多。 因此，术语频率通常除以文档长度（也就是文档中的术语总数）作为标准化的方式：</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>t</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">TF(t) = \\frac{Number of times term t appears in a document}{Total number of terms in the document}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.37144em;\"></span><span class=\"strut bottom\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathit\">t</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.6860000000000002em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">h</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">p</span><span class=\"mord mathit\">p</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<ul>\n<li>IDF: Inverse Document Frequency，衡量一个term的重要性。 在计算TF时，所有term都被视为同等重要。 然而，众所周知，某些term，例如 “is”, “of”, 以及 “that”，可能会出现很多次但不重要。通过计算以下内容，扩大罕见词的大小：<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mo>(</mo><mfrac><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>t</mi></mrow></mfrac><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">IDF(t) = log_e(\\frac{Total number of documents}{Number of documents with term t in it})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.37144em;\"></span><span class=\"strut bottom\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathit\">t</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">e</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.6860000000000002em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">h</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">t</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">s</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n</ul>\n<h4><span id=\"参考\"> 参考</span></h4>\n<p><a href=\"https://www.zhihu.com/question/19895141/answer/149475410\" target=\"_blank\" rel=\"noopener\">微软亚洲研究院 周明博士</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"概念\"><a class=\"markdownIt-Anchor\" href=\"#概念\"></a> 概念</h4>\n<p>自然语言处理是计算机处理人类语言的一门技术：</p>\n<ul>\n<li><strong>句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</li>\n<li><strong>信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。</li>\n<li><strong>文本挖掘（或者文本数据挖掘）</strong>：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。</li>\n<li><strong>机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。</li>\n<li><strong>信息检索</strong>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。</li>\n<li><strong>问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案.</li>\n<li><strong>对话系统</strong>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复.</li>\n</ul>\n<h4 id=\"算法\"><a class=\"markdownIt-Anchor\" href=\"#算法\"></a> 算法</h4>\n<ul>\n<li>朴素贝叶斯</li>\n</ul>\n<h4 id=\"练习\"><a class=\"markdownIt-Anchor\" href=\"#练习\"></a> 练习</h4>\n<h5 id=\"工具\"><a class=\"markdownIt-Anchor\" href=\"#工具\"></a> 工具</h5>\n<ul>\n<li>scikitLearn</li>\n<li><a href=\"https://www.nltk.org/data.html\" target=\"_blank\" rel=\"noopener\">nltk</a></li>\n</ul>\n<h5 id=\"数据\"><a class=\"markdownIt-Anchor\" href=\"#数据\"></a> 数据</h5>\n<ul>\n<li><a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\" target=\"_blank\" rel=\"noopener\">UCI SMS Spam Collection Data Set</a></li>\n</ul>\n<h5 id=\"步骤\"><a class=\"markdownIt-Anchor\" href=\"#步骤\"></a> 步骤</h5>\n<ol>\n<li>安装 scikitLearn</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install -U scikit-learn</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>安装 nltk</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip3 install -U nltk</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>下载stopwords包</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python3 -m nltk.downloader -d ./venv/share/nltk_data stopwords</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>\n<p>下载数据<br>\n到<a href=\"https://archive.ics.uci.edu/ml/datasets/sms+spam+collection\" target=\"_blank\" rel=\"noopener\">UCI SMS Spam Collection Data Set</a>下载数据，解压文件夹放在项目的根目录</p>\n</li>\n<li>\n<p>处理数据 &amp; 训练模型</p>\n</li>\n</ol>\n<ul>\n<li>\n<ol>\n<li>文本预处理<br>\n我们的数据的主要问题是它都是文本格式（字符串）。 到目前为止我们学到的分类算法需要某种数值特征向量才能执行分类任务。 实际上有很多方法可以将语料库转换为矢量格式。 最简单的是词袋方法，bag-of-words, 即文本中的每个唯一词将由一个数字表示。 我们将原始消息（字符序列）转换为向量（数字序列）</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>规范化文本<br>\n有很多方法可以范化， 例如Stemming。 NLTK有很多内置工具和方法规范化文本。因为许多人倾向于使用缩写或简写的方式，有时这些方法不是很有用。</li>\n</ol>\n</li>\n<li>\n<ol start=\"3\">\n<li>矢量化，使用词袋模型分三步完成：</li>\n</ol>\n<ul>\n<li>计算每个消息中出现一个term的次数（称为term频率）</li>\n<li>权重计数，以便频繁的令牌获得较低的权重（逆文档频率）</li>\n<li>将向量标准化为单位长度，从原始文本长度（L2标准）中抽象出来</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import pandas as pd</span><br><span class=\"line\">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class=\"line\">from sklearn.feature_extraction.text import TfidfTransformer</span><br><span class=\"line\">from sklearn.naive_bayes import MultinomialNB</span><br><span class=\"line\">from sklearn.pipeline import Pipeline</span><br><span class=\"line\">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class=\"line\"></span><br><span class=\"line\">messages = pd.read_csv(&apos;smsspamcollection/SMSSpamCollection&apos;, sep=&apos;\\t&apos;, names=[&apos;label&apos;, &apos;message&apos;])</span><br><span class=\"line\">messages[&apos;length&apos;] = messages[&apos;message&apos;].apply(len)</span><br><span class=\"line\"></span><br><span class=\"line\">def text_process(mess):</span><br><span class=\"line\">  &apos;&apos;&apos;</span><br><span class=\"line\">  Takes in a string of text, then performs the following:</span><br><span class=\"line\">  1. Remove all punctuation</span><br><span class=\"line\">  2. Remove all stopwords</span><br><span class=\"line\">  3. Returns a list of the cleaned text</span><br><span class=\"line\">  &apos;&apos;&apos;</span><br><span class=\"line\">  # Check characters to see if they are in punctuation</span><br><span class=\"line\">  nopunc = [char for char in mess if char not in string.punctuation]</span><br><span class=\"line\"></span><br><span class=\"line\">  # Join the characters again to form the string.</span><br><span class=\"line\">  nopunc = &apos;&apos;.join(nopunc)</span><br><span class=\"line\"></span><br><span class=\"line\">  # Now just remove any stopwords</span><br><span class=\"line\">  return [word for word in nopunc.split() if word.lower() not in stopwords.words(&apos;english&apos;)]</span><br><span class=\"line\"></span><br><span class=\"line\">msg_train, msg_test, label_train, label_test = train_test_split(messages[&apos;message&apos;], messages[&apos;label&apos;])</span><br><span class=\"line\"></span><br><span class=\"line\">pipeline = Pipeline([</span><br><span class=\"line\">  (&apos;bow&apos;, CountVectorizer(analyzer=(text_process))),</span><br><span class=\"line\">  (&apos;tfidf&apos;, TfidfTransformer()),</span><br><span class=\"line\">  (&apos;classifier&apos;, MultinomialNB())</span><br><span class=\"line\">  ])</span><br><span class=\"line\"></span><br><span class=\"line\">pipeline.fit(msg_train, label_train)</span><br><span class=\"line\">predictions = pipeline.predict(msg_test)</span><br><span class=\"line\">print(classification_report(label_test, predictions))</span><br><span class=\"line\">print(confusion_matrix(label_test, predictions))</span><br></pre></td></tr></table></figure>\n<h4 id=\"什么是tf-idf\"><a class=\"markdownIt-Anchor\" href=\"#什么是tf-idf\"></a> 什么是TF-IDF？</h4>\n<ul>\n<li>TF-IDF是term frequency-inverse document frequency的缩写</li>\n<li>TF: Term Frequency，衡量一个term在文档中出现的频率。 由于每个文档的长度不同，因此长文档中的term可能比较短的文档出现的次数多得多。 因此，术语频率通常除以文档长度（也就是文档中的术语总数）作为标准化的方式：</li>\n</ul>\n<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>t</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>a</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">TF(t) = \\frac{Number of times term t appears in a document}{Total number of terms in the document}\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.37144em;\"></span><span class=\"strut bottom\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathit\">t</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.6860000000000002em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">h</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">p</span><span class=\"mord mathit\">p</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span></span></span></span></span></p>\n<ul>\n<li>IDF: Inverse Document Frequency，衡量一个term的重要性。 在计算TF时，所有term都被视为同等重要。 然而，众所周知，某些term，例如 “is”, “of”, 以及 “that”，可能会出现很多次但不重要。通过计算以下内容，扩大罕见词的大小：<p><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><msub><mi>g</mi><mi>e</mi></msub><mo>(</mo><mfrac><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi></mrow><mrow><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>s</mi><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>m</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>i</mi><mi>t</mi></mrow></mfrac><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">IDF(t) = log_e(\\frac{Total number of documents}{Number of documents with term t in it})\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"strut\" style=\"height:1.37144em;\"></span><span class=\"strut bottom\" style=\"height:2.25188em;vertical-align:-0.8804400000000001em;\"></span><span class=\"base displaystyle textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.07847em;\">I</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">D</span><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">F</span><span class=\"mopen\">(</span><span class=\"mord mathit\">t</span><span class=\"mclose\">)</span><span class=\"mrel\">=</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">o</span><span class=\"mord\"><span class=\"mord mathit\" style=\"margin-right:0.03588em;\">g</span><span class=\"vlist\"><span style=\"top:0.15em;margin-right:0.05em;margin-left:-0.03588em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle scriptstyle cramped\"><span class=\"mord mathit\">e</span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"mopen\">(</span><span class=\"mord reset-textstyle displaystyle textstyle uncramped\"><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist\"><span style=\"top:0.6860000000000002em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle cramped\"><span class=\"mord textstyle cramped\"><span class=\"mord mathit\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">s</span><span class=\"mord mathit\" style=\"margin-right:0.02691em;\">w</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">h</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">i</span><span class=\"mord mathit\">t</span></span></span></span><span style=\"top:-0.22999999999999998em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped frac-line\"></span></span><span style=\"top:-0.677em;\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span><span class=\"reset-textstyle textstyle uncramped\"><span class=\"mord textstyle uncramped\"><span class=\"mord mathit\" style=\"margin-right:0.13889em;\">T</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">a</span><span class=\"mord mathit\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">b</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\" style=\"margin-right:0.10764em;\">f</span><span class=\"mord mathit\">d</span><span class=\"mord mathit\">o</span><span class=\"mord mathit\">c</span><span class=\"mord mathit\">u</span><span class=\"mord mathit\">m</span><span class=\"mord mathit\">e</span><span class=\"mord mathit\">n</span><span class=\"mord mathit\">t</span><span class=\"mord mathit\">s</span></span></span></span><span class=\"baseline-fix\"><span class=\"fontsize-ensurer reset-size5 size5\"><span style=\"font-size:0em;\">​</span></span>​</span></span></span><span class=\"sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter\"></span></span><span class=\"mclose\">)</span></span></span></span></span></p>\n</li>\n</ul>\n<h4 id=\"参考\"><a class=\"markdownIt-Anchor\" href=\"#参考\"></a> 参考</h4>\n<p><a href=\"https://www.zhihu.com/question/19895141/answer/149475410\" target=\"_blank\" rel=\"noopener\">微软亚洲研究院 周明博士</a></p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjz3pnqex0001z0ovrrwvjtz6","tag_id":"cjz3pnqf40005z0ovfb4h90he","_id":"cjz3pnqfc000bz0ovwpzcp4j7"},{"post_id":"cjz3pnqf10003z0ovvrf1w10g","tag_id":"cjz3pnqfb000az0ovz855drqb","_id":"cjz3pnqfi000gz0ovowgnfi5q"},{"post_id":"cjz3pnqf50006z0ov3ik1r9d9","tag_id":"cjz3pnqfg000ez0ovh43ru3zc","_id":"cjz3pnqfk000kz0ov5t8cyeic"},{"post_id":"cjz3pnqf80008z0ov513xohfn","tag_id":"cjz3pnqfj000iz0ovwkiajl4a","_id":"cjz3pnqfn000oz0ovul2lbhkj"},{"post_id":"cjz3pnqfa0009z0ovp1u8d9tw","tag_id":"cjz3pnqfl000mz0ov3tmdeboq","_id":"cjz3pnqfn000qz0ovegzqpbob"},{"post_id":"cjz3pnqfc000cz0ovzv9xe34x","tag_id":"cjz3pnqfn000pz0ovgyp2lh5j","_id":"cjz3pnqfo000sz0ov2zl2fqbj"},{"post_id":"cjz3pnqff000dz0ovnj5493xa","tag_id":"cjz3pnqfn000rz0ov0jloa7p6","_id":"cjz3pnqfo000uz0ovljiyp80r"},{"post_id":"cjz3pnqfh000fz0ov19lgbl59","tag_id":"cjz3pnqfo000tz0ovs1obkrb0","_id":"cjz3pnqfo000wz0ovasj2wm8o"},{"post_id":"cjz3pnqfi000hz0ov7bva1k7x","tag_id":"cjz3pnqfo000vz0ov045boobj","_id":"cjz3pnqfq000yz0ovxjj2beke"},{"post_id":"cjz3pnqfk000jz0ov0g5q3rrw","tag_id":"cjz3pnqfo000vz0ov045boobj","_id":"cjz3pnqfq0010z0ovqneipsg0"},{"post_id":"cjz3pnqfm000nz0ovqsryc9ek","tag_id":"cjz3pnqfo000vz0ov045boobj","_id":"cjz3pnqfr0013z0oveyqp4eic"},{"post_id":"cjz3rrdb10014z0ovl76ycqdt","tag_id":"cjz3pnqfo000vz0ov045boobj","_id":"cjz3rs24f0015z0ovgqsm4iru"},{"post_id":"cjz87p1ca0000z2ovakimkter","tag_id":"cjz3pnqfo000vz0ov045boobj","_id":"cjz87p1ck0002z2ovkqndt1a7"},{"post_id":"cjz87p1ck0003z2ovuu5qsupz","tag_id":"cjz3pnqfo000vz0ov045boobj","_id":"cjz87p1cq0005z2ovp4hoox2u"},{"post_id":"cjz87xsza0008z2ovewboc383","tag_id":"cjz87p1cl0004z2ovpah3yzib","_id":"cjz87xszc0009z2ovizrzzxe8"},{"post_id":"cjz87xn610007z2ov0sa4omo3","tag_id":"cjz88kuhz000az2ovk7gyan31","_id":"cjz88kui0000bz2ovardo2mfx"},{"post_id":"cjz89mp3e000cz2ovdb1deanh","tag_id":"cjz88kuhz000az2ovk7gyan31","_id":"cjz89n7a9000dz2ovvvz63c12"}],"Tag":[{"name":"JavaScript","_id":"cjz3pnqf40005z0ovfb4h90he"},{"name":"Machine Learning","_id":"cjz3pnqfb000az0ovz855drqb"},{"name":"优化方法","_id":"cjz3pnqfg000ez0ovh43ru3zc"},{"name":"特征工程","_id":"cjz3pnqfj000iz0ovwkiajl4a"},{"name":"学习计划","_id":"cjz3pnqfl000mz0ov3tmdeboq"},{"name":"前端，数据科学","_id":"cjz3pnqfn000pz0ovgyp2lh5j"},{"name":"回归算法","_id":"cjz3pnqfn000rz0ov0jloa7p6"},{"name":"模型选择","_id":"cjz3pnqfo000tz0ovs1obkrb0"},{"name":"分类算法","_id":"cjz3pnqfo000vz0ov045boobj"},{"name":"数据清洗","_id":"cjz3pnqfq000zz0ovjvbdjvr4"},{"name":"数据降维","_id":"cjz87p1cl0004z2ovpah3yzib"},{"name":"应用","_id":"cjz88kuhz000az2ovk7gyan31"}]}}