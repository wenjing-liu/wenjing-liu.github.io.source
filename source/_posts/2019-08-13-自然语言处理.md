---
title: "自然语言处理，NLP"
catalog: true
toc_nav_num: true
math: true
date: 2019-08-13 18:30:24
subtitle:
header-img: "/img/article_header/article_header.png"
tags:
- 应用
catagories:
- ML
---

#### 概念
自然语言处理是计算机处理人类语言的一门技术：
  - **句法语义分析**：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。
  - **信息抽取**：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。
  - **文本挖掘（或者文本数据挖掘）**：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。
  - **机器翻译**：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。
  - **信息检索**：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。
  - **问答系统**： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案.
  - **对话系统**：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复.

#### 算法
  - 朴素贝叶斯

#### 练习
##### 工具
  - scikitLearn
  - [nltk](https://www.nltk.org/data.html)
##### 数据
  - [UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)

##### 步骤
1. 安装 scikitLearn
~~~
pip3 install -U scikit-learn
~~~
2. 安装 nltk
~~~
pip3 install -U nltk
~~~
3. 下载stopwords包
~~~
python3 -m nltk.downloader -d ./venv/share/nltk_data stopwords
~~~
4. 下载数据
  到[UCI SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)下载数据，解压文件夹放在项目的根目录

5. 处理数据 & 训练模型
  - 1. 文本预处理
    我们的数据的主要问题是它都是文本格式（字符串）。 到目前为止我们学到的分类算法需要某种数值特征向量才能执行分类任务。 实际上有很多方法可以将语料库转换为矢量格式。 最简单的是词袋方法，bag-of-words, 即文本中的每个唯一词将由一个数字表示。 我们将原始消息（字符序列）转换为向量（数字序列）
  - 2. 规范化文本
    有很多方法可以范化， 例如Stemming。 NLTK有很多内置工具和方法规范化文本。因为许多人倾向于使用缩写或简写的方式，有时这些方法不是很有用。
  - 3. 矢量化，使用词袋模型分三步完成：
    - 计算每个消息中出现一个term的次数（称为term频率）
    - 权重计数，以便频繁的令牌获得较低的权重（逆文档频率）
    - 将向量标准化为单位长度，从原始文本长度（L2标准）中抽象出来

~~~
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix

messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\t', names=['label', 'message'])
messages['length'] = messages['message'].apply(len)

def text_process(mess):
  '''
  Takes in a string of text, then performs the following:
  1. Remove all punctuation
  2. Remove all stopwords
  3. Returns a list of the cleaned text
  '''
  # Check characters to see if they are in punctuation
  nopunc = [char for char in mess if char not in string.punctuation]

  # Join the characters again to form the string.
  nopunc = ''.join(nopunc)

  # Now just remove any stopwords
  return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'])

pipeline = Pipeline([
  ('bow', CountVectorizer(analyzer=(text_process))),
  ('tfidf', TfidfTransformer()),
  ('classifier', MultinomialNB())
  ])

pipeline.fit(msg_train, label_train)
predictions = pipeline.predict(msg_test)
print(classification_report(label_test, predictions))
print(confusion_matrix(label_test, predictions))
~~~
#### 什么是TF-IDF？
- TF-IDF是term frequency-inverse document frequency的缩写
- TF: Term Frequency，衡量一个term在文档中出现的频率。 由于每个文档的长度不同，因此长文档中的term可能比较短的文档出现的次数多得多。 因此，术语频率通常除以文档长度（也就是文档中的术语总数）作为标准化的方式：
$$
TF(t) = \frac{Number of times term t appears in a document}{Total number of terms in the document}
$$
- IDF: Inverse Document Frequency，衡量一个term的重要性。 在计算TF时，所有term都被视为同等重要。 然而，众所周知，某些term，例如 "is", "of", 以及 "that"，可能会出现很多次但不重要。通过计算以下内容，扩大罕见词的大小：
  $$
  IDF(t) = log_e(\frac{Total number of documents}{Number of documents with term t in it})
  $$
#### 参考
[微软亚洲研究院 周明博士](https://www.zhihu.com/question/19895141/answer/149475410)