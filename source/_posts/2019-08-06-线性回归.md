---
title: "线性回归 Linear Regression"
catalog: true
toc_nav_num: true
math: true
date: 2019-08-06 18:30:24
subtitle:
header-img: "/img/article_header/article_header.png"
tags:
- 回归算法
catagories:
- ML 算法

---

#### 概念
回归是为了预测数值型的目标值。
线性回归假设特征和结果之间满足线性关系。
求解回归方程的回归系数的过程就是回归。
回归系数是一个向量，输入也是向量，这些运算也就是求出二者的内积。

#### 数学理论
$$
y=WX
$$
$W$ 是回归系数向量.
给定输入是$X_{1}$, 那么预测结果就是 $y=X_{1}^{T}W$.

现在有一些$X$和$y$, 我们的目的就是找到$W$。 常用的方法就是找出使误差最小的$W$。即预测值$y$与真实值$y$之间的差值。采用最小二乘法：
$$
\sum_{i=1}^{n}(y_{i} - x_{i}^Tw)^2
$$
对$w$求导，得到$X^T(y-Xw)$, 令其等于零，得到
$$
\hat{w} = (X^TX)^{-1}X^Ty
$$
#### 评估指标

* **Mean Absolute Error** (MAE)：误差绝对值的平均值
$$\frac 1n\sum_{i=1}^n|y_i-\hat{y}_i|$$

* **Mean Squared Error** (MSE): 误差平方的均值
$$\frac 1n\sum_{i=1}^n(y_i-\hat{y}_i)^2$$

* **Root Mean Squared Error** (RMSE): 均方根误差

$$\sqrt{\frac 1n\sum_{i=1}^n(y_i-\hat{y}_i)^2}$$


#### ScikitLearn 中的线性回归用法

~~~
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

lm = LinearRegression()

lm.fit(X_train, y_train)
print(lm.intercept_)
print(lm.coef_)

predictions = lm.predict(X_test)

print('MAE: ', metrics.mean_absolute_error(y_test, predictions)
print('MSE: ', metrics.mean_squared_error(y_test, predictions))
print('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, predictions))
~~~
lm.intercept_: 是 $y=WX + w_{0}$ 中的$w_{0}$

lm.coef_： 是 $y=WX + w_{0}$ 中的$W$


预测误差(prediction error)、估计误差(estimation error)或者误差残留(residual error)：y_test - predictions 应该符合正态分布
~~~
sns.distplot((y_test - predictions), bins=50)
~~~

#### 数学知识补充
最小二乘法估计
$$
y = m_{0} + m_{1}x + e
$$
其中误差项 $e$ 引入用以解释不确定性的因素。

基本假设：
1. 零均值假设：误差项是期望为零的随机变量，即 $E(e)=0$
2. 不变方差假设：误差项 $e$ 的方差（用 $\sigma^2$ 表示）是常数且与 $x_{1}$, $x_{2}$,…. 的值无关
3. 独立性假设：$e$ 的变量是相互独立的
4. 正态性假设：误差项 $e$ 是正态随机变量,也即：误差项 $e$ 的值是独立的正态分布随机变量，带有均值0和不变方差 $\sigma^2$