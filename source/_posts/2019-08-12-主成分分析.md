---
title: "主成分分析，PCA"
catalog: true
toc_nav_num: true
math: true
date: 2019-08-12 18:30:24
subtitle:
header-img: "/img/article_header/article_header.png"
tags:
- 数据降维
catagories:
- ML

---

#### 概念

* 多重共线性 multi-collinearity
  多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。
* PCA
  通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量
* 降维
  就是找到k个超平面，把原n维空间的数据投影到这k个超平面上，使原n维数据降为k维。
* PCA核心思想
  降维后的数据尽可能分散
* PCA的目的
  是找到一个投影矩阵，使得样本点投影后尽可能分散，即使得样本点投影后的方差最大，并且为了避免投影的堆叠，投影矩阵中的各个向量应该是正交的。
* 中心化

#### PCA过程总结
1. 初始化X，使得所有样本之间的特征值均值为0，同时应用feature scaling，缩放到-0.5～0.5
2. 计算X的协方差矩阵S
3. 对S进行SVD分解，U即我们要求的新坐标系集合， Σ \SigmaΣ 为特征值集合（计算时特征值都会大于0，且结果会从小到大排列)
4. 按照特征值从大到小排序，要降低为k维，那么取前k个特征值对应的特征向量，就是新的k个坐标轴
5. 把X映射到新的坐标系中，完整降维操作
#### 主成份数量的选择
PCA使得数据从n维降低为k维度。
一般选择标准为：投影前后方差比例值，作为k值的选择标准。

#### Scikit Learn
~~~
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
%matplotlib inline
sns.set_style('whitegrid')

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

cancer = load_breast_cancer()
df = pd.DataFrame(cancer['data'], columns=cancer['feature_names'])

scaler = StandardScaler()
scaler.fit(df)
scaled_data = scaler.transform(df)

pca = PCA(n_components=2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)

plt.figure(figsize=(8, 6))
plt.scatter(x_pca[:,0], x_pca[:, 1], c=cancer['target'], cmap='plasma')
plt.xlabel('First Principle Component')
plt.ylabel('Second Principle Component')
~~~
#### 数学理论
* 矩阵换基底
  坐标变换地目标是，找到一组新的正交单位向量，替换原来的正交单位向量。
* 拉格朗日乘子法
  拉格朗日乘子法主要提供了一种求解函数在约束条件下极值的方法。
* 协方差矩阵
  协方差研究的目的是变量（特征）之间的关系
  每个样本点到样本中心距离的平方和的平均 = 样本各个特征方差和（自身协方差），样本的方差
* 特征向量
* 奇异值分解
  奇异值分解（svd: singular value decomposition）定义：对于任意的矩阵A，存在：
* 特征向量和奇异值分解的关系

#### 参考文章
[详细推导PCA算法](https://zhuanlan.zhihu.com/p/55297233)
